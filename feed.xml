<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sebiwi</title>
    <description>Tech and stuff</description>
    <link>https://sebiwi.github.io</link>
    
      
        <item>
          <title>We DevSecOps now</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-07-09-we-devsecops-now.jpg&quot; alt=&quot;Now that I have a DevSecOps team I only need another DevOps team to complete my organisation entity chain&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-07-09T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/we-devsecops-now/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/we-devsecops-now/</guid>
        </item>
      
    
      
        <item>
          <title>Features</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-07-02-features.jpg&quot; alt=&quot;Programming languages are not chosen based on their features, but rather on their popularity.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-07-02T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/features/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/features/</guid>
        </item>
      
    
      
        <item>
          <title>Summer thoughts</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-06-25-summer-thoughts.jpg&quot; alt=&quot;I aways have multiple threads in my head, thinking about the most random stuff on every single situation.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-06-25T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/summer-thoughts/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/summer-thoughts/</guid>
        </item>
      
    
      
        <item>
          <title>Trust</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-06-18-Trust.jpg&quot; alt=&quot;If you put some images in prodution, it's a good idea to go and see what the image actually does.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-06-18T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/trust/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/trust/</guid>
        </item>
      
    
      
        <item>
          <title>Acquisition</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-06-11-Acquisition.jpg&quot; alt=&quot;If you're excited about Microsoft's acquisition of GitHub, wait until you start seeing Mr. Clippy giving you advice in order to create merge requests.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-06-11T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/acquisition/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/acquisition/</guid>
        </item>
      
    
      
        <item>
          <title>Ansible Container: Chronicle of a death foretold</title>
          <description>&lt;p&gt;&lt;strong&gt;This article was co-written by the great &lt;a href=&quot;https://blog.octo.com/author/adrien-besnard-bes/&quot;&gt;Adrien Besnard&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Alright, here’s what’s up:&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;We tried Ansible Container. We’d rather keep using Dockerfiles for image
builds: creating a Docker image and provisioning servers with Ansible are two
very different things. Different in terms of lifecycle, philosophy and
workflow. So different, that in our opinion, they’re not compatible.&lt;/p&gt;

&lt;p&gt;Wanna know why? Read on.&lt;/p&gt;

&lt;p&gt;Disclaimer! While the current status of Ansible Container is not clear, it
seems that during the writing of this article the tool has &lt;a href=&quot;https://github.com/ansible/ansible-container/commit/2fa778a7c8d1699672314ac0b89c53554f435cb7&quot;&gt;been deprecated&lt;/a&gt;.
After the limitations we noticed, we won’t say that we didn’t see that
coming…&lt;/p&gt;

&lt;h2 id=&quot;friendly-reminders-because-were-friendly&quot;&gt;Friendly reminders (because we’re friendly)&lt;/h2&gt;

&lt;h3 id=&quot;ansible&quot;&gt;Ansible&lt;/h3&gt;

&lt;p&gt;Ansible is an automation software which allows you to do provisioning,
configuration and deployment tasks. Everything is written in a neat, simple,
easily readable YAML format, which is way better than old school Bash scripts.
But it remains code, Infrastructure as Code.&lt;/p&gt;

&lt;p&gt;We won’t go deep into the details but we can say it has been a game-changer
regarding automated configuration and deployment because of its simplicity: you
only need SSH access to a server and that’s all. Every action is done from a
centralized server (which can be your computer, but it’s usually a CI/CD server
such as Jenkins) which connects to the other servers using SSH in order to
execute actions (most of the time, that means running Python code). Simple,
easy, efficient.&lt;/p&gt;

&lt;p&gt;One of the cool abstraction of Ansible are the Roles which are a way to
describe how to setup an application without knowing the target host in
advance, in a reusable way.&lt;/p&gt;

&lt;h3 id=&quot;docker&quot;&gt;Docker&lt;/h3&gt;

&lt;p&gt;Docker is the de-facto container management tool. It allows you to build and
manage images, from which you can create containers. You can use Docker to do
this last part too if you want. Usually, when developing using a
container-oriented workflow, you will use Docker in order to package your
application.&lt;/p&gt;

&lt;p&gt;Sadly, the process of creating Docker images isn’t perfect. This, for example,
is the sample Dockerfile used in order to create an Elasticsearch image:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Dockerfile&quot;&gt;FROM openjdk:8-jre

# grab gosu for easy step-down from root
ENV GOSU_VERSION 1.10
RUN set -x \
    &amp;amp;&amp;amp; wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)&quot; \
    &amp;amp;&amp;amp; wget -O /usr/local/bin/gosu.asc &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc&quot; \
    &amp;amp;&amp;amp; export GNUPGHOME=&quot;$(mktemp -d)&quot; \
    &amp;amp;&amp;amp; gpg --keyserver ha.pool.sks-keyservers.net --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 \
    &amp;amp;&amp;amp; gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu \
    &amp;amp;&amp;amp; rm -rf &quot;$GNUPGHOME&quot; /usr/local/bin/gosu.asc \
    &amp;amp;&amp;amp; chmod +x /usr/local/bin/gosu \
    &amp;amp;&amp;amp; gosu nobody true

RUN set -ex; \
# https://artifacts.elastic.co/GPG-KEY-elasticsearch
    key='46095ACC8548582C1A2699A9D27D666CD88E42B4'; \
    export GNUPGHOME=&quot;$(mktemp -d)&quot;; \
    gpg --keyserver ha.pool.sks-keyservers.net --recv-keys &quot;$key&quot;; \
    gpg --export &quot;$key&quot; &amp;gt; /etc/apt/trusted.gpg.d/elastic.gpg; \
    rm -rf &quot;$GNUPGHOME&quot;; \
    apt-key list

# https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html
# https://www.elastic.co/guide/en/elasticsearch/reference/5.0/deb.html
RUN set -x \
    &amp;amp;&amp;amp; apt-get update &amp;amp;&amp;amp; apt-get install -y --no-install-recommends apt-transport-https &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* \
    &amp;amp;&amp;amp; echo 'deb https://artifacts.elastic.co/packages/5.x/apt stable main' &amp;gt; /etc/apt/sources.list.d/elasticsearch.list

ENV ELASTICSEARCH_VERSION 5.6.8
ENV ELASTICSEARCH_DEB_VERSION 5.6.8

RUN set -x \
    \
# don't allow the package to install its sysctl file (causes the install to fail)
# Failed to write '262144' to '/proc/sys/vm/max_map_count': Read-only file system
    &amp;amp;&amp;amp; dpkg-divert --rename /usr/lib/sysctl.d/elasticsearch.conf \
    \
    &amp;amp;&amp;amp; apt-get update \
    &amp;amp;&amp;amp; apt-get install -y --no-install-recommends &quot;elasticsearch=$ELASTICSEARCH_DEB_VERSION&quot; \
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*

ENV PATH /usr/share/elasticsearch/bin:$PATH

WORKDIR /usr/share/elasticsearch

RUN set -ex \
    &amp;amp;&amp;amp; for path in \
        ./data \
        ./logs \
        ./config \
        ./config/scripts \
    ; do \
        mkdir -p &quot;$path&quot;; \
        chown -R elasticsearch:elasticsearch &quot;$path&quot;; \
    done

COPY config ./config

VOLUME /usr/share/elasticsearch/data

COPY docker-entrypoint.sh /

EXPOSE 9200 9300
ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]
CMD [&quot;elasticsearch&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It makes you want to die, doesn’t it? It’s complex, verbose and hard to
interpret. What are the logic steps involved in the construction of this image?
If you spend some (a lot) of time reading this Dockerfile in order to
understand what it does, you will realize that it starts from an openjdk image,
it installs gosu, and then it does a lot of things in order to install
Elasticsearch.&lt;/p&gt;

&lt;h3 id=&quot;ansible-container&quot;&gt;Ansible Container&lt;/h3&gt;

&lt;p&gt;Now imagine this: you work with your infrastructure in the cloud, and you
configure and deploy everything using Ansible. You have a huge galaxy of roles
that allow you to do many things related to your application (like installing
Java and Zookeeper, to name a few). You want to transform your workflow in
order to start using a container-based approach, but you don’t want to lose all
your Ansible resources.&lt;/p&gt;

&lt;p&gt;What if instead of doing all that, you do this:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;container&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gosu&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;elasticsearch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Wouldn’t that be great? Wouldn’t it? Yes it would.&lt;/p&gt;

&lt;p&gt;So we started looking around and we realized that there’s a tool called Ansible
Container.&lt;/p&gt;

&lt;p&gt;From its homepage:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;WHY NOT BUILD YOUR CONTAINERS WITH ANSIBLE PLAYBOOKS? NOW YOU CAN.

Ansible Container represents an end to the command &amp;amp;&amp;amp; command &amp;amp;&amp;amp; command (and
so on) syntax you’ve been struggling with to build containers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Sounds pretty good, doesn’t it? Let’s take a look into it.&lt;/p&gt;

&lt;h2 id=&quot;lets-play&quot;&gt;Let’s play!&lt;/h2&gt;

&lt;p&gt;In order to play with Ansible Container, we’re going to deploy our own simple
Scala application which uses ZooKeeper in order to manage its state.&lt;/p&gt;

&lt;h3 id=&quot;our-application&quot;&gt;Our application&lt;/h3&gt;

&lt;p&gt;Our application is dead simple:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First, it reads its configuration from the /etc/archiver.conf file&lt;/li&gt;
  &lt;li&gt;For each cycle on configured frequency, it looks for files under the
configured directory. If it finds any, it compresses them and then it deletes
the original files&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The fun part is that you can launch as many instances of the application as you
want, and only one of them will do the compression/deletion tasks. That’s what
Zookeeper is used for.&lt;/p&gt;

&lt;p&gt;In order to make the application communicate with Zookeeper, we use Apache
Curator, which is a Zookeeper library. It provides many utilities we can use in
order to provide distributed lock policies and leader election.&lt;/p&gt;

&lt;p&gt;You can see the code of our application here:
https://gitlab.octo.com/abesnard/ansible-container-zookeeper-article/tree/provision-virtual-machines/archiver.&lt;/p&gt;

&lt;p&gt;We wanted to create an application that needs Zookeeper because it seemed like
a coherent test case provided the assumptions stated before: we have an
Ansible-managed workflow, on virtual machines, and we’re moving towards a
container-based approach.&lt;/p&gt;

&lt;h4 id=&quot;the-ansible-role&quot;&gt;The Ansible Role&lt;/h4&gt;

&lt;p&gt;The Ansible Role is fairly simple:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We create a user which will be used in order to run our application&lt;/li&gt;
  &lt;li&gt;We copy the JAR&lt;/li&gt;
  &lt;li&gt;We create the configuration file using some Ansible variables&lt;/li&gt;
  &lt;li&gt;And finally we run everything using systemd&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create system user&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yes&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;home&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/usr/lib/archiver&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create directories&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;directory&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;with_items&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/usr/lib/archiver&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;copy JAR file&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver.jar&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;dest&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/usr/lib/archiver/archiver.jar&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;copy_jar_file&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create config file&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;archiver {&lt;/span&gt;
        &lt;span class=&quot;no&quot;&gt;input-folder-path = &quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;no&quot;&gt;output-folder-path = &quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;no&quot;&gt;tick-period =  seconds&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;}&lt;/span&gt;


    &lt;span class=&quot;no&quot;&gt;zookeeper.servers = [&lt;/span&gt;
        &lt;span class=&quot;no&quot;&gt;{{ archiver_zookeeper_servers | map('quote') | join(', ') }}&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;]&lt;/span&gt;


    &lt;span class=&quot;no&quot;&gt;lock.timeout =  seconds&lt;/span&gt;

    &lt;span class=&quot;no&quot;&gt;dest: &quot;/etc/archiver.conf&quot;&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;owner: &quot;archiver&quot;&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;group: &quot;archiver&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create_config_file&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create systemd service&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;[Unit]&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;Description=Archiver&lt;/span&gt;

    &lt;span class=&quot;no&quot;&gt;[Service]&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;owner=archiver&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;Group=archiver&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;ExecStart=/usr/bin/java -jar &quot;/usr/lib/archiver/archiver.jar&quot;&lt;/span&gt;
    &lt;span class=&quot;no&quot;&gt;dest: &quot;/etc/systemd/system/archiver.service&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create_systemd_service&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;reload systemd&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;systemd&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;daemon_reload&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yes&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create_systemd_service.changed&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;start systemd service&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;systemd&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver.service&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yes&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;started&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;restart systemd service&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;systemd&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver.service&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yes&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;restarted&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create_systemd_service.changed or create_config_file.changed or copy_jar_file.changed&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;zookeeper&quot;&gt;ZooKeeper&lt;/h3&gt;

&lt;p&gt;In a few words: ZooKeeper is a distributed and resilient key-value store. It
also provides some abstractions, making it a great solution in order to have a
distributed lock… which is exactly what we want because we only want one
instance of our Archiver application to run compression/deletion tasks at a
time.&lt;/p&gt;

&lt;h4 id=&quot;the-ansible-role-1&quot;&gt;The Ansible role&lt;/h4&gt;

&lt;p&gt;Because we do not want to reinvent the wheel, we’re going to use an existing
Ansible role from the Ansible Galaxy in order to provision ZooKeeper on our
machines : https://galaxy.ansible.com/AnsibleShipyard/ansible-zookeeper/.&lt;/p&gt;

&lt;p&gt;It’s a little bit off-topic, but Ansible Galaxy is a great tool which you can
use to organize and centralize your roles. More information here: https://galaxy.ansible.com/!&lt;/p&gt;

&lt;h3 id=&quot;end-to-end&quot;&gt;End to end&lt;/h3&gt;

&lt;p&gt;Our playbook looks quite simple :&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;install ZooKeeper&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;become&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yes&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;become_method&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;sudo&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;AnsibleShipyard.ansible-zookeeper&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;zookeeper_hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;groups['zookeeper']&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;map('extract',&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hostvars,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;['ansible_'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;iface,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ipv4',&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'address'])&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}}&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;zookeeper_version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;3.4.12&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;install Archiver&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;archiver&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;become&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yes&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;become_method&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;sudo&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;archiver_zookeeper_servers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;groups['zookeeper']&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;map('extract',&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;hostvars,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;['ansible_'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;iface,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ipv4',&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'address'])&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}}&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;archiver_input_folder_path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/shared/to-archive&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;archiver_output_folder_path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/shared/archived&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;archiver_tick_period_in_seconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;archiver_lock_timeout_in_seconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, it works great on our two Vagrant boxes (it takes some time
because of the downloads of Java and ZooKeeper, but if you skip to the end…
Everything goes fine!) :&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/186568.js&quot; id=&quot;asciicast-186568&quot; async=&quot;&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;the-use-case&quot;&gt;The Use-Case&lt;/h2&gt;

&lt;h3 id=&quot;first-try&quot;&gt;First Try&lt;/h3&gt;

&lt;p&gt;As we said before, a perfect use-case of Ansible Container is to leverage on
all the roles which have already been created in order to Dockerize existing
applications. So let’s do it: we’re going to containerize ZooKeeper and our
Archiver application…&lt;/p&gt;

&lt;p&gt;We first write a container.yml file like this:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2&quot;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;settings&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;conductor&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu:xenial&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;project_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ansible-container-blog&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;zookeeper&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu:xenial&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;AnsibleShipyard.ansible-zookeeper&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;zookeeper_version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;3.4.12&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;archiver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu:xenial&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver&quot;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;archiver_zookeeper_servers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;zookeeper&quot;&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;archiver_input_folder_path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/shared/to-archive&quot;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;archiver_output_folder_path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/shared/archived&quot;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;archiver_tick_period_in_seconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;archiver_lock_timeout_in_seconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;depends_on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;links&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/shared:/shared&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let’s try it, and see what’s going on!&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/186571.js&quot; id=&quot;asciicast-186571&quot; async=&quot;&quot;&gt;&lt;/script&gt;

&lt;p&gt;It does not work, because we need to add some stuff in our role in order to
make it Docker compliant: Instead of having systemd to start a daemon, we need
to provide a Docker CMD.&lt;/p&gt;

&lt;p&gt;If we take a step back, it’s easy to say that most of your roles are not ready
to be used as-is in a container using Ansible Container: multiple tasks are
relevant only in the context of classic servers (systemd services, firewall,
mounts, etc.). It’s due to the fact that Ansible is more that a tool to install
applications: it goes way beyond this simple use case… which is not relevant
in a Docker context.&lt;/p&gt;

&lt;p&gt;Note that Ansible is aware of this problematic and that’s why the notion of
Container-Enabled Role has been created:
https://docs.ansible.com/ansible-container/roles/galaxy.html. So when you use
Ansible Galaxy, be aware of that!&lt;/p&gt;

&lt;h3 id=&quot;second-try&quot;&gt;Second Try&lt;/h3&gt;

&lt;p&gt;Fine… We’re going to make our role compliant:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First, add when: ansible_env.ANSIBLE_CONTAINER is not defined in the step
which are not relevant in a container context (so in our case, everything
which is related to systemd)&lt;/li&gt;
  &lt;li&gt;Then add a meta/container.yml file in our Ansible role in order to tell
Ansible Container what is the actual CMD which needs to be run:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ubuntu:xenial&quot;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;archiver&quot;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;java&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-jar&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/usr/lib/archiver/archiver.jar&quot;&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And here we go again!&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/186576.js&quot; id=&quot;asciicast-186576&quot; async=&quot;&quot;&gt;&lt;/script&gt;

&lt;p&gt;It’s alive! You can even see the Docker images which have been created by
Ansible Container (everything is properly named and tagged, which is pretty
nice) and the Conductor images (which are used by Ansible Container under the
hood to provision the containers):&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/186578.js&quot; id=&quot;asciicast-186578&quot; async=&quot;&quot;&gt;&lt;/script&gt;

&lt;p&gt;Just for you to know: we lied to you. We actually didn’t use the
AnsibleShipyard.ansible-zookeeper role as-is: we had to patch it for this
particular reason in order to continue our trial of Ansible Container:
https://gitlab.octo.com/abesnard/ansible-container-zookeeper-article/blob/build-containers-after-changes/ansible/roles/AnsibleShipyard.ansible-zookeeper.patch.&lt;/p&gt;

&lt;p&gt;Ansible Container even allows us to launch everything it created by creating an
Ansible playbook which actually starts the containers… Well, let’s do it!&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/186580.js&quot; id=&quot;asciicast-186580&quot; async=&quot;&quot;&gt;&lt;/script&gt;

&lt;p&gt;Pretty neat, right?&lt;/p&gt;

&lt;h2 id=&quot;reusage&quot;&gt;Reusage&lt;/h2&gt;

&lt;h3 id=&quot;as-is&quot;&gt;As-Is&lt;/h3&gt;
&lt;p&gt;We just saw that with a few minor modifications of our role, we’re now capable
of using Ansible Container in order to produce Docker images instead of
actually deploying our role to some virtual machines.&lt;/p&gt;

&lt;p&gt;Before going straight ahead to a Kubernetes cluster, we want to be sure that we
can reuse our newly created Docker image using Docker Compose. Easy, let’s
define our docker-compose.yml file:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3&quot;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;zookeeper&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;zookeeper:latest&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2181:2181&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;archiver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ansible-container-blog-archiver&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;depends_on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;zookeeper&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/tmp/shared:/shared:rw&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can see that instead of leveraging of the ansible-container-blog-zookeeper
image, we’re going to use the official Docker image of ZooKeeper: it’s totally
possible because of the plug’n’play spirit of these images.&lt;/p&gt;

&lt;p&gt;And run start Docker Compose:&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/186582.js&quot; id=&quot;asciicast-186582&quot; async=&quot;&quot;&gt;&lt;/script&gt;

&lt;p&gt;Everything is fine!&lt;/p&gt;

&lt;h3 id=&quot;update-configuration&quot;&gt;Update configuration&lt;/h3&gt;

&lt;p&gt;And now, what I want to do is to change the frequency of the file scan.&lt;/p&gt;

&lt;p&gt;Well… we can’t actually do this without building the images using Ansible
Container again. And for us, this is the main issue of Ansible Container: an
Ansible role is fundamentally different from a Dockerfile.&lt;/p&gt;

&lt;h4 id=&quot;build-time-vs-run-time&quot;&gt;Build Time V.S. Run Time&lt;/h4&gt;

&lt;p&gt;And here we are… using our role (which heavily leverages Ansible’s template
module), we created a Docker image that is designed in order to work only in
the context of the architecture described in the container.yml file: the Docker
image we created cannot be used in a plug’n’play fashion. You cannot easily
reuse a Docker image which have been created with Ansible Container.&lt;/p&gt;

&lt;p&gt;The cause of this is written in the /etc/archiver.conf file, all the value are
hardcoded in the Docker image:&lt;/p&gt;

&lt;div class=&quot;language-conf highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;archiver&lt;/span&gt; {
  &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;-&lt;span class=&quot;n&quot;&gt;folder&lt;/span&gt;-&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; = &lt;span class=&quot;s2&quot;&gt;&quot;/shared/to-archive&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;-&lt;span class=&quot;n&quot;&gt;folder&lt;/span&gt;-&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; = &lt;span class=&quot;s2&quot;&gt;&quot;/shared/archived&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;tick&lt;/span&gt;-&lt;span class=&quot;n&quot;&gt;period&lt;/span&gt; = &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;
}

&lt;span class=&quot;n&quot;&gt;zookeeper&lt;/span&gt;.&lt;span class=&quot;n&quot;&gt;servers&lt;/span&gt; = [
  &lt;span class=&quot;n&quot;&gt;zookeeper&lt;/span&gt;
]

&lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;.&lt;span class=&quot;n&quot;&gt;timeout&lt;/span&gt; = &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The only way to update theses properties is to change the container.yml file
like this and rebuild everything… And we do think that it’s not a good
practice in a Docker context: we usually use environment variables to set or
override properties.&lt;/p&gt;

&lt;h2 id=&quot;wrap-up&quot;&gt;Wrap Up&lt;/h2&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;

&lt;p&gt;Ansible Container is not a bad tool and we actually think that it’s a good
idea: writing a Dockerfile is often a pain, and leveraging on Ansible could
have been a good idea.&lt;/p&gt;

&lt;p&gt;But provisioning with Ansible heavily rely on the template module which sets the
properties during the provisioning, but do not permit to modify them afterward.
There is no such thing as Run Time with Ansible.&lt;/p&gt;

&lt;p&gt;We didn’t cover that in this article, but Ansible Container is also bad at
caching stuff, which is a shame because it’s one of the best things that the
different Layers boughts by Docker provide…&lt;/p&gt;

&lt;h3 id=&quot;alternatives&quot;&gt;Alternatives&lt;/h3&gt;

&lt;p&gt;There are multiple ways to bypass this limitation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The best way is to update your application in order to make it aware of the
environment variables (this kind of behavior is native with multiple
configuration framework, like Spring :
https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html
or Typesafe Config : https://github.com/lightbend/config) ;&lt;/li&gt;
  &lt;li&gt;Otherwise, before starting your application in your entrypoint.sh file, you
can use:
    &lt;ul&gt;
      &lt;li&gt;The envsubst program which will allow you to replace reference to any
environment variable by its value when called:
https://linux.die.net/man/1/envsubst&lt;/li&gt;
      &lt;li&gt;The confd program which is also a tool to fill a template using values,
except this time the values can come from other sources like Consul, etc.:
https://github.com/kelseyhightower/confd.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please also note that if your Dockerfile is a mess because of the multiple
scripts it embeds, you still can put all the logic in a clean separate python
file… maybe this simple solution can also be a good start.&lt;/p&gt;

</description>
          <pubDate>2018-06-07T09:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/ansible-container/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/ansible-container/</guid>
        </item>
      
    
      
        <item>
          <title>The Wizard: Scenarios</title>
          <description>&lt;h1 id=&quot;you-just-dont-know-how-to-do-it&quot;&gt;You just don’t know how to do it.&lt;/h1&gt;

&lt;p&gt;You’ve been thinking about it for a while too. It’s an important issue. The
crafting of the spell is the most important part. The journey is usually more
important than the destination itself. But you don’t feel like you are doing it
right.&lt;/p&gt;

&lt;p&gt;The purpose is simple, yet it has a duality of character. You want to enable
the transmission of information between all realms into a single library of
knowledge. A task you’ve already accomplished many times in the past.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;s&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# filebeat/tasks/main.yml&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;install Filebeat&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;package&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;filebeat=&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;update_cache&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;notify&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;restart Filebeat&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;copy configuration file&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;filebeat.yml.j2&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;dest&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/etc/filebeat/filebeat.yml&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;root&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;root&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;0600&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;notify&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;restart Filebeat&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;start Filebeat&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;filebeat&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;started&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You have crafted this spell properly, following the same principles you have
taught many people across the years, the same ones you’ve used for your own.
The same ones that Merlin taught you the first time he showed you how to create
a playground to refine your creations, thousand of years ago.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;testinfra.utils.ansible_runner&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;testinfra_hosts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testinfra&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ansible_runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AnsibleRunner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MOLECULE_INVENTORY_FILE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_hosts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'all'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_filebeat_is_installed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filebeat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;package&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'filebeat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filebeat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_installed&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_filebeat_is_running&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filebeat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'filebeat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filebeat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_running&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_filebeat_is_enabled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filebeat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'filebeat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filebeat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_enabled&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_filebeat_configuration_exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filebeat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/etc/filebeat/filebeat.yml'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filebeat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Nevertheless, you want to add an extension to the spell, depending on whether
you want to receive information about all the portals you’ve spawned into this
world or not. Visualisation is key.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# filebeat/tasks/main.yml&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;include module tasks&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;import_tasks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx_module.yml&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;activate_filebeat_nginx_module&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# filebeat/tasks/nginx_module.yml&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;copy nginx module file&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx.yml.j2&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;dest&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/etc/filebeat/modules.d/nginx.yml&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;root&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;root&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;0644&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx_module&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;notify&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;restart Filebeat&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;setup filebeat&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;filebeat setup -e &amp;amp;&amp;amp; touch /tmp/filebeat_configured&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx_module.changed&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;skip_ansible_lint&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You know how to craft the general part of the spell, but not the extension. If
only…&lt;/p&gt;

&lt;h2 id=&quot;n-appears-out-of-thin-air&quot;&gt;N appears out of thin air.&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Hey! What are you up to?&lt;/li&gt;
  &lt;li&gt;Fighting with an Ansible role.&lt;/li&gt;
  &lt;li&gt;Can I help?&lt;/li&gt;
  &lt;li&gt;Sure. I’m trying to conditionally add some tasks to my role, but only if I
want the nginx logs to be forwarded to my Elasticsearch cluster. I use the
nginx Filebeat module, which also happens to create pretty cool dashboards.&lt;/li&gt;
  &lt;li&gt;Great. Just use an include_tasks conditioned by a boolean variable defined
somewhere. You could name it &lt;code class=&quot;highlighter-rouge&quot;&gt;activate_filebeat_nginx_module&lt;/code&gt;, for example.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He’s good.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Yeah, that’s what I did. Then again, my problem isn’t that part, but rather
testing the conditional part. I don’t activate the module by default. The
&lt;code class=&quot;highlighter-rouge&quot;&gt;activate_filebeat_nginx_module&lt;/code&gt; is set to false in the role’s defaults. So my
Molecule test only tests the default case.&lt;/li&gt;
  &lt;li&gt;Yes. That’s because that’s the default scenario. Just create another scenario.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You’re confused.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N pulls up his sleeves and steps forward.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Remember how you taught me how to create a role with tests using Molecule?
The principle is rather simple. You type &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule init role --role-name
filebeat&lt;/code&gt; and Molecule creates all the resources you’re going to need in order
to test your role. That includes the test files and all the other playbooks you
may need: the prepare playbook, the side_effects playbook…&lt;/li&gt;
  &lt;li&gt;Indeed. I remember.&lt;/li&gt;
  &lt;li&gt;The thing is that Molecule creates a &lt;code class=&quot;highlighter-rouge&quot;&gt;default&lt;/code&gt; scenario in order to do that.
That’s why all your resources are under the &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule/default&lt;/code&gt; directory on
your role.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N waves his hands around in the air. Light appears everywhere, and you can see the structure of your current spell:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;filebeat/
├── README.md
├── defaults
│   └── main.yml
├── handlers
│   └── main.yml
├── meta
│   └── main.yml
├── molecule
│   └── default
└── tasks
    ├── main.yml
    └── nginx_module.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;You’re right.&lt;/li&gt;
  &lt;li&gt;If you examine the playbook on the default directory, you will see that there
are no variables specified for the role.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N waves his hands once again in the air and the light and the images change:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# filebeat/molecule/default/playbook.yml&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Converge&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;all&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;filebeat&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;This is because you are actually testing the &lt;code class=&quot;highlighter-rouge&quot;&gt;default&lt;/code&gt; scenario: the one
where you only use the default variables that are defined in the &lt;code class=&quot;highlighter-rouge&quot;&gt;default&lt;/code&gt;
directory of your role.&lt;/li&gt;
  &lt;li&gt;I see.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You have seen this before, but you’ve never really understood the abstractions
or their meaning, even if you’ve already been doing this for a while. You feel
on the verge of enlightenment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;So you just need to create a new scenario.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N claps his hands, and energy starts flowing through him as a new component of
the enclosure you used in order to craft your test appears.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;molecule init scenario --scenario-name activate_filebeat_nginx_module --role-name filebeat
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Initializing new scenario activate_filebeat_nginx_module...
Initialized scenario &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;filebeat/molecule/activate_filebeat_nginx_module successfully.


molecule
├── activate_filebeat_nginx_module
│   ├── Dockerfile.j2
│   ├── INSTALL.rst
│   ├── create.yml
│   ├── destroy.yml
│   ├── molecule.yml
│   ├── playbook.yml
│   ├── prepare.yml
│   └── tests
└── default
    ├── Dockerfile.j2
    ├── INSTALL.rst
    ├── create.yml
    ├── destroy.yml
    ├── molecule.yml
    ├── playbook.yml
    ├── prepare.yml
    └── tests
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You both walk to the new part of the playground.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In this new directory, you can specify everything you need in order to
properly test your new scenario. What do you need first?&lt;/li&gt;
  &lt;li&gt;I need to specify that I want to include the conditional part of my playbook.&lt;/li&gt;
  &lt;li&gt;Where would you do that?&lt;/li&gt;
  &lt;li&gt;I’d use the group_vars at the playbook level.&lt;/li&gt;
  &lt;li&gt;That’s good. Then again, there are no group_vars here. You only have a small
playbook that specifies everything you do in order to test your role.&lt;/li&gt;
  &lt;li&gt;So I’ll do it in the playbook.yml file.&lt;/li&gt;
  &lt;li&gt;Correct.&lt;/li&gt;
  &lt;li&gt;Isn’t that a bit dirty? I’ve always thought that you’re not supposed to use
variables on a playbook, but rather put them somewhere else.&lt;/li&gt;
  &lt;li&gt;That’s a style thing, but yes, I would say that generally it is a good
practice. Nevertheless, it is precisely what you need right now. As I just
said, your playbook defines every parameter used to test your role. It’s easy
to see how are you testing your role just by looking at it. You know precisely
which variables are defined and how. Even if the name of the scenario should be
explicit enough already in order to understand it. Try it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You mold the spell accordingly.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# filebeat/molecule/activate_filebeat_nginx_module/playbook.yml&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Converge&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;all&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;filebeat&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;activate_filebeat_nginx_module&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Good. Now you can test everything you wanted. Just remember that in order to
test every scenario, you need to use &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule test --all&lt;/code&gt;. This will execute
every scenario, which is probably what you want. If you want to speed up the
red/green/refactor loop, you can use the &lt;code class=&quot;highlighter-rouge&quot;&gt;create&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;converge&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;verify&lt;/code&gt;
actions like you taught me before, but you need to specify the scenario name
when executing every action, like this: &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule create --scenario-name
activate_filebeat_nginx_module&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You are impressed with N. It’s hard to believe that you were the one teaching
him things once. Maybe he’s been the one teaching you all from the start.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;That’s precisely what I needed.&lt;/li&gt;
  &lt;li&gt;I’m glad I could be of service. Do you need any more help?&lt;/li&gt;
  &lt;li&gt;No, I’m all set. I just…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You’re having a déjà-vu. N is outside your field of vision. You slowly turn
your head around…&lt;/p&gt;

&lt;p&gt;N is still there.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What?&lt;/li&gt;
  &lt;li&gt;Nothing.&lt;/li&gt;
&lt;/ul&gt;
</description>
          <pubDate>2018-06-07T09:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/the-wizard-3/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/the-wizard-3/</guid>
        </item>
      
    
      
        <item>
          <title>GDPR</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-06-04-GDPR.jpg&quot; alt=&quot;If you wanna stress an OPS person, ask him how he is going to handle GDPR.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-06-04T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/GDPR/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/GDPR/</guid>
        </item>
      
    
      
        <item>
          <title>Deadlock</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-05-28-deadlock.jpg&quot; alt=&quot;I happen to think that the hardest moment of the day is deciding what to get for lunch.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-05-28T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/deadlock/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/deadlock/</guid>
        </item>
      
    
      
        <item>
          <title>Stereotypes</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-05-21-stereotypes.jpg&quot; alt=&quot;A lot of persons told me that I should make Deb wear a dress and a ribbon. I'm not doing it.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-05-22T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/stereotypes/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/stereotypes/</guid>
        </item>
      
    
      
        <item>
          <title>Sidecar</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-04-30-sidecar.jpg&quot; alt=&quot;I'll probably end up getting a sidecar for human interations one of these days. Like a human service mesh.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-04-30T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/sidecar/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/sidecar/</guid>
        </item>
      
    
      
        <item>
          <title>The show must go on</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-04-23-the-show-must-go-on.jpg&quot; alt=&quot;People will do the dumbest things in order to convince their clients that everything is going alright.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-04-23T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/the-show-must-go-on/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/the-show-must-go-on/</guid>
        </item>
      
    
      
        <item>
          <title>Visage pamphlet</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-04-16-visage-pamphlet.jpg&quot; alt=&quot;Isn't it weird to being asked to delete your data, then not doing it afterwards? It is a technique known as the &amp;quot;Oxford Investigativa&amp;quot;.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-04-16T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/visage-pamphlet/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/visage-pamphlet/</guid>
        </item>
      
    
      
        <item>
          <title>Dear Jenkins</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-04-09-dear-jenkins.jpg&quot; alt=&quot;I say that I hate Jenkins, often. It's not true though.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-04-09T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/dear-jenkins/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/dear-jenkins/</guid>
        </item>
      
    
      
        <item>
          <title>Choices</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-04-02-choices.jpg&quot; alt=&quot;It seems crazy to look like a moron if you're not doing either golang or rust in 2018.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-04-02T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/choices/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/choices/</guid>
        </item>
      
    
      
        <item>
          <title>Go nuts</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-03-26-go-nuts.jpg&quot; alt=&quot;Fun fact: The #go channel on Freenode is about the board game, not about the programming language.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-03-26T08:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/comics/go-nuts/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/go-nuts/</guid>
        </item>
      
    
      
        <item>
          <title>Jokes</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-03-19-jokes.jpg&quot; alt=&quot;I like explaining my jokes to people. It's funnier, somehow. They tend to get frustrated. I can't really figure out why yet.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-03-19T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/jokes/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/jokes/</guid>
        </item>
      
    
      
        <item>
          <title>Microservices</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-03-12-microservices.jpg&quot; alt=&quot;What is the actual size of a microservice? When does it become a miliservice?&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-03-12T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/microservices/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/microservices/</guid>
        </item>
      
    
      
        <item>
          <title>Yoloprotect</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-03-05-yoloprotect.jpg&quot; alt=&quot;It's funny that bpfilter came out when we haven't even started using nftables, right?&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-03-05T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/yoloprotect/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/yoloprotect/</guid>
        </item>
      
    
      
        <item>
          <title>Velocity</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-02-26-velocity.jpg&quot; alt=&quot;I wonder if managers are going to understand eventually that velocity is not a KPI that should be use to assess teams&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-02-26T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/velocity/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/velocity/</guid>
        </item>
      
    
      
        <item>
          <title>The Wizard: Side effects</title>
          <description>&lt;h3 id=&quot;he-has-questions&quot;&gt;He has questions.&lt;/h3&gt;

&lt;p&gt;Nevertheless, he has not yet arrived. And you know you have answers. You
interrupt your summoning, and sit down. You can continue in another moment.
Everything happens for a reason. You just hope the reason is good.&lt;/p&gt;

&lt;p&gt;N appears right next to you.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hey, what’s up?&lt;/li&gt;
  &lt;li&gt;I’m setting up the auto-scaling configuration of the new project. Nothing huge.&lt;/li&gt;
  &lt;li&gt;Cool. I need some help.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He has progressed immensely since the last time you saw him. People are
starting to ask him for help now. You can’t help but feel pride when you see
what he has become.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Yeah sure, show me.&lt;/li&gt;
  &lt;li&gt;Remember that nginx role you helped me create?&lt;/li&gt;
  &lt;li&gt;What about it?&lt;/li&gt;
  &lt;li&gt;It’s flawed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Interesting. He did realize after all.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Is it, now?&lt;/li&gt;
  &lt;li&gt;Yes. I usually run my Ansible playbooks every night, in order to ensure that
all my infrastructure and services are at a certain state. At least the ones
in charge of server and middleware configuration.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That’s good stuff, you think.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;That’s good stuff.&lt;/li&gt;
  &lt;li&gt;It is. It works as long as my playbooks are idempotent.&lt;/li&gt;
  &lt;li&gt;Are your playbooks idempotent?&lt;/li&gt;
  &lt;li&gt;Yes, except for the ones I use for application deployment.&lt;/li&gt;
  &lt;li&gt;What’s your issue then?&lt;/li&gt;
  &lt;li&gt;My nginx role. The service stopped unexpectedly, and my role wouldn’t restart it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He realized the hard way. That is probably a good thing. It is the best way to
learn.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Okay. Let’s take a look.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You summon the enclosure you both worked on on that occasion. You need it in
order to properly analyze the spell. You check the structure of it:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# tasks/main.yml&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install epel-release&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;epel-release&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;notify&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It all comes back to you. You remember noticing the little issue with this
proposal. It was a time bomb. But then again, he found it. It’s precisely what
you wanted.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Well, you’re only starting your service when installing nginx.&lt;/li&gt;
  &lt;li&gt;Yes.&lt;/li&gt;
  &lt;li&gt;So if your service is stopped, and you launch your playbook again, nothing
will change when nginx is already installed.&lt;/li&gt;
  &lt;li&gt;Yes. So this whole testing thing… I don’t think it’s a good idea. I
refactored my code, the tests passed, and the service is not running in
production because of a side effect.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Patience is key.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Did you test that side effect?&lt;/li&gt;
  &lt;li&gt;Well… no. How could I? It’s a side effect.&lt;/li&gt;
  &lt;li&gt;Write the code that would cause your side effect, and then see if your actual
code is able to handle that scenario.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He considers the idea for half a second.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;So I’m supposed to put code which makes my components fail inside of my role?
That doesn’t make any sense, does it?&lt;/li&gt;
  &lt;li&gt;You don’t put it inside your role. You put it alongside your code. Watch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You reflect for a second, structuring your thoughts, trying to imagine exactly
what you need. You analyze the dimensions of the enclosure. Depth, width and
height. It won’t suffice for what you have in mind. Once you figure it out, you
close your eyes, and expand the enclosure in order to create a new dimension,
invisible to the naked eye. Inside this new pocket within time and space, you
place the side effect of the spell, just as he described it:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# molecule/default/side_effect.yml&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Stop nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;all&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;tasks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Stop nginx&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;stopped&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will probably suffice, you say to yourself.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Would you say this code is capable of recreating the side effect you described earlier?&lt;/li&gt;
  &lt;li&gt;Yes, I would.&lt;/li&gt;
  &lt;li&gt;Great, let’s use it now.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You concentrate as you modify the structure and logic of the enclosure in order
to use the new dimension:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# molecule/default/molecule.yml&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;scenario&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;test_sequence&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;destroy&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;syntax&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;lint&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;converge&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;side_effect&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;converge&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;idempotence&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;verify&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;So we’re converging twice?&lt;/li&gt;
  &lt;li&gt;Yes. You need to test whether your code is able to correct the state of your
component after a side effect.&lt;/li&gt;
  &lt;li&gt;Yes, indeed.&lt;/li&gt;
  &lt;li&gt;Now, launch your test.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N claps his hands, and as he separates them, a stream of bright red light
appears between them:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;o&quot;&gt;===================================&lt;/span&gt; FAILURES &lt;span class=&quot;o&quot;&gt;===================================&lt;/span&gt;
    __________________ test_nginx_is_running[ansible://instance] ___________________

    host &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &amp;lt;testinfra.host.Host object at 0x108390290&amp;gt;

        def test_nginx_is_running&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;host&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:
            &lt;span class=&quot;c&quot;&gt;# Given&lt;/span&gt;
            nginx &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; host.service&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'nginx'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# Then&lt;/span&gt;
    &amp;gt;       assert nginx.is_running
    E       assert False
    E        +  where False &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &amp;lt;service nginx&amp;gt;.is_running

    tests/test_default.py:20: AssertionError
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;He looks stunned for a while.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;So we weren’t testing the whole way, were we?&lt;/li&gt;
  &lt;li&gt;No, we weren’t. I mean, we were not testing for all of the edge cases we
could have had. You can’t possibly test for all of them. There’s way too
many. This was an interesting one though.&lt;/li&gt;
  &lt;li&gt;I see.&lt;/li&gt;
  &lt;li&gt;Well, you’re in red again. Go to green.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N modifies the spell structure.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# tasks/main.yml&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tasks file for nginx-tdd&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install epel-release&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;epel-release&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;started&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It looks good.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;And now let me test…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N does exactly the same thing he did before. Only this time, bright green light
emanates from his palms:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;o&quot;&gt;=============================&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test &lt;/span&gt;session starts &lt;span class=&quot;o&quot;&gt;==============================&lt;/span&gt;
    platform darwin -- Python 2.7.11, pytest-3.3.2, py-1.5.2, pluggy-0.6.0
    rootdir: /Users/sebiwi/stuff/ocac/skool-ops/craftmanship-ops/ansible-tdd/ansible-nginx-tdd/molecule/default, inifile:
    plugins: testinfra-1.7.1
collected 3 items

    tests/test_default.py ...                                                &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;100%]

    &lt;span class=&quot;o&quot;&gt;===========================&lt;/span&gt; 3 passed &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;7.78 seconds &lt;span class=&quot;o&quot;&gt;===========================&lt;/span&gt;
Verifier completed successfully.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;N looks at you.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;That was simple.&lt;/li&gt;
  &lt;li&gt;Was it?&lt;/li&gt;
  &lt;li&gt;I mean, I could have made my tests better from the beginning.&lt;/li&gt;
  &lt;li&gt;One bug in production is just a thing you didn’t test.&lt;/li&gt;
  &lt;li&gt;It seems like overkill for a simple nginx installation though.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He is right. But he needs to see the bigger picture.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It probably is. But now you know that you’re really testing interesting
scenarios, the ones you really want. Besides, think about it. The
possibilities are endless.&lt;/li&gt;
  &lt;li&gt;What do you mean?&lt;/li&gt;
  &lt;li&gt;You only have a single node component here, but you can use the same
principle in order to test incredibly complex things.&lt;/li&gt;
  &lt;li&gt;Such as?&lt;/li&gt;
  &lt;li&gt;High availability on distributed components, leader election, node failover…
you name it. You can simulate a disaster on a side effect and see how your
code will react to it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He reflects. He’s starting to get it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can I help you with anything else?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Your attention already shifted back to your summoning. What now?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No, I think that will do. I’ll probably go add this to my other playbooks.
You’re right, the possibilities are…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You’re no longer there again. N looks around in confusion.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How does he do that ?&lt;/li&gt;
&lt;/ul&gt;
</description>
          <pubDate>2018-02-21T08:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/blog/the-wizard-2/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/the-wizard-2/</guid>
        </item>
      
    
      
        <item>
          <title>YAML</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-02-19-yaml.jpg&quot; alt=&quot;What's with this new YAML sucks trend? Have you ever tried to read a JSON object? What about (not) goot old XML?&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-02-19T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/yaml/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/yaml/</guid>
        </item>
      
    
      
        <item>
          <title>Service Mesh</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-02-12-service-mesh.jpg&quot; alt=&quot;I wonder if Service Mesh is the new Serverless, or the new Blockchain.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-02-12T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/service-mesh/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/service-mesh/</guid>
        </item>
      
    
      
        <item>
          <title>Serverless</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-02-05-serverless.jpg&quot; alt=&quot;How come people keep talking about serverless, when everything underneath it is actually servers?&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-02-05T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/serverless/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/serverless/</guid>
        </item>
      
    
      
        <item>
          <title>Communication</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-01-29-communication.jpg&quot; alt=&quot;DevOps requires communication between development and operations, and it's better if they don't want to each each other.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-01-29T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/communication/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/communication/</guid>
        </item>
      
    
      
        <item>
          <title>Estimations</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-01-22-estimations.jpg&quot; alt=&quot;There are many valid, snarky responses you can use when someone asks you to estimate a user story.&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-01-22T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/estimations/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/estimations/</guid>
        </item>
      
    
      
        <item>
          <title>Healthchecks</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-01-15-healthchecks.jpg&quot; alt=&quot;If applications were sentient beings, we would annoy the fuck out of them.&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-01-15T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/healthchecks/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/healthchecks/</guid>
        </item>
      
    
      
        <item>
          <title>Meltburn and priorities</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-01-08-meltburn-and-priorities.jpg&quot; alt=&quot;I haven't applied any security patches since 1993, but at least my employees can't open YouTube, right?&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-01-08T01:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/meltburn-and-priorities/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/meltburn-and-priorities/</guid>
        </item>
      
    
      
        <item>
          <title>Twelve factor</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2018-01-01-twelve-factor.jpg&quot; alt=&quot;I need a twelve-factor app, not a twelve factor auth, please.&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2018-01-01T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/twelve-factor/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/twelve-factor/</guid>
        </item>
      
    
      
        <item>
          <title>Kubernetes vs Docker: Volumes!</title>
          <description>&lt;p&gt;&lt;em&gt;This post was co-written by the amazing Pierre-Yves Napoly.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;hey-everybody&quot;&gt;Hey everybody!&lt;/h2&gt;

&lt;p&gt;After reading both the Kubernetes and Docker “How does it work?” series, I
suess you can’t wait to transform your old-school infrastructure and put
all your applications inside containers. Indeed, containers are a great way to
make your applications portable and easy to deploy. Nevertheless, there is a
subject we have not discussed yet: data persistence.&lt;/p&gt;

&lt;p&gt;Before we start, we’d like to say that there are ways to handle data that are
more cloud-oriented than volumes. These include managed relational database
services, non-relational database services, and object storage services, all of
which are easier to operate than volumes, since they harness most of the
benefits of the cloud ecosystem. Volumes seem to match better &lt;a href=&quot;https://blog.octo.com/en/pet-vs-cattle-from-server-craftsman-to-software-craftsman/&quot;&gt;with pets than
with cattle&lt;/a&gt; and may be an obstacle to scalability unless you are using read
only volumes (more on this later). In any case, be sure to regularly backup
your data.&lt;/p&gt;

&lt;p&gt;A container is created from a fixed image, and all its changes are lost when
re-instantiating the image into a new container. It is an ephemeral resource.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/kube-vs-swarm/1/ephemeral-resource.jpg&quot; alt=&quot;Ephemeral resource&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How can we make sure that all the data in the container is persisted somewhere?
You could snapshot the container into a new image, but there is a limit in the
number of time you can do that. And what if your container dies? What if the
machine that’s hosting the image dies? If you see yourself in one of the
previous situations you should probably take a look at Volumes.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-volume&quot;&gt;What is a Volume?&lt;/h2&gt;

&lt;p&gt;A volume can be defined in many ways. Kubernetes says that:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“At its core, a volume is just a directory, possibly with some data in it,
which is accessible to the containers in a pod. How that directory comes to be,
the medium that backs it, and the contents of it are determined by the
particular volume type used.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Whereas Docker says that:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Volumes are directories that are stored outside of the container’s filesystem
and which hold reusable and shareable data that can persist even when
containers are terminated. This data can be reused by the same service on
redeployment, or shared with other services.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Even if Docker and Kubernetes use different words to define them, we can see
that the two concepts are really similar and in both cases volumes serve the
same purpose.&lt;/p&gt;

&lt;h2 id=&quot;why-would-anyone-use-them&quot;&gt;Why would anyone use them?&lt;/h2&gt;

&lt;p&gt;Basically, the two main reasons to use Volumes are data persistency and shared
resources, or in other words, to be able to keep your data if your container
dies, or to share data between multiple containers. These shared resources also
include Secrets, which can be made available to containers/pods through
Volumes.&lt;/p&gt;

&lt;p&gt;How are you supposed to do that though? The solution to this problem is not
easy. Are Volumes binded to a single node? How are multiple containers located
on different hosts supposed to use them then? Are they hosted on a single
machine? Are they even hosted on a machine? Do you even Volume?&lt;/p&gt;

&lt;p&gt;The answer to this is basically different implementations for different needs.
There are many types of Volumes (Types for Kubernetes, and Drivers for Docker),
each one with its own advantages and drawbacks.&lt;/p&gt;

&lt;p&gt;Let’s take a look at some of them, shall we?&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-volumes&quot;&gt;Kubernetes Volumes&lt;/h2&gt;

&lt;p&gt;Kubernetes came out with the notion of Volume as a resource first, then Docker
followed.  There are many Volume types. When I say many, &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/volumes/&quot;&gt;I mean a lot&lt;/a&gt;. You’ve
got local, node-hosted Volume types like emptyDir, hostPath, and local (duh).
You also have Volume types that are hosted on Cloud IaaS platforms, such as
gcePersistentDisk (GCE), awsElasticBlockStore (AWS), and AzureDiskVolume
(Azure). Some Volumes are even backed on traditional storage solutions, like
nfs, iscsi, or fc (fibre channel). Others can be backed on modern, distributed
file systems, like flocker, glusterfs and cephfs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/kube-vs-swarm/1/kube-logo.png&quot; alt=&quot;Kubernetes logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s no way we can describe all of them in a single article. We just can’t.
I’m sorry. We’ll do a couple of important ones though.&lt;/p&gt;

&lt;p&gt;The simplest kind of volume is the emptyDir Volume. These are created when a
Pod is assigned to a node, and they exist for as long as the Pod keeps running
on the node. If the Pod is removed from the node, the Volume is deleted. Then
you have basic, but more complex kind of Volumes: the hostPath Volumes. These
mount either a file or a directory from the node’s filesystem into the Pod
itself. Yay, volumes!&lt;/p&gt;

&lt;p&gt;These are great and all, but they do not actually provide us with the
persistence we’re looking for. The resources are either linked to the execution
and lifetime of the Pod, or to the underlying host, which is precisely what we
do not want.&lt;/p&gt;

&lt;p&gt;In order to have persistent Volumes with a different lifecycle than your pods,
you will need to use a different volume type. We’re going to see how this works
using the awsElasticBlockStore type based on the AWS EBS managed service.&lt;/p&gt;

&lt;p&gt;Basically, and as the name already hints, this type of Volume is backed against
an AWS Elastic Block Storage volume. This is cool because the contents of the
Volume will continue to exist, even if the Pod is removed: the EBS volume will
just be unmounted. There are some requirements for these to work: all the
cluster nodes must be EC2 instances, which need to be in the same availability
zone as the EBS volume, and a volume might only be mounted by a single
instance. Wanna see it? Hands to work!&lt;/p&gt;

&lt;p&gt;First, you will need a Kubernetes cluster running on AWS. There are many ways
to achieve this. I’ll be using &lt;a href=&quot;https://github.com/kubernetes/minikube&quot;&gt;minikube&lt;/a&gt; for this one, but you can use &lt;a href=&quot;https://github.com/kubernetes/kops&quot;&gt;kops&lt;/a&gt; if
you wanna go for a big scale cluster.&lt;/p&gt;

&lt;p&gt;Then, you will need a working EBS volume, with and its ID. You can create that
using the web console, or with the AWS CLI, like so:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws ec2 create-volume --availability-zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;eu-west-1b --size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5 --volume-type&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;gp2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Remember that the availability zone of the EBS volume must be the same as the
one for the ECS instances hosting the cluster.&lt;/p&gt;

&lt;p&gt;Once you have that, you only need to create a Pod, and mount the required
volume while specifying a mount point, like so:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongo-ebs&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongo&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongo-pod&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/data/db&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongo-volume&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongo-volume&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;awsElasticBlockStore&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;volumeID&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;Volume-ID&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;fsType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I’ll name this file mongo.yml. Then all you need to do is type the following
command in order to create the pod:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f mongo.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will create the pod with the associated AWS EBS volume mounted at the
specified mount path.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/F3sLkq1KEPFk9aX3TFmg4WygH.js&quot; id=&quot;asciicast-F3sLkq1KEPFk9aX3TFmg4WygH&quot; async=&quot;&quot;&gt;&lt;/script&gt;

&lt;p&gt;That was simple, right? Let’s see how we can achieve the same thing using
Swarm.&lt;/p&gt;

&lt;h2 id=&quot;swarm-volumes&quot;&gt;Swarm Volumes&lt;/h2&gt;

&lt;p&gt;Swarm might not look as mature as Kubernetes: it only comes with one type of
volume natively, which is a volume shared between the container and its Docker
host. This might come in handy for testing a deployment locally, but it won’t
do the job on a distributed cluster.  Between two deployments the container on
which the volume is mounted might move from a cluster node to another. If this
happens, it will lose the data that was on the precedent node and a new empty
volume will be recreated on the new node. Ouch.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/kube-vs-swarm/1/swarm-logo.png&quot; alt=&quot;Swarm logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One possible solution to palliate this problem is to associate placement
constraints with the container so it always runs on the same node. You should
never do this. It is a really nasty way to solve our problem and if your node
crashes, well… you do the math.&lt;/p&gt;

&lt;p&gt;But yeah, you must have guessed it, there are many solutions other than local
volume. Like for networks, Docker can use &lt;a href=&quot;https://docs.docker.com/engine/extend/legacy_plugins/#network-plugins&quot;&gt;different drivers&lt;/a&gt; to handle its
volumes even if it uses the local driver by default.&lt;/p&gt;

&lt;p&gt;There are two ways to install the drivers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Launch the driver as a system service, and configure the Docker host to use it.
This is the way most drivers works for now.&lt;/li&gt;
  &lt;li&gt;Install the driver as a Docker plugin, using the simple command:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker plugin install rexray/ebs
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Super simple right? Docker is gonna search for the plugin container on the
Docker Hub and it’s going to download it right after. Sadly, very few plugins
are available as of this moment. This will most likely change in the near
future.&lt;/p&gt;

&lt;p&gt;A great volume driver which can be installed as a Docker plugin is REX-Ray. It
is compatible with multiple storage providers and it is agnostic of the
provider it’s using, bringing better user experience to the balance.&lt;/p&gt;

&lt;p&gt;Let’s say we have a Swarm cluster and we need a Mongo database for an
application running on the same Swarm cluster. We are going to launch a Mongo
container that belongs to the same network as our application and map an EBS
volume to it using REX-ray to ensure data persistency.&lt;/p&gt;

&lt;p&gt;To use EBS volumes, we need to provide credentials. This can be achieved in
three different ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;By associating an AWS IAM role to the instance running Docker&lt;/li&gt;
  &lt;li&gt;Through a REX-ray configuration file&lt;/li&gt;
  &lt;li&gt;Through configuration variables while installing the plugin&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First we are going to install REX-Ray on our Swarm manager using a simple
command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker plugin install rexray/ebs &lt;span class=&quot;nv&quot;&gt;EBS_ACCESSKEY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;access_key &lt;span class=&quot;nv&quot;&gt;EBS_SECRETKEY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;secret_key
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then we create a Docker volume using REX-ray:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker volume create &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
--driver rexray/ebs  &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
--opt &lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5         &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
--opt &lt;span class=&quot;nv&quot;&gt;volumetype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;gp2 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
--name ebs_vol
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is going to create a 5 gigabytes EBS volume of gp2 type gp2.&lt;/p&gt;

&lt;p&gt;All we have to do now is to launch a new Docker service using the MongoDB
official image and map the volume to Mongo’s data directory:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker service create           &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
--network my_overlay_network    &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
--replicas 1                    &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
--mount &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;volume,src&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ebs_vol,target&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/data/db &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
--name mongodb mongo
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We now have a Mongo container accessible by all containers on
my_overlay_network at mongodb:27017, which ensures data persistency if the
service stops. Isn’t that great?&lt;/p&gt;

&lt;p&gt;From what I’ve seen so far, this is the best way to handle volumes with Swarm.
Note that you’ll have to implement your own regular backup policy for the
volume on the storage provider. But this will be outside of Docker Swarm scope.&lt;/p&gt;

&lt;p&gt;EMBEDED VIDEO: https://asciinema.org/a/NtXVkycNrBhWzfQUjHWUFRDfM&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/NtXVkycNrBhWzfQUjHWUFRDfM.js&quot; id=&quot;asciicast-NtXVkycNrBhWzfQUjHWUFRDfM&quot; async=&quot;&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;wow-that-was-great&quot;&gt;Wow, that was great!&lt;/h2&gt;

&lt;p&gt;Yeah, it is indeed pretty cool. We got to see what is a volume, how to use it,
and what should we use them for. We also know now how both Kubernetes and Swarm
handle their persistent data. It is nice to see that both solutions are mature
enough and capable of handling data using similar workflows. Even if the exact
procedure used to mount and use volumes is not exactly the same for the two of
them, you can see that the abstractions match in a certain way. Great minds
think alike, don’t they?&lt;/p&gt;

&lt;h3 id=&quot;anyway-tldr&quot;&gt;Anyway, TL;DR:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Map volumes to you containers to ensure data persistency&lt;/li&gt;
  &lt;li&gt;Use volumes on platform storage such as Amazon EBS or GCE Persistent Disk rather than local volumes&lt;/li&gt;
  &lt;li&gt;Platform storage compatibility is native in Kubernetes and easily installable in Docker&lt;/li&gt;
  &lt;li&gt;Be sure to backup your volume regularly&lt;/li&gt;
  &lt;li&gt;If dealing with cattle, consider using replicated databases instead of volumes for better scalability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That was fun, wasn’t it? See you on the next adventure!&lt;/p&gt;

</description>
          <pubDate>2017-12-26T08:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/blog/kubernetes-vs-docker-volumes/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/kubernetes-vs-docker-volumes/</guid>
        </item>
      
    
      
        <item>
          <title>Haskell goals</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2017-12-25-haskell-goals.jpg&quot; alt=&quot;I learnt Haskell. I can fullfill my dreams now. Hi I do Javascr... HAH! PEASANT!&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2017-12-25T07:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/haskell-goals/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/haskell-goals/</guid>
        </item>
      
    
      
        <item>
          <title>Containerize everything</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2017-12-18-containerize-everything.jpg&quot; alt=&quot;Let's put a .doc on a container. Is it a good idea? We'll also need Libreoffice in order to read it.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2017-12-18T08:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/containerize-everything/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/containerize-everything/</guid>
        </item>
      
    
      
        <item>
          <title>Beer</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2017-12-11-beer.jpg&quot; alt=&quot;Do you wanna go get a beer? I'd rather suffer excruciating pain before doing so.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2017-12-11T08:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/beer/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/beer/</guid>
        </item>
      
    
      
        <item>
          <title>We DevOps now</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2017-12-04-we-devops-now.jpg&quot; alt=&quot;I'll create a new DevOps team between Dev and Ops in order to increase inter-communication.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2017-12-04T08:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/we-devops-now/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/we-devops-now/</guid>
        </item>
      
    
      
        <item>
          <title>High performance computing</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2017-11-27-high-performance-computing.jpg&quot; alt=&quot;(Computer burns) Are you compiling the linux kernel? Nah, just running intellij.&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2017-11-27T08:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/high-performance-computing/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/high-performance-computing/</guid>
        </item>
      
    
      
        <item>
          <title>The endless struggle</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2017-11-21-the-endless-struggle.jpg&quot; alt=&quot;Vim is better than emacs.&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2017-11-21T20:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/the-endless-struggle/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/the-endless-struggle/</guid>
        </item>
      
    
      
        <item>
          <title>The Wizard: Ansible, Molecule and Test Driven Development</title>
          <description>&lt;h2 id=&quot;magic-has-existed-since-the-dawn-of-time&quot;&gt;Magic has existed since the dawn of time.&lt;/h2&gt;
&lt;p&gt;It has always been there, hidden in plain sight, making amazing things possible
for those willing to open their eyes and harness its power.&lt;/p&gt;

&lt;p&gt;You can’t remember the first time you used it, and yet it feels like you’ve
never existed without it. You’ve obliterated endless armies of enemies by
virtue of spells and enchantments, and you’ve also constructed awe-inspiring
marvels from the ground up. As a result, you’re renowned for your overwhelming
powers in every single one of your incarnations through time. Everyone knows
about &lt;strong&gt;the Wizard&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This affinity with magic grants you the ability to see the real nature of
things. Time for example, is not linear (as it was believed to be up until the
late 21st century). Sometimes, it is hard to keep up with it…&lt;/p&gt;

&lt;p&gt;Suddenly you wake up. Or you come into consciousness. What year is it?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;So yeah, the output is still red, I really don’t know what else to do.&lt;/li&gt;
  &lt;li&gt;What?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You look to your left. N is sitting right there. He looks at you with mild
concern in his eyes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/wizard/wizard-1.jpg&quot; alt=&quot;Wizard&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Are you okay?&lt;/li&gt;
  &lt;li&gt;Yeah, sorry.&lt;/li&gt;
  &lt;li&gt;As I was saying, I don’t know why this isn’t working.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You look at the spell he is trying to cast. His intention is clear: he wants to
share the work of his clan with all the sentient entities in this reality by
means of a portal.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# task file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;it-is-clear-why-he-is-failing&quot;&gt;It is clear why he is failing.&lt;/h2&gt;
&lt;p&gt;He is not taking into account the constraints of his clan’s environment. He
must first gather the requirements of his spell, and he’s not looking at the
right places. As a matter of fact, he is not looking at all. The answer is
simple. Nevertheless…&lt;/p&gt;

&lt;p&gt;You reflect for a second, wondering if someone even taught N how to properly
cast a spell.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How are you testing your role?&lt;/li&gt;
  &lt;li&gt;Hum…&lt;/li&gt;
  &lt;li&gt;Are you testing your role?&lt;/li&gt;
  &lt;li&gt;Yes, of course.&lt;/li&gt;
  &lt;li&gt;How?&lt;/li&gt;
  &lt;li&gt;I run my code on the target machine, and then I hope it doesn’t fail.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You feel betrayed by the sound of his words. This is unacceptable. A whole
stack of death spells appear in your head. Breathe, you repeat to yourself. In.
Out. You keep going.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Are you aware of the concept of Test-Driven Development?&lt;/li&gt;
  &lt;li&gt;Yeah, I used to work that way all the time when I was doing dev with the team.&lt;/li&gt;
  &lt;li&gt;So you’re familiar with it.&lt;/li&gt;
  &lt;li&gt;Sure, L taught me.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ah, a familiar name. You like L. You fought together at Desaix, before he
started mentoring his own apprentices on the arts of war. He must have taught N
a thing or two. You wonder if he will be capable of surviving the trial.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Why don’t you test your code that way instead?&lt;/li&gt;
  &lt;li&gt;Because it is not Java, it’s Ansible.&lt;/li&gt;
  &lt;li&gt;So?&lt;/li&gt;
  &lt;li&gt;It’s not the same.&lt;/li&gt;
  &lt;li&gt;Let’s start over. Do you mind if I grab the keyboard?&lt;/li&gt;
  &lt;li&gt;Not at all.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You harness your energy as you get ready to cast your first spell. You feel
alive again. You’re in familiar grounds. Before you start working, you need an
empty canvas, a playground to construct the structure and robustness of your
magic. An isolated enclosure which will allow you to try and cast your spells,
and which also follows the same rules as the target reality for the spell
you’re trying to create.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Have you ever heard of Molecule?&lt;/li&gt;
  &lt;li&gt;…hum?&lt;/li&gt;
  &lt;li&gt;It’s a Python tool. It allows you to test your Ansible code on many different levels, using local infrastructure.&lt;/li&gt;
  &lt;li&gt;Local? Like a Virtual Machine?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He’s fast. Time to make a choice. The Wanderer, or the Whale? The Whale is
indeed faster.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Not necessarily. You can use different providers. Vagrant and Docker are the
most common if you want to work locally. We’re going to use Docker, since
containers have a faster start-up time.&lt;/li&gt;
  &lt;li&gt;Right on, Docker!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Whale it is. You concentrate and cast your first incantation.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule init role --role-name nginx --driver-name docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You feel the energy flowing through. A whirlpool of blue water forms in the
sky, and The Whale appears from inside of it. It comes swimming down through
the air, looking at you both, and sings its song loudly. An empty enclosure
materializes out of thin air, trapping you and N inside of it. The Whale
disappears from the whirlpool where it came, right before the whirlpool itself
vanishes into nothingness, as fast as it had appeared.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Initializing new role nginx…

nginx/
├── README.md
├── defaults
│   └── main.yml
├── handlers
│   └── main.yml
├── meta
│   └── main.yml
├── molecule
│   └── default
│       ├── Dockerfile.j2
│       ├── INSTALL.rst
│       ├── create.yml
│       ├── destroy.yml
│       ├── molecule.yml
│       ├── playbook.yml
│       └── tests
│           └── test_default.py
├── tasks
│   └── main.yml
└── vars
    └── main.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;you-feel-the-need-to-test-n&quot;&gt;You feel the need to test N.&lt;/h2&gt;
&lt;p&gt;He’s looking at what you’ve produced, his eyebrows frowning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What do you think? It’s the standard role structure, isn’t it? - you ask, naively&lt;/li&gt;
  &lt;li&gt;Yes, with the exception of the molecule directory with all of that stuff in it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Good. He realises there’s a difference. How can you easily explain the fact
that, in order to control such an enclosure, you need intermediate spells to
both create it and destroy it at will? You decide that it is not important
right now, you’ll come back later to it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Don’t think about it yet. Let’s see if our test framework is working properly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You concentrate and you cast the spell on the enclosure/playground.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── destroy
    ├── dependency
    ├── syntax
    ├── create
    ├── converge
    ├── idempotence
    ├── lint
    ├── side_effect
    ├── verify
    └── destroy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;N is impressed by the fireworks. He has so many questions. You feel overjoyed,
but you give him a second so that he can get his ideas together.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Test matrix? Default? And what are all of those things?&lt;/li&gt;
  &lt;li&gt;When I talked about testing Ansible code on many different levels, this is
what I meant. The default scenario is what gets executed when you type
&lt;code class=&quot;highlighter-rouge&quot;&gt;molecule test&lt;/code&gt; on your terminal, and it contains all of these actions:
destroy, dependency, syntax, create, converge, idempotence…&lt;/li&gt;
  &lt;li&gt;Yeah, I can read.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;snarky&quot;&gt;Snarky.&lt;/h4&gt;
&lt;p&gt;He wants you to know he’s not a child. He contemplates the enclosure’s
structure. You wait for a while until he starts talking again.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Are these… actions executed in order?&lt;/li&gt;
  &lt;li&gt;Yes, unless you say otherwise.&lt;/li&gt;
  &lt;li&gt;What do you mean?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You decide now is the time to go down the rabbit hole.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Since I typed &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule test&lt;/code&gt;, molecule executes the default test sequence,
which corresponds to all of these actions. We can also redefine different
sequences for different targets if the default test sequence does not satisfy
our testing criteria.&lt;/li&gt;
  &lt;li&gt;Different sequences? Like skipping all tests? Or executing only… let’s say… the linting?&lt;/li&gt;
  &lt;li&gt;Sure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You raise your hand, and you snap your fingers. You hear warping sounds as the
enclosure somehow shifts. Everything looks the same, yet everything feels
different.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# molecule.yml&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;dependency&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;galaxy&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;lint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yamllint&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;platforms&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;instance&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;centos:7&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;provisioner&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ansible&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;lint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ansible-lint&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;scenario&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;test_sequence&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;lint&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;verifier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;testinfra&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;lint&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;flake8&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You take a deep breath, and you cast the spell again.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$  &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    └── lint
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'lint'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Yamllint on files found &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; /private/tmp/test/nginx/...
Lint completed successfully.
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Flake8 on files found &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; /private/tmp/test/nginx/molecule/default/tests/...
Lint completed successfully.
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Ansible Lint on /private/tmp/test/nginx/molecule/default/playbook.yml...
Lint completed successfully.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Once you’re finished, you look back at N. He gets where you’re going.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;That was faster.&lt;/li&gt;
  &lt;li&gt;Yes.&lt;/li&gt;
  &lt;li&gt;I think we should add some actions back though.&lt;/li&gt;
  &lt;li&gt;I agree. Which ones?&lt;/li&gt;
  &lt;li&gt;Let me see the first test matrix we saw.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He studies the playground spells thoroughly.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Why is the destroy action executed first?&lt;/li&gt;
  &lt;li&gt;Because the whole point is to be able to test that your role works in newly
created infrastructure.&lt;/li&gt;
  &lt;li&gt;Yeah, but it’s going to take longer if I need to create and destroy the
container each time I launch my test, right?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Good. He is connecting the dots, seeing outside of the box. You decide that you
like N a little better now. Enough to keep the game going.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Yes, but we will deal with that later. Just concentrate on the actions that
we should execute in order to fully test our role.&lt;/li&gt;
  &lt;li&gt;Okay, so then we need &lt;code class=&quot;highlighter-rouge&quot;&gt;destroy&lt;/code&gt;… what is &lt;code class=&quot;highlighter-rouge&quot;&gt;dependency&lt;/code&gt;?&lt;/li&gt;
  &lt;li&gt;It allows you to pull dependencies from the galaxy, if you need them.&lt;/li&gt;
  &lt;li&gt;Ah, let’s skip that then. What’s the difference between &lt;code class=&quot;highlighter-rouge&quot;&gt;syntax&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;lint&lt;/code&gt;?&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;syntax&lt;/code&gt; action runs your playbook using the &lt;code class=&quot;highlighter-rouge&quot;&gt;--syntax-check&lt;/code&gt; option
native in Ansible, whereas &lt;code class=&quot;highlighter-rouge&quot;&gt;lint&lt;/code&gt; checks your files using flake8, yamllint
and ansible-lint.&lt;/li&gt;
  &lt;li&gt;So &lt;code class=&quot;highlighter-rouge&quot;&gt;create&lt;/code&gt; uses the create.yml file we just saw under the molecule directory
inside the role right?&lt;/li&gt;
  &lt;li&gt;Yes. It creates a Docker container suitable for Ansible use. And &lt;code class=&quot;highlighter-rouge&quot;&gt;destroy&lt;/code&gt;
uses the destroy.yml file. They’re both playbooks, so these actions are
managed by Ansible.&lt;/li&gt;
  &lt;li&gt;Eat your own dog food, alright. So &lt;code class=&quot;highlighter-rouge&quot;&gt;converge&lt;/code&gt; executes my role on the container?&lt;/li&gt;
  &lt;li&gt;That’s right.&lt;/li&gt;
  &lt;li&gt;What about &lt;code class=&quot;highlighter-rouge&quot;&gt;side-effect&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;verify&lt;/code&gt;?&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;side-effect&lt;/code&gt; lets you produce situations in which you will be able to test
more things, like HA failover, for example. I don’t think we’ll be needing
that one for this situation. And &lt;code class=&quot;highlighter-rouge&quot;&gt;verify&lt;/code&gt; is where all the magic happens.
That’s where our unit tests get executed.&lt;/li&gt;
  &lt;li&gt;Unit tests? What?&lt;/li&gt;
  &lt;li&gt;You’ll see in a second.&lt;/li&gt;
  &lt;li&gt;Alright. So I guess we’ll need &lt;code class=&quot;highlighter-rouge&quot;&gt;destroy&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;syntax&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;lint&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;create&lt;/code&gt;,
&lt;code class=&quot;highlighter-rouge&quot;&gt;converge&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;verify&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;destroy&lt;/code&gt; again. No, wait. No destroy at the end.
That’s why it is at the beginning.&lt;/li&gt;
  &lt;li&gt;As you wish.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You notice something is missing, but you decide to keep going to take advantage
of the synergy. You will get back to it, eventually. You snap your fingers once
again, altering the enclosure to match his words.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# molecule.yml&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;scenario&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;test_sequence&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;destroy&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;syntax&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;lint&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;converge&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;verify&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You cast the spell once again, using the modified reality of the enclosure. The
fireworks appear one more time, this time slightly smaller.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$  &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── destroy
    ├── syntax
    ├── lint
    ├── create
    ├── converge
    └── verify
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;n-seems-happy&quot;&gt;N seems happy.&lt;/h3&gt;
&lt;p&gt;He understands how it works. You take advantage of his
motivation to feed him more knowledge.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This is going to act as our full test scenario. We’re not going to run all of that every single time we test though.&lt;/li&gt;
  &lt;li&gt;Hum? Why not? What are we doing instead?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Knowledge is power. Everyone knows that. If you’re able to acquire knowledge
faster, you’re able to acquire power faster as well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Because we want our feedback loop to be as short and quick as possible. So
we’re going to &lt;code class=&quot;highlighter-rouge&quot;&gt;create&lt;/code&gt; our resources, &lt;code class=&quot;highlighter-rouge&quot;&gt;converge&lt;/code&gt; them with our Ansible role,
and &lt;code class=&quot;highlighter-rouge&quot;&gt;verify&lt;/code&gt; that the results are what we expect. You can do this by typing
&lt;code class=&quot;highlighter-rouge&quot;&gt;molecule create&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule converge&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule verify&lt;/code&gt;. Then, if it all
works, we’re going to run our whole test sequence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You let that sink in for a while, hoping that he doesn’t give up. You look
outside the window. It’s raining. You like rain. At least on Earth. It’s better
than on Venus, where it melts the skin off of your bones if you don’t cast a
protection enchantment around you. That is if the temperature or the pressure
don’t get you first. N doesn’t give up. He looks at you.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Okay, I think I have it. Let’s create the container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You start summoning The Whale again, but you only hear its song this time. It
is as loud as it was before. The enclosure disappears and reappears around you.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule create
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    └── create
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'create'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You look at N, and you think about the next steps. This is going to sound
familiar to him.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Let’s run our tests. They are located on the test_default.py file, under the
molecule directory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You clap your hands once, loudly. As you separate them a stream of shiny green
light emanates from them.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;nv&quot;&gt;$molecule&lt;/span&gt; verify
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    └── verify
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'verify'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Testinfra tests found &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; /private/tmp/test/nginx/molecule/default/tests/...
    Verifier completed successfully.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Everything is okay, for now. Back to N.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are the three steps of Test-Driven Development?&lt;/li&gt;
  &lt;li&gt;Red, green, refactor.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L had done his job right.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Excellent. What are you trying to accomplish?&lt;/li&gt;
  &lt;li&gt;I want to expose my web app using nginx.&lt;/li&gt;
  &lt;li&gt;What do you need for that?&lt;/li&gt;
  &lt;li&gt;Well, I need to install nginx.&lt;/li&gt;
  &lt;li&gt;Let’s go to red then.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You spawn a glob of energy with your hands and use it to craft a rule within
the enclosure’s reality:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;testinfra.utils.ansible_runner&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;testinfra_hosts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testinfra&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ansible_runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AnsibleRunner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MOLECULE_INVENTORY_FILE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_hosts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'all'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_nginx_is_installed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Given&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nginx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;package&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nginx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Then&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nginx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_installed&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;This feels familiar.&lt;/li&gt;
  &lt;li&gt;It’s because it is.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You clap your hands together, just as you did a minute ago. Nevertheless, red
shiny light appears this time as you separate them.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule verify

&lt;span class=&quot;o&quot;&gt;===================================&lt;/span&gt; FAILURES &lt;span class=&quot;o&quot;&gt;===================================&lt;/span&gt;
_________________ test_nginx_is_installed[ansible://instance] __________________

host &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &amp;lt;testinfra.host.Host object at 0x10a9a7350&amp;gt;

    def test_nginx_is_installed&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;host&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:
        &lt;span class=&quot;c&quot;&gt;# Given&lt;/span&gt;
        nginx &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; host.package&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'nginx'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# Then&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt;       &lt;/span&gt;assert nginx.is_installed
E       assert False
E        +  where False &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &amp;lt;package nginx&amp;gt;.is_installed

tests/test_default.py:14: AssertionError
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;See? That’s red. We’re good. What’s next?&lt;/li&gt;
  &lt;li&gt;Green. So now we’ll add the task we need in order to install the server.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You close your eyes and concentrate. The structure of the spell N had in the
first place is recreated from the ground.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tasks file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Back to square one. It is now the same spell N had when he asked you for help.
You cast the spell, knowing what will happen beforehand.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;nv&quot;&gt;$molecule&lt;/span&gt; converge
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── create
    └── converge
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'create'&lt;/span&gt;
Skipping, instances already created.
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'converge'&lt;/span&gt;

    TASK &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;nginx : Install nginx] &lt;span class=&quot;k&quot;&gt;***************************************************&lt;/span&gt;
    fatal: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;instance]: FAILED! &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;changed&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;failed&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;msg&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;No package matching 'nginx' found available, installed or updated&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;rc&quot;&lt;/span&gt;: 126, &lt;span class=&quot;s2&quot;&gt;&quot;results&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;No package matching 'nginx' found available, installed or updated&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;No fireworks. N seems discouraged. You think about giving him a little push. He
deserves it. It’s not about the magic itself, it’s about context. He’s not
getting the sources for his portal from the right index. He needs the Epel
codex.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;At least it’s easier to test now.&lt;/li&gt;
  &lt;li&gt;Don’t be sad. The nginx package is contained within the epel-release repository.&lt;/li&gt;
  &lt;li&gt;What? You could have told me that from the beginning, couldn’t you?&lt;/li&gt;
  &lt;li&gt;We wouldn’t be talking about infrastructure testing if I had, would we?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He smiles. He’s getting in the game, getting the feel of the mindset. He likes
it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Let’s try it again.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You modify the structure of the spell once again, specifying that you want your
portal to be created following the instructions on the Epel codex.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tasks file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install epel release&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;epel-release&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You cast the spell, with the new structure.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule converge
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── create
    └── converge
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'create'&lt;/span&gt;
Skipping, instances already created.
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'converge'&lt;/span&gt;
…
    PLAY RECAP &lt;span class=&quot;k&quot;&gt;*********************************************************************&lt;/span&gt;
    instance                   : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3    &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The spell works, or at least it seems to work. The fireworks make the air
vibrate within the enclosure. N seems happy. He doesn’t have the right reflexes
yet. Can you blame him? It’s his first existence, and he’s just starting to
play with magic. He won’t realize that his portal is not yet active, he’s just
dazzled with the fireworks. It’s time for a new trial.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Excellent! I guess we’re done!&lt;/li&gt;
  &lt;li&gt;Yeah. Did you try it?&lt;/li&gt;
  &lt;li&gt;Oh right, let’s see. Can I login to the container somehow?&lt;/li&gt;
  &lt;li&gt;Yeah, &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule login&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@instance /]# curl localhost
curl: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;7&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; Failed to connect to ::1: Cannot assign requested address
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Hm. But… ah, right! I never started the service.&lt;/li&gt;
  &lt;li&gt;Be my guest, add the missing code needed to do it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He is about to modify the spell’s structure, when he suddenly stops. He looks
at you.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No. It’s not the right way. Can’t I test that first?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He passed the test. He might make a good disciple. You start to understand why
L started teaching in the first place.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sure, just add another test.&lt;/li&gt;
  &lt;li&gt;Alright…&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_nginx_is_running&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Given&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nginx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nginx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Then&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nginx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_running&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;I love writing tests in Python by the way. It’s way simpler than in Java.&lt;/li&gt;
  &lt;li&gt;Don’t let L hear you say that. Try it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You know the Whale won’t let N get away with that, but you want to see him try.
Failure is part of the learning process. It is almost more important than
success. It’s the only way he can learn to get up and keep trying.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule verify
    E       AssertionError: Unexpected &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;code 1 &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;CommandResult&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;u&lt;span class=&quot;s1&quot;&gt;'systemctl is-active nginx'&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;exit_status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1, &lt;span class=&quot;nv&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;u&lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;stderr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;u&lt;span class=&quot;s1&quot;&gt;'Failed to get D-Bus connection: Operation not permitted'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;N claps his hands like you did before, but no light appears as he separated his
hands. He looks as you, confused.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hm, this doesn’t look like the good kind of red. It is not an assertion
error.&lt;/li&gt;
  &lt;li&gt;It is not. Testinfra (your Python unit testing library) uses Systemd, Upstart
or SysV in order to test if the services are running. Docker containers don’t
have an init system. They do have Tini, but it doesn’t work the same way.&lt;/li&gt;
  &lt;li&gt;So I need to change the image that I’m using. Is there a Centos image that
includes Systemd?&lt;/li&gt;
  &lt;li&gt;Yeah, it’s called centos/systemd.&lt;/li&gt;
  &lt;li&gt;Go figure. Where can I modify that?&lt;/li&gt;
  &lt;li&gt;On the molecule.yml file. You will need to specify the container’s command
instruction in order to start Systemd, because Molecule overrides it by
default. You will also need a privileged container, since Systemd requires
CAP_SYS_ADMIN and non-privileged containers do not have that capability.&lt;/li&gt;
  &lt;li&gt;Hum, I didn’t quite get all of that, but I think I’ll ask you about it again
tomorrow. Let me redo the configuration.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;platforms&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;instance&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;centos/systemd&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;privileged&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/usr/sbin/init&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The Whale will surely accept this offering. You know it will not reject your
summoning anymore. N verifies the spell again.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;===================================&lt;/span&gt; FAILURES &lt;span class=&quot;o&quot;&gt;===================================&lt;/span&gt;
____________ test_nginx_is_running_and_enabled[ansible://instance] _____________

host &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &amp;lt;testinfra.host.Host object at 0x110adff50&amp;gt;

    def test_nginx_is_running&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;host&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:
        &lt;span class=&quot;c&quot;&gt;# Given&lt;/span&gt;
        nginx &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; host.service&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'nginx'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# Then&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;&amp;gt;       &lt;/span&gt;assert nginx.is_running
E       assert False
E        +  where False &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &amp;lt;service nginx&amp;gt;.is_running

tests/test_default.py:20: AssertionError

-- Docs: http://doc.pytest.org/en/latest/warnings.html
&lt;span class=&quot;o&quot;&gt;================&lt;/span&gt; 1 failed, 1 passed, 1 warnings &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;6.74 seconds &lt;span class=&quot;o&quot;&gt;================&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Done, we’re back to red. Now I’ll make the changes in order to start the
service.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You smile as you see N craft. He is not ready yet, but he is well under way.
You feel proud of him.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Alright, I’m done. What do you think?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He shows you the structure of his spell.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tasks file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install epel release&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;epel-release&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;systemctl start nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;All of your previously found pride turns into disappointment. This is
preposterous. It can’t be serious. He can’t be serious. He should understand
that you can’t be spawning portals indefinitely. The results could be
catastrophic.&lt;/p&gt;

&lt;p&gt;But then…you contemplate him. You realize he’s tired. Idempotence is a hard
concept to grasp too. Benjamin Peirce taught it for over 50 years at Harvard.
You remember the first time Merlin brought you to an enclosure like this one.
Just the memory of it makes you feel goosebumps on the back of your neck.&lt;/p&gt;

&lt;p&gt;You decide that you’re going to let him continue, for education purposes only.
He’ll realize the problem with his proposal.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Why don’t you try it?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He casts the spell himself.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule converge
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── create
    └── converge
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'create'&lt;/span&gt;
Skipping, instances already created.
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'converge'&lt;/span&gt;

    PLAY RECAP &lt;span class=&quot;k&quot;&gt;*********************************************************************&lt;/span&gt;
    instance                   : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4    &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Right after the fireworks, a green portal appears right in front of both of you. It seems to work.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Great. I’ll verify now.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule verify
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    └── verify
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'verify'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Testinfra tests found &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; /Users/sebiwi/stuff/test/nginx/molecule/default/tests/...
    &lt;span class=&quot;o&quot;&gt;=============================&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test &lt;/span&gt;session starts &lt;span class=&quot;o&quot;&gt;==============================&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;=====================&lt;/span&gt; 2 passed, 1 warnings &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;6.70 seconds &lt;span class=&quot;o&quot;&gt;=====================&lt;/span&gt;
Verifier completed successfully.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Green light everywhere.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Perfect! Green!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Or is it? You decide to give him a little push in the right direction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Try running the whole test suite.&lt;/li&gt;
  &lt;li&gt;Alright&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$molecule&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── destroy
    ├── syntax
    ├── lint
    ├── create
    ├── converge
    └── verify

&lt;span class=&quot;gp&quot;&gt;---&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'lint'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Yamllint on files found &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; /Users/sebiwi/stuff/test/nginx/...
Lint completed successfully.
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Flake8 on files found &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; /Users/sebiwi/stuff/test/nginx/molecule/default/tests/...
Lint completed successfully.
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Ansible Lint on /Users/sebiwi/stuff/test/nginx/molecule/default/playbook.yml...
    &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ANSIBLE0012] Commands should not change things &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;nothing needs doing
    /Users/sebiwi/stuff/test/nginx/tasks/main.yml:14
    Task/Handler: Start nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;No fireworks, no lights.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What? Oh, that’s just ansible-lint, it often fails for that sort of thing,
I’ll just deactivate it for the task.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You stare in disbelief as N “corrects” the structure of the spell.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tasks file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install epel release&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;epel-release&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;systemctl start nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;skip_ansible_lint&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;He finishes the modifications and he casts the spell once again.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$  &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── destroy
    ├── syntax
    ├── lint
    ├── create
    ├── converge
    └── verify

All steps completed successfully.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;He looks at you proudly. You’re dismayed. He’s being naive and reckless. You
realize that it is time to handle the situation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We’re green now. I think we’re finished.&lt;/li&gt;
  &lt;li&gt;Not really. Previously, ansible-lint said that you’re changing things even if
nothing needs to be done.&lt;/li&gt;
  &lt;li&gt;Yeah, it’s because I was starting a service.&lt;/li&gt;
  &lt;li&gt;Yes, but since you’re not using an Ansible module, you’re executing an action
each time you run your playbook.&lt;/li&gt;
  &lt;li&gt;I thought Ansible was idempotent.&lt;/li&gt;
  &lt;li&gt;Not really. You can make idempotent code with Ansible. It doesn’t mean
everything you do with it is going to be idempotent. Some things are not
meant to be idempotent.&lt;/li&gt;
  &lt;li&gt;Like what?&lt;/li&gt;
  &lt;li&gt;Application deployments, for example. You’re deploying a new version of the
application. It is meant to change things.&lt;/li&gt;
  &lt;li&gt;I see. But this role should be idempotent.&lt;/li&gt;
  &lt;li&gt;Certainly. You can add a new action to your default scenario test suite in
order to test for idempotence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You close your eyes as you change the reality of the enclosure around you once again.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;scenario&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;test_sequence&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;destroy&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;syntax&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;lint&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;converge&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;idempotence&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;verify&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You cast the spell again.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$  &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── destroy
    ├── syntax
    ├── lint
    ├── create
    ├── converge
    ├── idempotence
    └── verify

&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'idempotence'&lt;/span&gt;
ERROR: Idempotence &lt;span class=&quot;nb&quot;&gt;test &lt;/span&gt;failed because of the following tasks:
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;instance] &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;gt; nginx : Start nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;N looks at you.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;So we weren’t actually at green yet.&lt;/li&gt;
  &lt;li&gt;We weren’t. Idempotence is a behaviour of our code. It also needs testing.&lt;/li&gt;
  &lt;li&gt;I can fix it though.&lt;/li&gt;
  &lt;li&gt;Show me.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You hand over the control of the enclosure to N. He modifies the spell
structure.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tasks file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install epel release&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;epel-release&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;started&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You smile with relief.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Much better. Try it.&lt;/li&gt;
  &lt;li&gt;Right away.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$  &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── destroy
    ├── syntax
    ├── lint
    ├── create
    ├── converge
    ├── idempotence
    └── verify

All steps completed successfully.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;And now we’re green.&lt;/li&gt;
  &lt;li&gt;Yes!&lt;/li&gt;
  &lt;li&gt;Let’s continue with our TDD approach. Red, Green, and… ?&lt;/li&gt;
  &lt;li&gt;Refactor! Do you see any possible improvements on this code?&lt;/li&gt;
  &lt;li&gt;Well, you could use your “Start nginx” task as a handler, which is triggered
by the installation of nginx, couldn’t you?&lt;/li&gt;
  &lt;li&gt;Yes, I could.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tasks file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install epel release&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;epel-release&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;notify&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# handlers file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;started&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$  &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── destroy
    ├── syntax
    ├── lint
    ├── create
    ├── converge
    ├── idempotence
    └── verify

All steps completed successfully.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;n-looks-euphoric&quot;&gt;N looks euphoric.&lt;/h3&gt;

&lt;p&gt;He’s applying his well-earned war experience to a whole different situation,
and it is working like a charm. There’s a slight catch within your proposal,
but you decide that you will let him discover it by himself.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Wow, that worked out great! What else?&lt;/li&gt;
  &lt;li&gt;You could have only one “yum” task and iterate over the packages that you want to install.&lt;/li&gt;
  &lt;li&gt;Good call.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tasks file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install epel-release and nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;yum&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;present&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;with_items&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;epel-release&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;notify&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
...
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'converge'&lt;/span&gt;

    PLAY &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Converge] &lt;span class=&quot;k&quot;&gt;****************************************************************&lt;/span&gt;

    TASK &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Gathering Facts] &lt;span class=&quot;k&quot;&gt;*********************************************************&lt;/span&gt;
    ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;instance]

    TASK &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;nginx : Install epel-release and nginx] &lt;span class=&quot;k&quot;&gt;**********************************&lt;/span&gt;
    failed: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;instance] &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=[&lt;/span&gt;u&lt;span class=&quot;s1&quot;&gt;'epel-release'&lt;/span&gt;, u&lt;span class=&quot;s1&quot;&gt;'nginx'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;changed&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;failed&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;item&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;epel-release&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;nginx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;msg&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;No package matching 'nginx' found available, installed or updated&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;rc&quot;&lt;/span&gt;: 126, &lt;span class=&quot;s2&quot;&gt;&quot;results&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;No package matching 'nginx' found available, installed or updated&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]}&lt;/span&gt;

    PLAY RECAP &lt;span class=&quot;k&quot;&gt;*********************************************************************&lt;/span&gt;
    instance                   : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1    &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Hm. I guess I need to install epel-release before I can install nginx. It
doesn’t work if I do both at the same time.&lt;/li&gt;
  &lt;li&gt;True.&lt;/li&gt;
  &lt;li&gt;I wouldn’t had realized that without the tests. I would have done my
refactoring, and it would have worked, since epel-release would already be
installed on my machine when trying to install it along with nginx in a single
task.&lt;/li&gt;
  &lt;li&gt;Indeed. Also, with the current code structure, you’re not only starting nginx
after installing nginx, but also after installing epel-release.&lt;/li&gt;
  &lt;li&gt;Right. I shouldn’t. That was a test, wasn’t it?&lt;/li&gt;
  &lt;li&gt;Maybe.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N transforms the spell structure back to its previous form. You feel relaxed.
You always feel this way after creating something impressive. You feel the same
way you felt when you structured the pyramids, only to a minor extent. This
time, however, the actual creator is N. You only acted as a catalyzer. The
knowledge was always inside of N, he just needed to hear the right questions.&lt;/p&gt;

&lt;p&gt;N looks tired, but he wants to keep going.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We just went through a full Red/Green/Refactor cycle. Sweet!&lt;/li&gt;
  &lt;li&gt;Do you want to do another one?&lt;/li&gt;
  &lt;li&gt;Absolutely!&lt;/li&gt;
  &lt;li&gt;What happens if your VM gets rebooted?&lt;/li&gt;
  &lt;li&gt;Nothing, nginx will still be installed.&lt;/li&gt;
  &lt;li&gt;Will it be running?&lt;/li&gt;
  &lt;li&gt;Oh, no. It won’t. Yikes.&lt;/li&gt;
  &lt;li&gt;You should enable it then. You know what to do.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N creates a new rule within the enclosure’s reality.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test_nginx_is_enabled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# Given&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nginx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nginx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Then&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nginx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_enabled&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;He claps his hands together. Red light appears everywhere.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;molecule verify
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    └── verify
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Scenario: &lt;span class=&quot;s1&quot;&gt;'default'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Action: &lt;span class=&quot;s1&quot;&gt;'verify'&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing Testinfra tests found &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; /Users/sebiwi/stuff/test/nginx/molecule/default/tests/…

    &lt;span class=&quot;o&quot;&gt;===================================&lt;/span&gt; FAILURES &lt;span class=&quot;o&quot;&gt;===================================&lt;/span&gt;
    __________________ test_nginx_is_enabled[ansible://instance] ___________________

    host &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &amp;lt;testinfra.host.Host object at 0x103b20fd0&amp;gt;

       def test_nginx_is_enabled&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;host&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:
            &lt;span class=&quot;c&quot;&gt;# Given&lt;/span&gt;
            nginx &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; host.service&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'nginx'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# Then&lt;/span&gt;
    &amp;gt;       assert nginx.is_enabled
    E       assert False
    E        +  where False &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &amp;lt;service nginx&amp;gt;.is_enabled

    tests/test_default.py:28: AssertionError
    -- Docs: http://doc.pytest.org/en/latest/warnings.html
    &lt;span class=&quot;o&quot;&gt;================&lt;/span&gt; 1 failed, 2 passed, 1 warnings &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;7.29 seconds &lt;span class=&quot;o&quot;&gt;================&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Red.&lt;/li&gt;
  &lt;li&gt;Go for Green.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N modifies the structure of the spell to match the new reality constraints.
He’s got a clear head now, he knows what is missing.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# handlers file for nginx&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Start nginx&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;started&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$  &lt;/span&gt;molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Test matrix
└── default
    ├── destroy
    ├── syntax
    ├── lint
    ├── create
    ├── converge
    ├── idempotence
    └── verify

All steps completed successfully.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The green portal appears in front of you both. You know it is sustainable, you
both tested it thoroughly. It’s a proper wizard spell. People would say it is
one of your own. N looks at you and smiles.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Green.&lt;/li&gt;
  &lt;li&gt;Yes. Can you refactor?&lt;/li&gt;
  &lt;li&gt;I don’t think so. It looks pretty clean to me.&lt;/li&gt;
  &lt;li&gt;I agree.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;you-do-agree&quot;&gt;You do agree.&lt;/h3&gt;
&lt;p&gt;N seems in charge. He needs more time to develop himself, to
learn, to become comfortable with this whole new world. You won’t be as useful
during that period as you were now. It’s time to go.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You know how to continue, right?&lt;/li&gt;
  &lt;li&gt;Yes, I think I can manage. This is pretty cool. Thanks.&lt;/li&gt;
  &lt;li&gt;No problem kid. Just call me if you need more help.&lt;/li&gt;
  &lt;li&gt;Will do. Hey, do you..?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N looks at where you used to be 10 seconds ago. You are no longer there.&lt;/p&gt;
</description>
          <pubDate>2017-11-20T08:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/blog/the-wizard-1/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/the-wizard-1/</guid>
        </item>
      
    
      
        <item>
          <title>Good ideas</title>
          <description>&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/comics/2017-11-16-good-ideas.jpg&quot; alt=&quot;Maybe we should delete /var. Ok.&quot; /&gt;&lt;/p&gt;
</description>
          <pubDate>2017-11-16T20:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/comics/good-ideas/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/comics/good-ideas/</guid>
        </item>
      
    
      
        <item>
          <title>Ansible reporting with ARA: Ansible Run Analysis</title>
          <description>&lt;p&gt;I had a Slack conversation with some friends from work (&lt;strong&gt;B&lt;/strong&gt;, &lt;strong&gt;R&lt;/strong&gt; and &lt;strong&gt;G&lt;/strong&gt;)
the other day. It went somewhat like this:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;: &lt;em&gt;“Hey, have you guys ever heard of &lt;a href=&quot;https://ara.readthedocs.io/en/latest/&quot;&gt;ARA&lt;/a&gt;?”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This was followed by 5 minutes of the deafening sound of contextualisation and
Google searches by everyone on the channel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: &lt;em&gt;“Nope, but every piece of technology that could enable Ansible to catch
up with PuppetDB/Board would be very much appreciated”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;G&lt;/strong&gt;: &lt;em&gt;“Reporting is cheating”&lt;/em&gt; (This statement makes way more sense in French)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Me&lt;/strong&gt;: &lt;em&gt;“It looks dope, it’s way better than scrolling through the output of a
Jenkins job and you won’t be needing things like profilers anymore. I’ll try it
and I’ll tell you if it sucks or not”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So this is it.&lt;/p&gt;

&lt;h2 id=&quot;what-is-ara&quot;&gt;What is ARA?&lt;/h2&gt;

&lt;p&gt;ARA is an acronym. It means &lt;a href=&quot;https://ara.readthedocs.io/en/latest/&quot;&gt;Ansible Run Analysis&lt;/a&gt;. It provides you with a
detailed analysis of your Ansible playbook runs, which includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tasks description, status and output&lt;/li&gt;
  &lt;li&gt;Play description and duration&lt;/li&gt;
  &lt;li&gt;Hosts involved&lt;/li&gt;
  &lt;li&gt;Files executed&lt;/li&gt;
  &lt;li&gt;Parameters used&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/ara/dashboard-basic.png&quot; alt=&quot;ARA Dashboard&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;ARA Dashboard&lt;/figcaption&gt;

&lt;p&gt;And a bunch of other cool things. We will take a look into that in a second.
Yeah, really.&lt;/p&gt;

&lt;h2 id=&quot;how-does-it-work&quot;&gt;How does it work?&lt;/h2&gt;

&lt;p&gt;Basically ARA is an Ansible callback. It records everything related to a
playbook’s execution into a database, which is SQLite by default. That’s it. &lt;a href=&quot;https://en.wikipedia.org/wiki/KISS_principle&quot;&gt;As
simple as possible&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/ara/ara-callback.png&quot; alt=&quot;ARA Callback&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;ARA Dashboard&lt;/figcaption&gt;

&lt;p&gt;At least that’s the most important part. But also included with ARA is a CLI
which allows you to query the aforementioned database interactively, a web
interface coded in Flask which allows you to check out all of your data
dynamically, and two Ansible modules: &lt;a href=&quot;https://ara.readthedocs.io/en/latest/usage.html#ara-record&quot;&gt;ara_record&lt;/a&gt; and &lt;a href=&quot;https://ara.readthedocs.io/en/latest/usage.html#ara-read&quot;&gt;ara_read&lt;/a&gt;. I’ll talk about
these two later.&lt;/p&gt;

&lt;h2 id=&quot;how-to-install-it&quot;&gt;How to install it?&lt;/h2&gt;

&lt;p&gt;It depends on what you want to do with it, really.&lt;/p&gt;

&lt;p&gt;You can have a simple local database for testing purposes, or if you’re working
on a project by yourself and you feel like recording everything you’re doing for
further analysis. If that’s your use case, then the process is really simple.
You just need to install the ARA dependencies:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# RHEL/CentOS&lt;/span&gt;
yum -y install gcc python-devel libffi-devel openssl-devel redhat-rpm-config

&lt;span class=&quot;c&quot;&gt;# Ubuntu&lt;/span&gt;
apt-get -y install gcc python-dev libffi-dev libssl-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And then install it using pip:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install ara
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will create a .ara directory at the user’s home, with an SQLite database
inside of it, as well as a logfile. Then, in order to launch the dashboard, just
use some &lt;code class=&quot;highlighter-rouge&quot;&gt;ara-manage&lt;/code&gt; magic:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ara-manage runserver
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will launch the development server which will listen on port 9191 by
default. You can specify different IP and port settings by using the &lt;code class=&quot;highlighter-rouge&quot;&gt;-i&lt;/code&gt; and the
&lt;code class=&quot;highlighter-rouge&quot;&gt;-p&lt;/code&gt; options, respectively. After this you’re all set, and you can go and see all
your beautiful dashboards on the web interface.&lt;/p&gt;

&lt;p&gt;Then again, you’ll probably want your dashboard to be accessible at all times if
you’re working on a serious project. What I did was that I installed ARA in the
same machine where I installed the Jenkins server, I created a systemd unit file
in order to launch the dashboard as a service, and I exposed it using nginx.
Simple.&lt;/p&gt;

&lt;p&gt;The ARA installation part remains the same for this case, unless you want to use
MySQL or PostgreSQL instead of SQLite. I didn’t. &lt;a href=&quot;http://ara.readthedocs.io/en/latest/configuration.html#ara-database&quot;&gt;Here’s a guide of the schemas
you’ll need to create and the extra configuration variables you will need to add
in that case&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your service file will look somewhat like this one:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=ara
After=network.target

[Service]
User=jenkins
Type=simple
TimeoutStartSec=0
Restart=on-failure
RestartSec=10
RemainAfterExit=yes
ExecStart=/usr/bin/ara-manage runserver -h
0.0.0.0 -p 9191

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can change the IP and the port by modifying the ExecStart instruction on the
Service namespace of the file. Note that I’m starting the process as the Jenkins
user, since it’s the same user that will be running the Ansible playbooks.
Moving on.&lt;/p&gt;

&lt;p&gt;For nginx, you get something like this:&lt;/p&gt;

&lt;div class=&quot;language-nginx highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;access_log&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;/var/log/nginx/access_ara_ssl.log.json&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;access_log&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;/var/log/nginx/access_ara_ssl.log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;error_log&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;/var/log/nginx/error_ara_ssl.log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;kn&quot;&gt;listen&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;443&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;ssl&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;ssl_certificate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;/etc/nginx/certificats/ara.mycert.crt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;ssl_certificate_key&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;/etc/nginx/certificats/ara.mykey.key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;server_name&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ara.amazing.io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;kn&quot;&gt;location&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kn&quot;&gt;proxy_pass&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http://jenkins-ip:9191&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And that’s it. Your service will be accessible on the host. You can also use
Apache if you want, or any software capable of working as a reverse proxy for
that matter.&lt;/p&gt;

&lt;p&gt;Finally, you also need to tell Ansible to use the ARA callback. This needs to be
done no matter which installation option you choose. It is fairly simple though.
You just need to add the following lines to your ansible.cfg file:&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;[defaults]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Callback
&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;callback_plugins&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$ara_location/plugins/callbacks&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# ara_record and ara_read modules
&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;action_plugins&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$ara_location/plugins/actions&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;library&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$ara_location/plugins/modules&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The next time an Ansible playbook is executed, the ARA callback will connect to
the SQLite database located on the .ara directory of the user’s home directory.
You can also configure these as environment variables. Just in case (shrugs).&lt;/p&gt;

&lt;h2 id=&quot;playing-with-ara&quot;&gt;Playing with ARA&lt;/h2&gt;

&lt;p&gt;Let’s talk about the Ansible modules and the CLI.&lt;/p&gt;

&lt;p&gt;The modules are called ara_record and ara_read. These allow you to do some cool
things. The first one, ara_record, allows you to record data using a key/value
format, just like you would do with an Ansible fact.  In other words, something
like this:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Record useless value ara_record&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;“all_your_base”&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;“are belong to us”&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Which will translate into a record on the ARA database, or something like this
on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/ara/dashboard-record.png&quot; alt=&quot;Dashboard record&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Record on the dashboard&lt;/figcaption&gt;

&lt;p&gt;You can also record things using the ad-hoc command directly, like so:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ansible localhost -m ara_record                   &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-a &lt;span class=&quot;s2&quot;&gt;&quot;playbook=7ab3858f-5d17-48f1-bb94-19ccdec0c983 &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
key=all_your_base                                 &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
value='are belong to us'&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can obtain the playbook ID using the ARA CLI, with &lt;code class=&quot;highlighter-rouge&quot;&gt;ara playbook list&lt;/code&gt;. A
cool thing about this is that you can actually record data once the playbook is
already done, if you have the playbook’s id.&lt;/p&gt;

&lt;p&gt;The ara_read module, on the other hand, lets you read (duh) data that has
previously been recorded &lt;strong&gt;on the same playbook run&lt;/strong&gt;. This is interesting, because
it enables you to use it across plays as long as you stay within the playbook’s
scope. Thus, you can record a value and then use the same value on a different
play, like an ID, an IP, the output of a command, or almost anything you can
think of.&lt;/p&gt;

&lt;p&gt;Honestly, I don’t see any amazing use cases for this module. I think that if
you’re using this as a key element in order to pass data between your plays,
you’re basically structuring your playbooks/roles/plays the wrong way. But
that’s just me.&lt;/p&gt;

&lt;p&gt;Finally, the web interface is quite cool since it allows you to filter
information by hostname, keyword, or status. This is particularly useful when
you’re trying to see the changed, or failed tasks of a playbook, or when you’re
trying to see all the tasks linked to a certain role or middleware.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/ara/search-elastic.png&quot; alt=&quot;Search elastic&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Full text search&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/ara/search-failed.png&quot; alt=&quot;Search failed&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Search by status&lt;/figcaption&gt;

&lt;h3 id=&quot;alright-lets-recap&quot;&gt;Alright, let’s recap.&lt;/h3&gt;

&lt;h4 id=&quot;its-great-because&quot;&gt;It’s great because:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It helps you troubleshooting your playbooks easily. You can search the
information you want to find on the dashboard, instead of browsing through the
extremely long output of a Jenkins job trying to figure out what went wrong&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It’s super easy to install it locally, and relatively easy to do it on a dedicated
machine, exposed through a web server&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is under active development by the
RedHat team, which probably means that we will be seeing some new features soon&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can use the CLI or query the database yourself if you don’t like dashboards
that much. You can also build your own dashboard by querying the database if you
don’t like Flask. You probably won’t do this.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;its-not-that-great-because&quot;&gt;It’s not that great because:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If you use it with a Continuous Integration server, such as Jenkins, you’ll need
to check two different web interfaces in order to realize what is going on with
your jobs. This is far from ideal. You can also generate static reports in HTML
format, but you’ll also need a web-server in order to see them, which is a
little bit annoying.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is no built-in database suppression purge politique.
So far ARA has been running for two weeks, and we’ve got 122MB of data on a
heavy-usage Ansible platform. This isn’t huge by any standards, but I’d still
like to be able to delete old data automatically, without having to create a
custom SQL query in order to delete the oldest playbook executions. Maybe this
would go against the whole “simple is better” principle that seems to guide ARA
development.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I really liked ARA. I think I’ll install it on every Ansible project I work on
from now on. It’s a quick-win in any case.&lt;/p&gt;

&lt;p&gt;I’d love to see better integration with Continuous Integration servers, or a
simpler workflow in order to work with them. Maybe I just haven’t found it yet.&lt;/p&gt;

&lt;p&gt;Big up to &lt;a href=&quot;https://dmsimard.com/about&quot;&gt;David&lt;/a&gt; for his work on ARA. Keep up the good work!&lt;/p&gt;

</description>
          <pubDate>2017-10-30T20:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/blog/ara/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/ara/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Docker! Episode 5 - Get some work(ers) done!</title>
          <description>&lt;h2 id=&quot;where-are-my-workers-man&quot;&gt;Where are my Workers man?&lt;/h2&gt;

&lt;p&gt;Right, workers.&lt;/p&gt;

&lt;p&gt;Same procedure as before: main, configure and test.&lt;/p&gt;

&lt;p&gt;In order to configure the Worker node, we will first check if Swarm
mode is already activated, like we did on the previous roles. If it
is, we won’t do squat. If it isn’t, we will go fetch the token needed
to join the cluster as a Worker node from the Leader node, and then
join the cluster as a Worker node. You should know that the token used
in order to join the cluster as a Worker node is different from the
one user to join it as a non-Leader Manager node. So:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if Swarm Mode is already activated&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker_info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Recover Swarm Leader token&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker swarm join-token worker | grep token | cut -d ' ' -f 6&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;worker_token&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Swarm:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;delegate_to&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Join Swarm Cluster as Worker&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker swarm join --token&lt;/span&gt;  
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Swarm:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Once again, the delegate_to flag is needed in order to actually
recover the token from the Leader node itself. Then, for the test
part:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if node is Worker&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker node ls | grep&lt;/span&gt; 
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker_info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;delegate_to&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Fail if node is not Worker&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;that&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reachable'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Leader'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is quite similar to the previous test, in the way that first,
information is collected form the Leader node. Then, the assertions
are slightly different: we will test that the node is not a Leader nor
a non-Leader manager node, therefore asserting that it is a Worker
node, and then we will test if the node is Active, since we want our
Worker to work, by running containers. That’s why they’re Workers,
right?&lt;/p&gt;

&lt;p&gt;Once we’re done with that, we’ll just add our newly created role to
the swam.yml file:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Create Swarm Worker nodes&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-worker&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;configure/swarm-worker&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;swarm-worker&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Nothing special right here, just use the previously defined
swarm-worker group and you’re all set.&lt;/p&gt;

&lt;h2 id=&quot;did-it-work-then&quot;&gt;Did it work then?&lt;/h2&gt;

&lt;p&gt;Sure it did. We tested manually. Oh right, we also did that molecule
thingy at the beginning, didn’t we? Now, testing is something that
should be done at every step of the way, but I’ll just show you now
that everything we coded works, is idempotent and it’s syntax is
valid.&lt;/p&gt;

&lt;p&gt;Use the test target from the Makefile:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will launch the whole molecule testing pipeline, first checking
if the virtual machines are already created, and checking the validity
of the playbook’s syntax as well:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sebiwi ~ molecule &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Destroying instances...
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Checking playbook&lt;span class=&quot;s1&quot;&gt;'s syntax…
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If it is, it will then proceed to create the instances:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;playbook: swarm.yml
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Creating instances...
Bringing machine &lt;span class=&quot;s1&quot;&gt;'swarm-manager-01'&lt;/span&gt; up with &lt;span class=&quot;s1&quot;&gt;'virtualbox'&lt;/span&gt; provider...
Bringing machine &lt;span class=&quot;s1&quot;&gt;'swarm-manager-02'&lt;/span&gt; up with &lt;span class=&quot;s1&quot;&gt;'virtualbox'&lt;/span&gt; provider...
Bringing machine &lt;span class=&quot;s1&quot;&gt;'swarm-manager-03'&lt;/span&gt; up with &lt;span class=&quot;s1&quot;&gt;'virtualbox'&lt;/span&gt; provider...
Bringing machine &lt;span class=&quot;s1&quot;&gt;'swarm-worker-01'&lt;/span&gt; up with &lt;span class=&quot;s1&quot;&gt;'virtualbox'&lt;/span&gt; provider...
Bringing machine &lt;span class=&quot;s1&quot;&gt;'swarm-worker-02'&lt;/span&gt; up with &lt;span class=&quot;s1&quot;&gt;'virtualbox'&lt;/span&gt; provider...
Bringing machine &lt;span class=&quot;s1&quot;&gt;'swarm-worker-03'&lt;/span&gt; up with &lt;span class=&quot;s1&quot;&gt;'virtualbox'&lt;/span&gt; provider...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Once they are up, the playbook itself will be launched on the newly
created infrastructure:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Starting Ansible Run...

PLAY &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Bootstrap coreos hosts] &lt;span class=&quot;k&quot;&gt;**************************************************&lt;/span&gt;

TASK &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;bootstrap/ansible-bootstrap : Check &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;Python is installed] &lt;span class=&quot;k&quot;&gt;**************&lt;/span&gt;
fatal: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;swarm-manager-01]: FAILED! &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;changed&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;failed&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;rc&quot;&lt;/span&gt;: 127, &lt;span class=&quot;s2&quot;&gt;&quot;stderr&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;Warning: Permanently added '[127.0.0.1]:2222' (ECDSA) to the list of known hosts.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Shared connection to 127.0.0.1 closed.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;stdout&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/bin/sh: /home/core/bin/python: No such file or directory&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;stdout_lines&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/bin/sh: /home/core/bin/python: No such file or directory&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]}&lt;/span&gt;
...ignoring
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If everything works fine, another idempotence test will be executed,
which will just verify if there are any changes when the playbook is
ran using the –dry-run option:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PLAY RECAP &lt;span class=&quot;k&quot;&gt;*********************************************************************&lt;/span&gt;
swarm-manager-01           : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;15   &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
swarm-manager-02           : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;16   &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
swarm-manager-03           : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;16   &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
swarm-worker-01            : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;16   &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
swarm-worker-02            : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;16   &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
swarm-worker-03            : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;16   &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6    &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0    &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0

&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Idempotence &lt;span class=&quot;nb&quot;&gt;test &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;progress &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;can take a few minutes&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;...
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Starting Ansible Run...
Idempotence &lt;span class=&quot;nb&quot;&gt;test &lt;/span&gt;passed.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Finally, ansible-lint is executed in order to verify the playbook
style and usage of deprecated tasks/options, and then the
infrastructure is destroyed:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Executing ansible-lint...
&lt;span class=&quot;gp&quot;&gt;--&amp;gt; &lt;/span&gt;Destroying instances...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-worker-03: Forcing shutdown of VM...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-worker-03: Destroying VM and associated drives...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-worker-02: Forcing shutdown of VM...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-worker-02: Destroying VM and associated drives...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-worker-01: Forcing shutdown of VM...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-worker-01: Destroying VM and associated drives...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-manager-03: Forcing shutdown of VM...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-manager-03: Destroying VM and associated drives...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-manager-02: Forcing shutdown of VM...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-manager-02: Destroying VM and associated drives...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-manager-01: Forcing shutdown of VM...
&lt;span class=&quot;gp&quot;&gt;==&amp;gt; &lt;/span&gt;swarm-manager-01: Destroying VM and associated drives...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now, what I basically do when running these tests is that I run just
the &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule create&lt;/code&gt; to create the infrastructure, and then I’ll just
run &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule converge&lt;/code&gt; to test that my roles are working properly and
&lt;code class=&quot;highlighter-rouge&quot;&gt;molecule idempotence&lt;/code&gt; to verify that they are indeed idempotent. This
helps reducing the duration of the feedback loop, which in turn helps
me to develop faster. Just remember to launch the whole pipeline from
time to time to check if your roles are able to correctly configure
newly-created infrastructure.&lt;/p&gt;

&lt;h2 id=&quot;lets-play&quot;&gt;Let’s play!&lt;/h2&gt;

&lt;p&gt;So if you followed all the steps correctly you should have a working
Swarm cluster by now. Congratulations! Let’s see what it is capable
of.&lt;/p&gt;

&lt;p&gt;First, the smallest schedulable unit of work in a Swarm cluster is not
a container, but a service. Let us create one of those on the leader
node:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker service create --replicas 1 --name redis --update-delay 10s redis:3.0.6
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This says that we want to create a service, with one replica, with the
name of redis, with a 10 second update delay, using the 3.0.6 version
of the redis image. The update delay is the time between updates of
tasks (containers) of a service. This means that the tasks will be
updated one at the time, with a 10 second delay between them. You can
then list your services using the &lt;code class=&quot;highlighter-rouge&quot;&gt;ls&lt;/code&gt; command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;core@swarm-manager-01 ~ $ docker service ls
ID            NAME   REPLICAS  IMAGE        COMMAND
09j27f6ehaq6  redis  0/1       redis:3.0.6
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And see information regarding the different tasks of the service using
the &lt;code class=&quot;highlighter-rouge&quot;&gt;ps&lt;/code&gt; command, with the service name:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;core@swarm-manager-01 ~ $ docker service ps redis
ID                         NAME     IMAGE        NODE             DESIRED STATE  CURRENT STATE           ERROR
06cj3g824k8r0jjpoew0uip7z  redis.1  redis:3.0.6  swarm-worker-03  Running        Running 42 seconds ago
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can see the image, the desired state, the current state and the
node in which the container is running.&lt;/p&gt;

&lt;p&gt;You can also scale up/down your services, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;scale&lt;/code&gt; command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;core@swarm-manager-01 ~ $ docker service scale redis=11
redis scaled to 11
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=s9F5fhJQo34&amp;amp;t=2m&quot;&gt;This scales your nodes up to 11&lt;/a&gt;. Sick!&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;core@swarm-manager-01 ~ $ docker service ps redis
ID                         NAME          IMAGE        NODE             DESIRED STATE  CURRENT STATE            ERROR
06cj3g824k8r0jjpoew0uip7z  redis.1       redis:3.0.6  swarm-worker-03  Running        Running 8 minutes ago
5wa862swszkvklchaug02powy  redis.2       redis:3.0.6  swarm-worker-02  Running        Running 25 seconds ago
67w0vlk9v7gh9h5qgwmsnjgya  redis.3       redis:3.0.6  swarm-worker-01  Running        Running 25 seconds ago
3ws3a9xwt1h4r962gg8htiun8  redis.4       redis:3.0.6  swarm-worker-03  Running
...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can use the same command with a different number in order to scale
down (to 1, for exampe).&lt;/p&gt;

&lt;p&gt;Let’s try to update a service in order to see the rolling updates
work. We’re going to go from redis version 3.0.6 to 3.0.7. Exciting,
huh? For this, we will use the &lt;code class=&quot;highlighter-rouge&quot;&gt;update&lt;/code&gt; command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker service update --image redis:3.0.7 redis
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will launch the rolling update process. It will take some time
due to the update delay we set before. If you launch a &lt;code class=&quot;highlighter-rouge&quot;&gt;ps&lt;/code&gt; command on
the service, you should be able to see your containers updating:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;core@swarm-manager-01 ~ $ docker service ps redis
ID                         NAME          IMAGE        NODE             DESIRED STATE  CURRENT STATE            ERROR
...
e8lf3q9ic8674fba8a863ciwh  redis.5       redis:3.0.7  swarm-worker-02  Running        Running 22 seconds ago
4b7wmnhc6iqod481ge5njvw7o   \_ redis.5   redis:3.0.6  swarm-worker-01  Shutdown       Shutdown 29 seconds ago
...
985dkagqrz8n40hke704an0pk  redis.10      redis:3.0.6  swarm-worker-01  Running        Running 3 minutes ago
0k4pn77hy4s6e3g778gohktnh  redis.11      redis:3.0.7  swarm-worker-01  Running        Running 3 seconds ago
4rn0r67hc8uscl0lq7kvx64t5   \_ redis.11  redis:3.0.6  swarm-worker-01  Shutdown       Shutdown 11 seconds ago
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This should happens with every node eventually. You can see the
service status if you use the &lt;code class=&quot;highlighter-rouge&quot;&gt;inspect&lt;/code&gt; command on it:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;core@swarm-manager-01 ~ $ docker service inspect --pretty redis
ID:             buye01j0ofdmt32lqplgvknic
Name:           redis
Mode:           Replicated
 Replicas:      11
Update status:
 State:         updating
 Started:       2 minutes ago
 Message:       update in progress
Placement:
UpdateConfig:
 Parallelism:   1
 Delay:         10s
 On failure:    pause
ContainerSpec:
 Image:         redis:3.0.7
Resources:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Once it’s done, you should be able to see the &lt;code class=&quot;highlighter-rouge&quot;&gt;completed&lt;/code&gt; state on the
same inspection:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Update status:
 State:         completed
 Started:       3 minutes ago
 Completed:     51 seconds ago
 Message:       update completed
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Afterwards, when you’re bored with it, you can delete it using the
&lt;code class=&quot;highlighter-rouge&quot;&gt;rm&lt;/code&gt; command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker service rm redis
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;What about the Routing Mesh? Let’s try to expose a port. We’ll launch
an nginx service with two replicas, and then we’ll try to access it on
the node with no container workload. This way, we will see if the
request is routed all the way to the corresponding backend, even when
the backend is not hosted on the accessed node. Just a little
reminder: when you expose a service using the Routing Mesh, you map it
to a certain port, and the every node in the cluster listens on that
port and routes the request all the way to the containers. So:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker service create --name amazing-web-server --publish 8080:80 --replicas 2 nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;By doing this, we will map the 8080 port on all nodes to the 80 port
inside the containers. Let us then see where our containers are
running:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;core@swarm-manager-01 ~ $ docker service ps amazing-web-server
ID                         NAME                  IMAGE  NODE             DESIRED STATE  CURRENT STATE                   ERROR
6viw0duiqjobqwlajs8flrbk1  amazing-web-server.1  nginx  swarm-worker-03  Running        Running less than a second ago
8vmfut5b34e04h84ojvyaeb30  amazing-web-server.2  nginx  swarm-worker-02  Running        Running less than a second ago
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can see that they are running on swarm-worker-02 (10.0.0.122) and
swarm-worker-03 (10.0.0.123). So, if we try to access 10.0.0.122:8080:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/5/normal-node.png&quot; alt=&quot;Normal node&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;This isn't that amazing&lt;/figcaption&gt;

&lt;p&gt;Cool, that works. What if we try to access swarm-worker-01
(10.0.0.121) though?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/5/other-node.png&quot; alt=&quot;Other node&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;This is pretty rad&lt;/figcaption&gt;

&lt;p&gt;Now, you still need a reverse-proxy or a load-balancer in order to
forward requests to the right Swarm node in order to access the right
service, but still, the ease of use and effectiveness of the system is
undeniable.&lt;/p&gt;

&lt;p&gt;What about node failover? Let us find out!&lt;/p&gt;

&lt;p&gt;Let us kill the Leader node first, to see what happens:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vagrant destroy swarm-manager-01 --force
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;No more Leader. Access the second manager node and see what’s going on:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vagrant ssh swarm-manager-02
core@swarm-manager-02 ~ $ docker node ls
ID                           HOSTNAME          STATUS   AVAILABILITY  MANAGER STATUS
1qmj079wp0cg5kys5ej8cs58i    swarm-worker-02   Ready    Active
3qeyfmoixwg7k64i6sw78gmms    swarm-worker-01   Ready    Active
3rgjfac5qau5rft2wpcpliaek *  swarm-manager-02  Ready    Drain         Leader
6ok8wzq137dxs7uow5xd3rjkd    swarm-manager-01  Unknown  Drain         Unreachable
9rarjje9gner4lwrcshymtszm    swarm-manager-03  Ready    Drain         Reachable
a1lsaawsrcoohajmc1luon0mn    swarm-worker-03   Ready    Active
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So, swarm-manager-02 became the Leader. Sweet!&lt;/p&gt;

&lt;p&gt;Before, we saw that the nginx containers were running on
swarm-worker-02 and swarm-worker-03. Now, we will destroy both nodes
to see what happens:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vagrant destroy swarm-worker-02 swarm-worker-03 --force
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If we check the service status:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;core@swarm-manager-02 ~ $ docker service ps amazing-web-server
ID                         NAME                      IMAGE  NODE             DESIRED STATE  CURRENT STATE                ERROR
95u3ndb01onpx6ki5daohuwf1  amazing-web-server.1      nginx  swarm-worker-01  Running        Running about a minute ago
3zx3cua2eu1dgykphj8rsnuwd   \_ amazing-web-server.1  nginx  swarm-worker-02  Shutdown       Running 7 minutes ago
aff0ashfqbcjxbka0jz6sbril  amazing-web-server.2      nginx  swarm-worker-01  Running        Running about a minute ago
8va90yo2vn2uwvzjrc22d0pjv   \_ amazing-web-server.2  nginx  swarm-worker-03  Shutdown       Running 7 minutes ago
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can see that the containers running on swarm-worker-02 and
swarm-worker-03 are in ‘Shutdown’ state, and that there are two new
running containers on swarm-worker-01.&lt;/p&gt;

&lt;p&gt;What a time to be alive!&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;This whole thing was fun. I (and hopefully you too) learned/noticed
some things along the way:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Swarm Standalone and Swarm Mode are two (very) different things. The
latter is already integrated in the Docker Engine after v1.12.&lt;/li&gt;
  &lt;li&gt;The Engine is divided into many different pieces as of today,
including RunC and containerd, which are used to run the containers
themselves and manage their lifecycle.&lt;/li&gt;
  &lt;li&gt;The whole “let’s divide the Engine into little independent pieces”
seems to have paid off, since every independent component is now
evolving with its own lifecycle, and furthermore, it helps
understanding the Engine as a whole.&lt;/li&gt;
  &lt;li&gt;Docker uses a different container networking standard than
Kubernetes, called CNM, with its own abstractions and resources.&lt;/li&gt;
  &lt;li&gt;The Network Model, including CNM and the Network Routing Mesh seems
easier to understand than the whole Kubernetes, even if it seemed
harder to grasp before actually looking at it. I thought it would be
hell at first, but it came out okay, kinda.&lt;/li&gt;
  &lt;li&gt;Swarm works quite nicely as of now. It can handle the High
Availability natively, and it doesn’t need an external key/value
datastore like other solutions do.&lt;/li&gt;
  &lt;li&gt;It is super easy to install. When I say super easy, I mean it’s
super super easy. &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-4/&quot;&gt;You should compare what we just did to what we had
to do for the Kubernetes cluster&lt;/a&gt;, and you’ll see that these guys
really worked their asses off in order to get the installation
procedure right, nice and simple.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I would have liked to do some other things as well:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Explore the volume management further (or at all). It’s a super
interesting subject, and I’m dying to check it out. I’d love to
compare it to the way Kube handles them.&lt;/li&gt;
  &lt;li&gt;I didn’t get to deploy complex applications or services on the
cluster. &lt;a href=&quot;https://docs.docker.com/compose/bundles/#creating-a-stack-from-a-bundle&quot;&gt;No DABs and no stacks&lt;/a&gt;. This seemed interesting enough, but
I didn’t have enough time to actually try it.&lt;/li&gt;
  &lt;li&gt;I didn’t get to compare the usage of a different key/value datastore
instead of the internal one, in terms of performance or ease of use.&lt;/li&gt;
  &lt;li&gt;Run some benchmarks on leader election convergence time. I just know
it works, I don’t know how fast it can be, and how does that change
when your Manager group increases in size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I might do all of these things in the near future. Or not. Who knows.
Anyway. I had a blast, I hope you did too!&lt;/p&gt;

</description>
          <pubDate>2017-10-16T21:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-docker-5/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-docker-5/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Docker! Episode 4 - Control your Swarm!</title>
          <description>&lt;h2 id=&quot;code-please&quot;&gt;Code, please!&lt;/h2&gt;

&lt;p&gt;Yay, code!&lt;/p&gt;

&lt;p&gt;First of all, we will need the actual virtual machines where our
cluster will run. We will create these using Vagrant. If you don’t
know what Vagrant is, or why are we using it, &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-3/&quot;&gt;check out this article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For this deployment, we will have three Manager nodes, and three
worker nodes.  Why three Manager nodes you say? It may seem overkill,
but in order to have High Availability you need to have an odd number
of Manager nodes, otherwise, you will not get consensus from Raft. We
will see this in action later on.&lt;/p&gt;

&lt;p&gt;We do not need an etcd cluster, since the key-value datastore is
already included in the internals of the Docker Engine, and therefore,
we will not include any machines for it.&lt;/p&gt;

&lt;p&gt;First of all, I’ll describe the amount of machines I want and their
configurations as variables:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# General cluster configuration&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$swarm_manager_instances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$swarm_manager_instance_memory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$swarm_manager_instance_cpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$swarm_worker_instances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$swarm_worker_instance_memory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$swarm_worker_instance_cpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Afterwards, I’ll specify that I want to use the CoreOs (Contaner
Linux) Vagrant box, from an URL:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Box management: CoreOS&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;box&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;coreos-stable&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;box_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;https://storage.googleapis.com/stable.release.core-os.net/amd64-usr/current/coreos_production_vagrant.json&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Just a little reminder, Container Linux is a lightweight Linux
distribution that uses container to run applications. It ships with
basic GNU utilities so you can do all your business, and some other
interesting things, like kubelet, Docker, etcd and flannel. We’ll only
be using Docker for this part. In short, CoreOS’s Container Linux is
an OS specially designed to run containers, and we’re going to profit
from that in our context. If you wanna know more about CoreOS, &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-3/&quot;&gt;check
this article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’ll configure our Manager nodes with the variables we previously
defined:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Swarm manager instances configuration&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;vg&quot;&gt;$swarm_manager_instances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;define&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vm_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;swarm-manager-%02d&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Name&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vm_name&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# RAM, CPU&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;provider&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:virtualbox&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;gui&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;false&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;memory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;vg&quot;&gt;$swarm_manager_instance_memory&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;vg&quot;&gt;$swarm_manager_instance_cpus&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# IP&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:private_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;ip: &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10.0.0.&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;110&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And then we’ll do the same thing with our Worker nodes:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Swarm worker instances configuration&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;vg&quot;&gt;$swarm_worker_instances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;define&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vm_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;swarm-worker-%02d&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Name&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vm_name&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# RAM, CPU&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;provider&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:virtualbox&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;gui&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;false&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;memory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;vg&quot;&gt;$swarm_worker_instance_memory&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;vg&quot;&gt;$swarm_worker_instance_cpus&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# IP&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:private_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;ip: &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10.0.0.&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Easy. If you wanna take a look at the Vagrantfile, it’s right here. Moving on.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/qCbprH0VYRFHDhDhXGvDUXDkE.js&quot; id=&quot;asciicast-qCbprH0VYRFHDhDhXGvDUXDkE&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;Next up, I’ll set up a Makefile with a Vagrant target, which will
create all the virtual machines, and generate a configuration file
with the SSH configuration needed to interact with them. Ill also add
a clean target, which will destroy all the machines, and it will
delete the SSH configuration file, if it exists.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-raw&quot;&gt;.PHONY: vagrant clean

vagrant:
    &quot;@vagrant up&quot;
    @vagrant ssh-config &amp;gt; ssh.config

clean:
    @vagrant destroy -f
    @[ ! -f ssh.config ] || rm ssh.config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I’ll also put in a list of phony targets, since the targets previously
defined are just actions and don’t really generate files.&lt;/p&gt;

&lt;p&gt;We’ve got the virtual machines, and the SSH configuration. For the
Ansible part, we’ll start by creating an inventory with all our six
nodes in it, separated by groups:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[swarm-leader]
swarm-manager-01

[swarm-manager]
swarm-manager-01
swarm-manager-02
swarm-manager-03

[swarm-worker]
swarm-worker-01
swarm-worker-02
swarm-worker-03
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You will probably notice there is a &lt;code class=&quot;highlighter-rouge&quot;&gt;swarm-leader&lt;/code&gt; group in the
inventory, which contains a single host. Like I said in the first
article, there might be many Managers in a cluster; Nevertheless,
there is only one Leader at any given moment. We will use this group
to launch specific actions for the Leader, common actions for all
Manager nodes using the swarm-manager group, and actions destined for
the non-Leader Manager nodes using the swarm-manager group, and
subtracting the swarm-leader group from it. This may seem complex, but
it is actually super easy, you will see.&lt;/p&gt;

&lt;p&gt;No IP configurations over here, we’ll just use the SSH configuration
file we generated earlier. In order to do that, we have to specify it
on our ansible.cfg file. No ansible.cfg file? Just create it:&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;[defaults]&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;ansible_managed&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Please do not modify this file directly as it is managed by Ansible and could be overwritten.&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;retry_files_enabled&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;remote_user&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;core&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;[ssh_connection]&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;ssh_args&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;-F ssh.config&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We’ll also disable retry_files, and specify that we want to use the
“core” user when connecting to the machines using SSH.&lt;/p&gt;

&lt;p&gt;I’ve already said this before, but CoreOS only ships with basic GNU
utilities, which means no Python. And no Python means no Ansible,
except for the raw module, the script module and the synchronize
module. What we’re going to do is that we’re going to install a
lightweight Python implementation called PyPy using only those
modules, and then use that Python implementation in order to execute
the rest of our playbook. Neat huh?&lt;/p&gt;

&lt;p&gt;We’ll use the same role we used for the Kubernetes provisioning
project. &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-4/&quot;&gt;If you want to read more about it, like the technical
explanation behind it, you can find all the information here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So basically, we’ve got a role now under
roles/bootstrap/ansible-bootstrap, which has 3 files under the tasks
directory: main.yml, configure.yml and test.yml. The configure.yml
file holds all the tasks necessary in order to install PyPy. The
test.yml file verifies if Python is correctly installed by doing
&lt;code class=&quot;highlighter-rouge&quot;&gt;python --version&lt;/code&gt;. The main.yml file wraps these two files, adding
the &lt;code class=&quot;highlighter-rouge&quot;&gt;test&lt;/code&gt; tag to the test.yml part:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# filename: roles/bootstrap/ansible-bootstrap/tasks/main.yml&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install and configure PyPy&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;configure.yml&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Test PyPy installation&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test.yml&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Gather ansible facts&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I’ll follow this approach for each role on this project, so each role
will have a smoketest, which will be enough to tell us if the
component is correctly installed. This is pretty useful in order to
test the already deployed infrastructure, as a conformance test, and
check for deltas which might need to be corrected.&lt;/p&gt;

&lt;p&gt;Now that we have our first role, it’s time to create a playbook. Since
we’ll be deploying a Swarm cluster, I’ll just name it swam.yml:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Bootstrap coreos hosts&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;all&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;gather_facts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bootstrap/ansible-bootstrap&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ansible-bootstrap&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It’s quite straightforward so far, I’ll just launch the recently
created role on each hosts, without gathering facts, since Python is
not yet installed on the machines. Facts will be gathered at the end
of the role though, as seen on the previous code snippet.&lt;/p&gt;

&lt;p&gt;Next up, tests. We’ll use molecule for the win. &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-3/&quot;&gt;I spoke to you all
about molecule on a previous article&lt;/a&gt;. It is basically a testing tool
for Ansible code. It creates ephemeral infrastructure (either virtual
machines or containers), tests your roles on it (not only the
execution of the roles, but also the syntax and their idempotence),
and then it destroys it. Since there are no CoreOS containers, and
Virtualbox virtual machines through Vagrant being the target platform,
I’ll just use the Vagrant driver.&lt;/p&gt;

&lt;p&gt;In order to test with molecule, I’m going to create a molecule.yml
file, in which I’m going to define the Ansible files to use for the
test, as well as the Vagrant machine’s specification and
configuration.&lt;/p&gt;

&lt;p&gt;First, I’ll specify which Ansible configuration to use, and which
playbook to run:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;ansible&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;config_file&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;./ansible.cfg&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;playbook&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm.yml&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then, I’ll specify which Vagrant box to use for the virtual machines:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;platforms&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;coreOS&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;box&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;coreos-stable&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;config.vm.box_url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://storage.googleapis.com/stable.release.core-os.net/amd64-usr/current/coreos_production_vagrant.json&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then, what will my provider be, and how much physical resources will be used by each instance:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;providers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtualbox&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;virtualbox&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2048&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;cpus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After that, I’ll define the specifics of each instance, including both
hostname and IP addresses:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;instances&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-manager-01&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;ansible_groups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-leader&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-manager&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;interfaces&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;network_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;private_network&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;static&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10.0.0.101&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;auto_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;append_platform_to_hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;no&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-manager-02&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;ansible_groups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-manager&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;interfaces&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;network_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;private_network&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;static&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10.0.0.102&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;auto_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;append_platform_to_hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;no&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-manager-03&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;ansible_groups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-manager&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;interfaces&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;network_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;private_network&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;static&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10.0.0.103&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;auto_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;append_platform_to_hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;no&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-worker-01&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;ansible_groups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-worker&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;interfaces&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;network_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;private_network&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;static&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10.0.0.121&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;auto_config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;append_platform_to_hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;no&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;With that in place, I just need to run &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule test&lt;/code&gt; in order to
test that my infrastructure is created and configured correctly. This
is actually an oversimplification of everything that can be done using
molecule, but since &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-4/&quot;&gt;I already wrote about it on a previous
article&lt;/a&gt;,
just can just head there and read about it if you’re really
interested.&lt;/p&gt;

&lt;p&gt;And with that, I also get to add two new (phony) Makefile target:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;smoketest:
    @ansible-playbook -i inventories/vagrant.ini swarm.yml --tags test

test:
    @molecule test
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The smoketest target allows me to run a conformance test on all the
already deployed infrastructure, to check for deltas and see if
something’s wrong whenever I want, and the test target allows me to
test the code on fresh, newly created infrastructure, and to check for
Ansible-specific good practices. Remember, this uses Molecule V1, so
if you try to run it using Molecule V2 it will probably not work.&lt;/p&gt;

&lt;p&gt;Tests are set up thusly. Moving on.&lt;/p&gt;

&lt;h2 id=&quot;manage-lead-and-follow&quot;&gt;Manage: Lead and follow&lt;/h2&gt;

&lt;p&gt;Next up, we need to setup the three Manager nodes. I’ll start by
creating a swarm-leader role, under the configuration roles directory:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;roles/
├── bootstrap
│   └── ansible-bootstrap
└── configure
    └── swarm-leader
         └── tasks
             ├── configure.yml
             ├── main.yml
             └── test.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And for this role, we’ll use the same task division strategy we used
before in order to add our smoketest.&lt;/p&gt;

&lt;p&gt;First, the main.yml file, which is fairly simple, and only includes
the other two yaml files, using a tag for the test file:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Create Manager Leader&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;configure.yml&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Test Manager Leader&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test.yml&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The configure file first checks if the cluster is already on Swarm
mode. If it is, it doesn’t do anything else. If it isn’t, it creates
the first Swarm node, creating thus the Swarm cluster, which will be
joined by the subsequent nodes. It also disables scheduling on the
Leader, making sure that the Leader does not handle any workload and
that it concentrates its resources on leading the cluster:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if Swarm Mode is already activated&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker_info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Create Swarm Manager Leader if it is not activated&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker swarm init --advertise-addr&lt;/span&gt; 
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Swarm:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Disable Leader scheduling&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker node update --availability drain&lt;/span&gt; 
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Swarm:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;and&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;disable_leader_scheduling&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This last part is not actually necessary, specially for small
clusters. Nevertheless it is usually a good practice, since the leader
election process can be really intensive in terms of resource
consumption. The &lt;code class=&quot;highlighter-rouge&quot;&gt;disable_leader_scheduling&lt;/code&gt; variable is defined on
the role’s defaults, and you can override it if you want your Leader
to handle workloads.&lt;/p&gt;

&lt;p&gt;Fairly simple. Notice the &lt;code class=&quot;highlighter-rouge&quot;&gt;changed_when: false&lt;/code&gt; parameter on the first
command task. It is there because running  &lt;code class=&quot;highlighter-rouge&quot;&gt;docker info&lt;/code&gt; will not
change the state of the cluster, and it is therefore not a real
action, just a way of collecting information.&lt;/p&gt;

&lt;p&gt;Next, for the smoketest, I’ll verify if the created Manager node is in
fact a Leader (which it should be, since it was the first Manager node
to be created), and whether its status is “Drain”, since the Leader
node is not supposed to handle any workload:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if Manager node is Leader&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker node ls | grep&lt;/span&gt; 
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker_info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Fail if Manager node is not Leader&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;that&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Leader'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now that the role is set, I’ll just add it to the swarm.yml playbook:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Create Swarm Leader node&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-leader&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;configure/swarm-leader&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;swarm-leader&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Using the host group we discussed earlier, and the proper tag in order
to identify the action. And that’s it for the Manager Leader. We need
some non-Manager Leader for that High Availability though!&lt;/p&gt;

&lt;p&gt;So we’ll just repeat the previous process, we’ll create a
swarm-manager role up next, with the same structure of the previous
role (main.yml, configure.yml, test.yml).&lt;/p&gt;

&lt;p&gt;I won’t show you the main.yml: it is basically the same one we saw
before. The configure.yml file, on the other hand, checks if Swarm
mode is activated on the node, the same way the Leader role does, but
if it isn’t, it recovers the token needed to join the cluster as a
Manager node from the Leader node, and joins the cluster with it. If
Swarm mode is already activated, it does nothing:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if Swarm Mode is already activated&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker_info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Recover Swarm Leader token&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker swarm join-token manager | grep token | cut -d ' ' -f 6&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;leader_token&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Swarm:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;delegate_to&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Join Swarm Cluster as Manager&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker swarm join --token&lt;/span&gt;  
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Swarm:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Disable Manager scheduling&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker node update --availability drain&lt;/span&gt; 
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Swarm:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;active'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;and&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;disable_manager_scheduling&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice the &lt;code class=&quot;highlighter-rouge&quot;&gt;delegate_to&lt;/code&gt; option on the token recovery task. This needs
to be done because the token must be recovered from the Leader node
and the Leader node only. Scheduling is also disabled on these nodes,
by default, because of the reason specified above on the Leader node.
This time, the &lt;code class=&quot;highlighter-rouge&quot;&gt;disable_manager_scheduling&lt;/code&gt; variable is also defined
on the role’s defaults. You can override this variable if you want
your Managers to handle workloads.&lt;/p&gt;

&lt;p&gt;The test file verifies different  things as well:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if node is Manager&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker node ls | grep&lt;/span&gt; 
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker_info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Fail if node is not Manager&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;assert&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;that&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reachable'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Drain'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;docker_info.stdout&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It recovers the nodes information, and then it verifies that the node
Manager type is &lt;code class=&quot;highlighter-rouge&quot;&gt;Reachable&lt;/code&gt; rather than &lt;code class=&quot;highlighter-rouge&quot;&gt;Leader&lt;/code&gt;, as it was for the
Leader node. It also verifies that the nodes are “drained” since we
don’t want them to run containers.&lt;/p&gt;

&lt;p&gt;Finally, once the role is ready, I’ll add it to the swarm.yml file:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Create Swarm Manager nodes&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;swarm-manager:!swarm-leader&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roles&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;configure/swarm-manager&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;swarm-manager&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice the &lt;code class=&quot;highlighter-rouge&quot;&gt;!&lt;/code&gt; sign on the hosts part of the play. This specifies that
we want to run the role on every node on the swarm-manager group, that
isn’t in the swarm-leader group, thus preventing the Leader node to
try to join the cluster as a non-Leader Manager. Sweet!&lt;/p&gt;

&lt;p&gt;Once we finish all this, we should have everything we need
Manager-wise. Time to get some Workers running! I’ll probably talk to
you about that on the next article though.&lt;/p&gt;

&lt;p&gt;Stay in touch!&lt;/p&gt;

</description>
          <pubDate>2017-10-02T21:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-docker-4/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-docker-4/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Docker! Episode 3 - Swarm load balancing, service discovery and security</title>
          <description>&lt;h2 id=&quot;why-service-discovery&quot;&gt;Why Service Discovery?&lt;/h2&gt;

&lt;p&gt;Right, Service Discovery. &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-2/&quot;&gt;From a previous article&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;“...What if your application needs to advertise its own IP address to
a container that is hosted on another node? It doesn’t actually knows
its real IP, since his local IP is getting translated into another IP
and a port on the host machine. ”
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;That’s why&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The Docker Engine has an embedded DNS server within it, which is used by
containers when Docker is not running in Swarm mode, and for tasks, when it is.
It provides name resolution to all the containers that are on the host in
bridge, overlay or MACVLAN networks. Each container forwards its queries to the
Docker Engine, which in turn checks if the container or service is on the same
network as the container that sent the request in the first place. If it is, it
searches the IP (or virtual IP) address that matches a container, a task’s or a
service’s name in its internal key-value store and returns it to the container
that sent the request. Pretty cool, huh?&lt;/p&gt;

&lt;p&gt;As I said before, the Docker Engine will only return an IP address if the
matching resource is within the same network as the container that generated
the request. What is also cool about this is that Docker Hosts only store the
DNS entries that belong to networks  in which the node has containers or tasks.
This means that they will not store information that’s irrelevant to them
practically, or that other containers do not need to know.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/3/service-discovery.png&quot; alt=&quot;Service discovery&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://success.docker.com/Architecture/Docker_Reference_Architecture%3A_Designing_Scalable%2C_Portable_Docker_Container_Networks&quot;&gt;Discover what's up&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;For example, in this image, there is a network called mynet. There are two
services running on the network: myservice, and client. myservice has two tasks
associated to it, whereas client only has one.&lt;/p&gt;

&lt;p&gt;Client then executes a curl request to myservice, and therefore, it also does a
DNS request. The container built-in Resolver forwards the query to the Docker
Engine’s DNS server. The request to myservice is then resolved to the 10.0.0.3
virtual IP. This information is then forwarded back to client.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/Q8lfVLua5NIBsIknhojPaRk4g.js&quot; id=&quot;asciicast-Q8lfVLua5NIBsIknhojPaRk4g&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;What if client requests something that’s not in the internal key-value store?
For example, if client does a curl request for the sebiwi.github.io domain, the
same flow will the triggered. The DNS query is forwarded by the resolver to the
DNS server. Since the sebiwi.github.io name is not present in the key-value
store (which in turn means that it is not a service within the network), the
Docker Engine will forward the request to its default configured DNS server.&lt;/p&gt;

&lt;p&gt;This methodology seems quite logical and simple, but it is only possible due to
the existence of a key-value store integrated with the Docker Engine.&lt;/p&gt;

&lt;h2 id=&quot;and-load-balancing&quot;&gt;And Load Balancing?&lt;/h2&gt;

&lt;p&gt;Yeah, there’s native load balancing too! There are basically two types of load
balancing: internal and external. Internal is all about load balancing requests
that are made from within the Swarm cluster (from other containers), whereas
external load balancing targets ingress traffic that enters a cluster. Both of
these functionalities are provided by the Docker Engine itself.&lt;/p&gt;

&lt;p&gt;Let’s talk about internal first. This feature is automatically enabled once a
Service is created. So when a Service is created, it get a virtual IP address
right away, on the Service’s network. As we said before in the Service
Discovery part, when a Service is requested the resulting DNS query is
forwarded to the Docker Engine, which in turn returns the IP of the service, a
virtual IP. Traffic sent to that virtual IP is load balanced to all of the
healthy containers of that service on the network. All the load balancing is
done by Docker, since only one entry-point is given to the client (one IP).&lt;/p&gt;

&lt;p&gt;Things change slightly when doing external load balancing. First of all, the
load balancing is not activated by default, but rather when you expose a
service using the –publish flag at creation or update time. When this happens,
every node in the cluster starts listening on the published port. This means
that every node can respond to a request for the service mapped onto that port.&lt;/p&gt;

&lt;p&gt;What is really interesting is what happens when a node receives a request, but
it does not have an instance of the container within it. Since Docker 1.12
(same version that integrated Swarm Mode to the Docker Engine), there is a
feature called Routing Mesh, which uses &lt;a href=&quot;http://www.linuxvirtualserver.org/software/ipvs.html&quot;&gt;IP Virtual Servers&lt;/a&gt; (ipvs) and iptables
in order to load balance requests in layer 4. Basically, ipvs implements layer
4 load balancing functionalities on the Linux Kernel, which allows to redirect
requests for TCP/UDP-based services to the real backends (containers in this
case). In Swarm’s specific case, every node listens on the exposed port, and
then forwards the request to the exposed service’s VIP, using a special overlay
network called ingress. This overlay network is only used when transporting
external traffic to the requested services. In this scope, the same internal
load balancing strategies described in the previous section are used.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/3/load-balancing.png&quot; alt=&quot;Load balancing&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://success.docker.com/Architecture/Docker_Reference_Architecture%3A_Universal_Control_Plane_2.0_Service_Discovery_and_Load_Balancing&quot;&gt;Balance that load&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;In this picture, a service is created with two replicas, on the appnet overlay
network. We can see that the service is exposed on port 8000 on the three
nodes. This is great, because traffic destined for app can be forwarded to any
node. In this case, there is an external load balancer, that just happens to
forward the request to the only node that does not have an instance of the
service. This request is handled and forwarded by the IPVS on the third node,
which redirects it to one of the actual containers on the cluster for that
service, using the ingress network and therefore the aforementioned method of
load balancing. Neat.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/9gKRhNitM8sS777UWA8cCIlTS.js&quot; id=&quot;asciicast-9gKRhNitM8sS777UWA8cCIlTS&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;Just for the record, when using Docker Enterprise Edition’s Universal Control
Plane, this Routing Mesh is also capable of routing layer 7 traffic, by
inspecting the HTTP header of requests, therefore operating at an application
level. This is actually another feature of Docker Swarm, called HTTP Routing
Mesh, or HRM, which allows each created service to be accessed through a DNS
label. When using HRM, the normal Routing Mesh is used as well.  Every HTTP/1.1
TCP request contains a Host Header. At service creation time, the desired label
must be specified. When a service is created using the
io.github.sebiwi.ucp.mesh.http label, the HRM routes all requests with the Host
field specified in the previously defined label to the VIP of the service. This
enables direct access to the service under the form of a hostname, so
theoretically it is the simplest way to expose your service to the internet.
You won’t need an external load balancer in order to forward traffic to the
right port at ingress. That’s pretty cool.&lt;/p&gt;

&lt;h2 id=&quot;wheres-the-security-though&quot;&gt;Where’s the security though?&lt;/h2&gt;

&lt;p&gt;Security is implemented by means of isolation and encryption.&lt;/p&gt;

&lt;p&gt;The isolation part works as follows: every network is segmented from each other
to prevent all traffic between them. This provides actual layer 3 separation.
The Docker Engine also manages host firewall rules which prevent access between
different networks and which also manage ports for containers. Since all of
this is managed by the Docker Engine itself, it changes dynamically according
to tasks, services and networks that are created inside of the cluster. Traffic
generated from inside containers to outside networks is allowed, and so are
responses generated from this traffic. Ingress traffic is denied by default,
and is only accepted through exposing service on ports, using the previously
described methods.&lt;/p&gt;

&lt;p&gt;Let us talk about encryption. All the control plane traffic between nodes is
secured through TLS. All managers and nodes have signed certificates in them,
which are created automatically by Swarm and are rotated automatically as well.
For data plane traffic, all traffic is encrypted using IPSec tunnels when
leaving the source container, and it is decrypted once it arrives to the
destination container. This guarantees security even when you do not fully
control the underlying network infrastructure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/3/ipsec.png&quot; alt=&quot;Load balancing&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://success.docker.com/Architecture/Docker_Reference_Architecture%3A_Designing_Scalable%2C_Portable_Docker_Container_Networks&quot;&gt;No one will ever know&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;In the picture, when container 1 sends traffic to container 2, everything is
encrypted on the way out, and then it is decrypted once it arrives to host B,
before entering the destination container. The Swarm leader periodically
regenerates symmetric keys for IPSec, and it distributes them to all the
cluster nodes.&lt;/p&gt;

&lt;p&gt;Docker Enterprise Edition’s Universal Control Plane also has advanced security
options, like role based access control, for example. I won’t discuss these
further, since I haven’t actually tried them myself.&lt;/p&gt;

&lt;p&gt;Anyways, that’s all for today. Next time, we will start coding stuff! Stick
around, we’ll be right back!&lt;/p&gt;

</description>
          <pubDate>2017-09-21T21:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-docker-3/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-docker-3/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Docker! Episode 2 - Swarm networking</title>
          <description>&lt;h2 id=&quot;so-what-about-networking-then&quot;&gt;So what about networking then?&lt;/h2&gt;

&lt;p&gt;Right, networking. From a &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-2/&quot;&gt;previous article on Kubernetes
Networking&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;“For each container that is created, a virtual ethernet device is
attached to this bridge, which is then mapped to eth0 inside the
container, with an ip within the aforementioned network range. Note
that this will happen for each host that is running Docker, without
any coordination between the hosts.  Therefore, the network ranges
might collide.

Because of this, containers will only be able to communicate with
containers that are connected to the same virtual bridge. In order to
communicate with other containers on other hosts, they must rely on
port-mapping. This means that you need to assign a port on the host
machine to each container, and then somehow forward all traffic on
that port to that container. What if your application needs to
advertise its own IP address to a container that is hosted on another
node? It doesn’t actually knows its real IP, since his local IP is
getting translated into another IP and a port on the host machine. You
can automate the port-mapping, but things start to get kinda complex
when following this model.

That’s why Kubernetes chose simplicity and skipped the dynamic
port-allocation deal. It just assumes that all containers can
communicate with each other without Network Address Translation (NAT),
that all containers can communicate with each node (and vice-versa),
and that the IP that a container sees for itself is the same that the
other containers see for it”
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Docker doesn’t do this. They just went nuts and decided to go crazy on the
dynamic port-forwarding part. For this, they relied heavily on the existing
Linux kernel’s networking stack. This is fairly cool, since the existing Linux
networking features are pretty mature and robust already.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/2/networking.png&quot; alt=&quot;Docker networking&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://blog.docker.com/2016/03/docker-networking-design-philosophy/&quot;&gt;You there?&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;In order to provide its networking, Docker uses numerous Linux networking tools
as building blocks to handle all of its forwarding, segmentation and management
needs. Primarily, the most used tools are Linux bridges, network namespaces,
virtual ethernet devices and iptables.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;Linux bridge&lt;/strong&gt; is a virtual implementation of a physical switch inside of the
Linux kernel. It forwards traffic basing itself on MAC addresses, which are in
turn discovered dynamically by inspecting traffic.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/0c2Msy5pcD0lqrXYyM9gPbJw5.js&quot; id=&quot;asciicast-0c2Msy5pcD0lqrXYyM9gPbJw5&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;A &lt;strong&gt;network namespace&lt;/strong&gt; is an isolated network stack with its own collection of
interfaces, routes and firewall rules. Network namespaces are used to provide
isolation between processes, analog to regular namespaces They ensure that two
containers, even if they are on the same host, won’t be able to communicate
with each other unless explicitly configured to do so.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/zA8WmkS4dkxXcoWY94RXQvT81.js&quot; id=&quot;asciicast-zA8WmkS4dkxXcoWY94RXQvT81&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Virtual ethernet&lt;/strong&gt; devices or veth are interface that act as connections between
two network namespaces. They have a single interface in each namespace. When a
container is attached to a Docker Network, one end of the veth is placed inside
the container under the name of ethx, and the other is attached to the Docker
Network.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/UMJSwnyI9ADRG3Wgi0RPA1a7I.js&quot; id=&quot;asciicast-UMJSwnyI9ADRG3Wgi0RPA1a7I&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Iptables&lt;/strong&gt; is a package filtering system, which acts as a layer 3/4 firewall and
provide packet marking, masquerading and dropping features. The native Docker
Drivers use iptables in heavy amounts in order to do network segmentation, port
mapping, mark traffic and load balance packets.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/vSlSCYzz365LJNKufNyxZxz1U.js&quot; id=&quot;asciicast-vSlSCYzz365LJNKufNyxZxz1U&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;Now that you know all that, let’s talk about models.&lt;/p&gt;

&lt;h2 id=&quot;give-me-some-cnm&quot;&gt;Give me some CNM!&lt;/h2&gt;

&lt;p&gt;I talked to you about the Container Network Interface (CNI) &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-5/&quot;&gt;when talking about
Kubernetes on a previous article&lt;/a&gt;. Docker uses a different standard, called the
Container Network Model (CNM) which is implemented by Docker’s libnetwork.&lt;/p&gt;

&lt;p&gt;There are three main components of the CNM model: Sandboxes, Endpoints and
Networks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/2/cnm.jpeg&quot; alt=&quot;CNM diagram&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;You there?&lt;/figcaption&gt;

&lt;p&gt;The &lt;strong&gt;Sandbox&lt;/strong&gt; contains the configuration of the container’s network stack, such
as interface management, IP and MAC addresses, routing tables and DNS settings.
A Sandbox may contain endpoints from multiple networks.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Endpoint&lt;/strong&gt; connects a Sandbox to a Network. An Endpoint can belong to only
one Network, and one Sandbox. It gives connectivity to the services that are
exposed in a Network by a container.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Network&lt;/strong&gt; is a collection of Endpoints that are able to talk to each other. A
Network consists of many endpoints.&lt;/p&gt;

&lt;p&gt;Each one of these components has an associated CNM object on libnetwork and a
couple of other abstractions that allow the whole thing to work together
nicely.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;NetworkController object&lt;/strong&gt; exposes an API entrypoint to libnetwork, which
users (like the Docker Engine) use in order to allocate and manage Networks. It
also binds a specific driver to a network.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Driver object&lt;/strong&gt;, not directly visible to the user, makes Networks work in the
end. It is configured through the NetworkController. There are both native
Drivers (such as Bridge, Host, None, Overlay, and MACVLAN) and remote (from
plugin providers) that can be used for different situations depending on your
needs. Basically the Driver owns a network and handles all of its management.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Network object&lt;/strong&gt; is an implementation of the Network component. It is created
using the NetworkController. The corresponding Driver object will be notified
upon its creation, or modification. The Driver will then connect Endpoints that
belong to the same Network, and isolate those who belong to different ones.
This provided connectivity can span many hosts, therefore, the Network object
has a global scope within a cluster.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Endpoint object&lt;/strong&gt; is basically a Service Endpoint. A Network object provides
an API to create and manage Endpoints. It can only be attached to one Network.
It provides connectivity from and to Services provided by other containers in
the Network. They are global to the cluster as well, since they represent a
Service rather than a particular container.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Sandbox object&lt;/strong&gt;, much like the component described above, represents the
configuration of the container’s network stack, such as interface management,
IP and MAC addresses, routing tables and DNS settings. It is created when a
user requests an Endpoint creation on a Network. When this happens, the Driver
in charge of the Network allocates the necessary network resources, such as an
IP address, and passes that information, labeled as SandboxInfo back to
libnetwork, which will in turn use the specific OS tools to create the network
configuration on the container that correspond to the previously mentioned
Sandbox. A Sandbox object may have multiple Endpoints, and therefore, may be
connected to multiple Networks. Its scope is local, since it is associated to a
particular container on a given host.&lt;/p&gt;

&lt;p&gt;As I said earlier, there are two basic type of Drivers: &lt;strong&gt;native&lt;/strong&gt; and &lt;strong&gt;remote&lt;/strong&gt;.
Native Drivers don’t require any extra modules and are included in the Docker
Engine by default.&lt;/p&gt;

&lt;p&gt;Native Drivers include: &lt;strong&gt;Host, Bridge, Overlay, MACVLAN and None&lt;/strong&gt;.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/8tjtYy6K7l5hCAo0Gk9Wg8VtM.js&quot; id=&quot;asciicast-8tjtYy6K7l5hCAo0Gk9Wg8VtM&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;When using the &lt;strong&gt;Host driver&lt;/strong&gt;, the container uses the Host’s network stack,
without any namespace separation, and while sharing all of the host’s
interfaces.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/xQwYXKrskPqjbXh5dCuJpnrbI.js&quot; id=&quot;asciicast-xQwYXKrskPqjbXh5dCuJpnrbI&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;Bridge driver&lt;/strong&gt; creates a Docker-managed Linux bridge on the Docker host. By
default, all containers created on the same bridge can talk to each other.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/7D9BlymkNju8qwgTYpPWeCS9u.js&quot; id=&quot;asciicast-7D9BlymkNju8qwgTYpPWeCS9u&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;Overlay driver&lt;/strong&gt; creates an overlay network that may span over multiple
Docker hosts. It uses both local Linux bridges and VXLAN to overlay
inter-container communication over physical networks.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/xi12J6wrrEM3v3gNoOHJW1Baw.js&quot; id=&quot;asciicast-xi12J6wrrEM3v3gNoOHJW1Baw&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;MACVLAN&lt;/strong&gt; driver uses the MACVLAN bridge mode to establish connections
between container interfaces and parent host interfaces. They can be used to
assign IP addresses that are routable on physical networks to containers.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/bh7F1lWgFknXoOWuEcADiglsB.js&quot; id=&quot;asciicast-bh7F1lWgFknXoOWuEcADiglsB&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;None&lt;/strong&gt; driver gives a container its own network stack and namespace, without
any interfaces. Therefore, it stays isolated from every other Network, and even
its own host’s network stack.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/YrMcavMgMDlahlKIHudQyA7YN.js&quot; id=&quot;asciicast-YrMcavMgMDlahlKIHudQyA7YN&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;Remote drivers are created either by vendors or the community. The Remote
Drivers that are compatible with CNM are &lt;a href=&quot;http://contiv.github.io/&quot;&gt;contiv&lt;/a&gt;, &lt;a href=&quot;https://www.weave.works/docs/net/latest/overview/&quot;&gt;weave&lt;/a&gt;, &lt;a href=&quot;https://www.projectcalico.org/&quot;&gt;calico&lt;/a&gt; (&lt;strong&gt;which we used
on our Kubernetes deployment!&lt;/strong&gt;) and &lt;a href=&quot;https://github.com/openstack/kuryr&quot;&gt;kuryr&lt;/a&gt;. I won’t be talking about these since
we will not be using them.&lt;/p&gt;

&lt;p&gt;Different networks drivers have different scopes. We will talk about overlay
networks, since they hold a “swarm” scope, which means that they have the same
Network ID through the cluster, and which is what we wanted to explain in the
first place.&lt;/p&gt;

&lt;h2 id=&quot;overlay-networks&quot;&gt;Overlay Networks!&lt;/h2&gt;

&lt;p&gt;How come you don’t need a key-value datastore, you say? It’s all because of
Docker’s Network Control Plane. It manages the state of Docker Networks within
a Swarm cluster, while also propagating control-plane data. It uses a gossip
protocol to propagate all the aforementioned information. It is scoped by
Network, which is quite cool since it dramatically reduces the amount of
updates a host receives.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/2/control-plane.png&quot; alt=&quot;Control plane&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://success.docker.com/Architecture/Docker_Reference_Architecture%3A_Designing_Scalable%2C_Portable_Docker_Container_Networks&quot;&gt;Control that cluster&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;It is built upon many components that work together in order to achieve fast
convergence, even in large scale clusters and networks.&lt;/p&gt;

&lt;p&gt;Messages are passed in a peer-to-peer fashion, expanding the information in
each exchange to an even larger group of nodes. Both the intervals of
transmission and the size of the peering groups are fixed, which helps keeping
the network usage in check. Network failures are detected using hello messages
which helps to rule out both link congestion and false node failures. Full
state syncs are done often, in order to achieve consistency faster and fix
network partitions. Also, topology-aware algorithms are used in order to
optimize peering groups using relative latency as criteria.&lt;/p&gt;

&lt;p&gt;Overlay networks rely heavily on this Network Control Plane. It uses standard
&lt;a href=&quot;https://en.wikipedia.org/wiki/Virtual_Extensible_LAN&quot;&gt;VXLAN&lt;/a&gt; to encapsulate container traffic and send it to other containers.
Basically, VXLAN is an encapsulation format that wraps Layer 2 segments with an
IP/UDP header, and then send it over Layer 3 networks. In this case, the Layer
2 frames come from a container. This “underlay” header provides transportation
between hosts on the underlay network, while the overlay is the stateless VXLAN
tunnel, that exists as point-to-multipoint connections between each host
participating in the overlay network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/2/overlay-network.png&quot; alt=&quot;Overlay networks&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://success.docker.com/Architecture/Docker_Reference_Architecture%3A_Designing_Scalable%2C_Portable_Docker_Container_Networks&quot;&gt;Like this&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;For example, in the previous diagram:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;c1 is sending c2 packets through a shared overlay network. What
happens is that c1 does a DNS lookup for c2. They are both on the
same overlay network, so the Docker Engine local DNS service
resolves c2 to 10.0.0.3, its overlay address.&lt;/li&gt;
  &lt;li&gt;C1 generates an L2 frame destined for the MAC address of c2.&lt;/li&gt;
  &lt;li&gt;The overlay network driver then encapsulates the frame with a VXLAN
header, with the physical address of host-B, which he knows by
knowing the state and location of every VXLAN tunnel endpoint through
the control plane.&lt;/li&gt;
  &lt;li&gt;The packet is then sent and routed as a normal packet using the
physical network.&lt;/li&gt;
  &lt;li&gt;Finally, the packet is received by host-B, decapsulated by the
overlay network driver, and passed on to c2.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This whole process seriously resembles the &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-2/&quot;&gt;SDN protocol implemented by flannel&lt;/a&gt;,
described on the Kubernetes networking part of its “How does it work” series,
with flanneld replacing the overlay network driver. Neat, huh? Great minds
think alike.&lt;/p&gt;

&lt;p&gt;What about the driver itself? It automates all the VXLAN configuration needed
for an overlay network. When creating the network, the Docker Engine creates
the necessary infrastructure on each host. A Linux bridge is created for each
overlay network, with its associated VXLAN interfaces. The overlay is only
instantiated on hosts when a container that belongs to the network is scheduled
on the host, preventing unnecessary spreading of the overlay networks when they
are not needed.&lt;/p&gt;

&lt;p&gt;When a container is created, at least two network interfaces are created inside
of it: one that connects it to the overlay bridge, and the other to the
docker_gwbridge. The overlay bridge is the ingress/egress point to the overlay
network that the VXLAN encapsulates. It also extends the overlay across all the
hosts that participate in this particular overlay. There is one per overlay
subnet on each host, with the same name as the overlay network. The
docker_gwbridge is the egress bridge for all the traffic that leaves the
cluster. There is only one docker_gwbridge per host. Container-to-container
traffic flows do not go through this bridge.&lt;/p&gt;

&lt;p&gt;As I said before, overlay networks span multiple Docker hosts. They can be
either managed by a Swarm cluster on Swarm Mode, or without it. Nevertheless,
when you’re not using Swarm Mode, you will need a valid key-value store service
(&lt;a href=&quot;https://coreos.com/etcd&quot;&gt;etcd&lt;/a&gt;, &lt;a href=&quot;https://www.consul.io/&quot;&gt;Consul&lt;/a&gt; or &lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Zookeeper&lt;/a&gt;) in order for the network to work properly. This
mode of operation is not encouraged by Docker, and it might even be deprecated
in the future (which further discredits the whole “we’re not deprecating Swarm
Standalone” argument).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/2/bridge-overlay.png&quot; alt=&quot;Overlay networks&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Cool stuff&lt;/figcaption&gt;

&lt;p&gt;That’s all for the networking part. I’ll talk to you about service discovery,
load-balancing and security on the next one. Don’t go anywhere!&lt;/p&gt;

</description>
          <pubDate>2017-09-04T21:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-docker-2/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-docker-2/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Docker! Episode 1 - Swarm general architecture</title>
          <description>&lt;p&gt;Hey there!&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;I hacked another thing together, this time in order to install a highly available
Docker Swarm cluster on CoreOS (yeah, Container Linux), using Ansible.&lt;/p&gt;

&lt;p&gt;If you want to try it:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/sebiwi/docker-coreos.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;docker-coreos
make up
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You will need Ansible 2.2+, Docker, Vagrant and Molecule&lt;/p&gt;

&lt;h2 id=&quot;why&quot;&gt;Why?&lt;/h2&gt;

&lt;p&gt;Well, &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-1/&quot;&gt;I did the same thing with Kubernetes a while ago&lt;/a&gt;, because I wanted to test
all the different new Kubernetes features and have a local cluster to play with.
I wanted to do and have the same thing with Docker in Swarm mode. I’m also
planning on comparing both of them eventually. So I needed the whole prime
matter first: installation procedures, general architecture, networking model,
concepts and abstractions and so on.&lt;/p&gt;

&lt;p&gt;Finally, I thought it would be nice to explain how every Swarm component fit
together, so everyone can understand what’s under the hood. I keep getting
questions like “what is a Kubernetes and where can I buy one?” at work, and if
it is “better than a Docker”. You won’t find the answer to the last question in
this article, but at least you will (hopefully) understand how Swarm works. You
can make up your own mind afterwards. Especially after reading both articles.&lt;/p&gt;

&lt;h2 id=&quot;okay-you-got-me-what-is-a-swarm&quot;&gt;Okay, you got me. What is a Swarm?&lt;/h2&gt;

&lt;p&gt;Docker Swarm is Docker’s container orchestration and clustering tool. It allows
you to deploy and orchestrate containers on a large number of hosts, while
enabling you to do some other cool things in the way.&lt;/p&gt;

&lt;p&gt;There is usually mild confusion when talking about Swarm, which is relatively
normal since the name has been used to refer to different things over the years.
First, there was Docker Swarm, currently known as Docker Swarm Standalone. It
basically allowed you to turn a pool of Docker hosts into a single, large,
virtual Docker host. In this scenario, a discovery service or a key-value data
store like &lt;a href=&quot;https://www.consul.io/&quot;&gt;Consul&lt;/a&gt;, &lt;a href=&quot;https://coreos.com/etcd/&quot;&gt;etcd&lt;/a&gt; or &lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Zookeeper&lt;/a&gt; was needed in order to obtain high
availability on the manager nodes (I will discuss this point further later on in
the series).&lt;/p&gt;

&lt;p&gt;Nowadays, and since Docker v1.12.0-rc1, there is something called Swarm Mode,
which is included by default in the Docker Engine. Swarm Mode allows you to
manage natively a cluster of Docker Engines. It is highly integrated with
another toolkit developed by Docker, called &lt;a href=&quot;https://github.com/docker/swarmkit&quot;&gt;Swarmkit&lt;/a&gt;, which removes the need of
using a key-value data store for service discovery like you needed to do when
using Swarm Mode Standalone: it is already included in Swarm Mode.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/1/swarm.png&quot; alt=&quot;Swarm logo&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Swarm me up&lt;/figcaption&gt;

&lt;p&gt;So there’s two of them. Theoretically, Docker has no plans of deprecating Docker
Swarm Standalone. From the Docker Swarm’s GitHub page:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Docker does not currently have a plan to deprecate Docker Swarm. The
Docker API is backward compatible so Docker Swarm will continue to
work with future Docker Engine versions.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nevertheless, from the Docker Swarm’s overview page:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;You are viewing docs for legacy standalone Swarm. These topics describe
standalone Docker Swarm. In Docker 1.12 and higher, Swarm mode is integrated
with Docker Engine. Most users should use integrated Swarm mode… Standalone
Docker Swarm is not integrated into the Docker Engine API and CLI commands.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Which means that even if they’re still maintaining Swarm’s Standalone, Swarm
Mode is where their money is. Therefore, it’s what we’re going to be using for
this project.&lt;/p&gt;

&lt;p&gt;Let us discuss architecture first.&lt;/p&gt;

&lt;h2 id=&quot;docker-architecture&quot;&gt;Docker architecture&lt;/h2&gt;

&lt;p&gt;Before talking about Swarm, I’d like to talk about Docker itself. I’ve mentioned
the Docker Engine quite a few times without actually describing what it really
is. It is basically a client-server application with three major components: a
command-line interface (CLI), a REST API and a server.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The server is a daemon process called dockerd. It listens for Docker API
requests and manages all Docker resources, such as images, networks, containers
and volumes. It can also communicate with other daemons to manage Docker
services (more on this later)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The REST API is served by the Docker Engine, and it allows clients to talk to
the daemon and control every aspect of it. It can be accessed with any HTTP
client, but if you want to stay “official” there are many standard SDKs on many
languages, and there is also the standard CLI&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The command-line interface is the primary and most frequently used way of
communicating with the server&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/1/engine-components-flow.png&quot; alt=&quot;Engine components&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://docs.docker.com/engine/docker-overview/#docker-engine&quot;&gt;Not your regular engine&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;Basically, you use the CLI to talk and interact with the Docker daemon, through
the REST API. The daemon then creates all the necessary resources such as
containers, networks and volumes. The client and the server may coexist on the
same machine, or they may also be on different hosts.&lt;/p&gt;

&lt;p&gt;Docker is composed of many different things at an engine-level too. It is
Docker’s plan to &lt;a href=&quot;https://blog.docker.com/2015/06/runc/&quot;&gt;separate and release all of its infrastructure plumbing
software&lt;/a&gt;, and they’re doing a great job so far. As of Docker 1.12, the Engine
is decomposed and built upon two different tools: &lt;a href=&quot;https://runc.io/&quot;&gt;runC&lt;/a&gt; and &lt;a href=&quot;https://containerd.io/&quot;&gt;containerd&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, runC is a CLI tool for running containers. The Docker Engine used to do
that before. First by using &lt;a href=&quot;https://linuxcontainers.org/&quot;&gt;LXC&lt;/a&gt;, and then with &lt;a href=&quot;https://github.com/opencontainers/runc/tree/master/libcontainer&quot;&gt;libcontainer&lt;/a&gt;. Nowadays,
libcontainer is still used, but only by runC. Another cool thing about this is
that runC is able to run Open Container Initiative (&lt;a href=&quot;https://www.opencontainers.org/&quot;&gt;OCI&lt;/a&gt;) containers. OCI is a
standardisation initiative which specify a common interface for containers
images and container engines, effectively enabling any container build with a
any OCI-compatible tool to run on any OCI-compatible engine. In other words,
these are containers that abide by a de jure container standard.&lt;/p&gt;

&lt;p&gt;What is de jure, you say? Check it:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;De facto is a Latin phrase that means in fact (literally by or
from fact) in the sense of “in practice but not necessarily ordained
by law” or “in practice or actuality, but not officially established”,
as opposed to de jure.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So basically, “De facto” is used, whereas “De jure” comes from a widely
accepted standard or law. Cool, huh? This is great, since it will theoretically
allow containers created with one engine to be run on a different engine.
Therefore its name, Open Container Initiative.At the time of writing this
article, the main OCI-compatible tools are Docker, and Rocket (Rkt).&lt;/p&gt;

&lt;p&gt;containerd is a daemon that uses runC to manage containers. It exposes a CRUD
container interface using gRPC, with a very simple API. This way, all the
container lifecycle actions (like starting, stopping, pausing or destroying
containers) are delegated by the Docker Engine to containerd which uses runC to
execute all its container-related actions. The Engine still manages
volumes, networks and builds, but containers are now containerd’s territory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/1/runc-containerd.png&quot; alt=&quot;Engine components&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://medium.com/@tiffanyfayj/docker-1-11-et-plus-engine-is-now-built-on-runc-and-containerd-a6d06d7e80ef&quot;&gt;Like a layer cake&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;There’s also another component called containerd-shim which sits between
containerd and runC, acts as the container’s process parent and allow the
runtime (runC) to exit after starting the containers. This whole thing is
pretty funny, because the Docker Engine is not able to run containers by itself
anymore: it delegates all these tasks to runC and containerd (and
containerd-shim, somehow).&lt;/p&gt;

&lt;p&gt;That’s about it for the Docker architecture. Let’s talk about Swarm now.&lt;/p&gt;

&lt;h2 id=&quot;swarm-architecture-and-concepts&quot;&gt;Swarm architecture and concepts&lt;/h2&gt;

&lt;p&gt;A Swarm is created by one or many Docker Engines, which uses swarmkit for all
the cluster-management and orchestration features. You can enable Swarm mode by
either creating a new Swarm or joining an existing Swarm.&lt;/p&gt;

&lt;p&gt;When using Swarm, you don’t launch single containers, but rather services. But
before talking about that, I need to talk about nodes.&lt;/p&gt;

&lt;p&gt;A node is just a Docker Engine that is a member of the Swarm. There are two
types of nodes: Managers and Worker nodes. A Manager node receives a service
definition and then it dispatches tasks to Worker nodes accordingly. They also
do the orchestration and cluster management functions required to maintain the
desired state of the swarm. There may be many Manager nodes on a Swarm, but
there is only one leader, which is elected by all the other Manager nodes using
the &lt;a href=&quot;https://en.wikipedia.org/wiki/Raft_(computer_science)&quot;&gt;Raft algorithm&lt;/a&gt; and which performs all the orchestration tasks.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/GpkVwipWeyutZySkYL7NpXPDs.js&quot; id=&quot;asciicast-GpkVwipWeyutZySkYL7NpXPDs&quot; async=&quot;&quot; height=&quot;5&quot;&gt;&lt;/script&gt;

&lt;p&gt;Worker nodes receive and execute tasks from Manager nodes. By default, Manager
nodes are also Worker nodes, but they can be configured to not accept any
workload &lt;strong&gt;&lt;em&gt;&lt;del&gt;(just like real-life managers)&lt;/del&gt;&lt;/em&gt;&lt;/strong&gt; therefore becoming acting as
Manager-only nodes. There is also an agent on every Worker node, which reports
on the state of its tasks to the Manager &lt;strong&gt;&lt;em&gt;&lt;del&gt;(so basically, middle-management)&lt;/del&gt;&lt;/em&gt;&lt;/strong&gt;.
That way, the Manager can maintain the desired state of the cluster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-docker/1/swarm-diagram.png&quot; alt=&quot;Swarm diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A service is the definition of one (or many) tasks to be executed on Worker
nodes.&lt;/em&gt; When creating a service, you need to specify which container image to
use, and which commands to execute inside the containers. A service may be
global, or replicated. When the service is global, it will run on every
available node once. When it’s replicated, the Manager distributes the given
number of tasks on the nodes based on the desired scale number. This number may
also be one.&lt;/p&gt;

&lt;p&gt;A task is just a container and the commands to be run inside of the container.
It is the standard scheduling unit of Swarm. Once a task is assigned to a node,
it can’t be moved to another node. It either runs on the selected node, or
fails.&lt;/p&gt;

&lt;p&gt;So that’s about it for this first chapter. On the next episode, I’ll talk to
you about Docker (and Swarm’s) networking model.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;

</description>
          <pubDate>2017-06-08T21:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-docker-1/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-docker-1/</guid>
        </item>
      
    
      
        <item>
          <title>Enforcing strict EC2 instance tagging policies with Lambda</title>
          <description>&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;I coded something in order to enforce strict tagging policies on AWS EC2 instances using
Python and a bunch of AWS services (Lambda, Cloudtrail, SNS, and S3). &lt;strong&gt;If you keep reading,
I’m going to talk to you about AWS Lambda and Serverless computing, or FaaS (Function as a service).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can check the source code and permission related template files here:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://github.com/sebiwi/broom
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you want to use it, create a Cloudtrail trail, activate it on every region, create an S3 bucket
to store the Cloudtrail logs, create a SNS topic for notifications, create a Lambda function using
the Python code (adapted to your resources and use case), an IAM role using the policy that comes
with it, and finally activate events from the S3 bucket to the Lambda function on object creation.&lt;/p&gt;

&lt;p&gt;I can teach you how to do all of these things if you don’t know how. Just keep reading.&lt;/p&gt;

&lt;h2 id=&quot;why&quot;&gt;Why?&lt;/h2&gt;

&lt;p&gt;I like to be thorough and organized with my cloud resources. I usually work with and manage several
shared AWS accounts for different teams. Without a proper tagging workflow, the EC2 resource set can
become a mess. You’ll probably find yourself asking your team who created an untagged instance, and
if they are still using it. That feels kinda dull, since we’re in 2017 and all.&lt;/p&gt;

&lt;p&gt;What do I want in order to fix this? I want to analyze each instance that is created on my AWS account,
and destroy it if it is not properly tagged. I also want to get a notification when this happens, with
useful information about the instance creation request, such as the instance type, or the username of
the user that created the resource.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/broom/broom.png&quot; alt=&quot;Broom&quot; class=&quot;center-image&quot; width=&quot;140px&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Keep your stuff clean!&lt;/figcaption&gt;

&lt;p&gt;I decided to call this thing Broom, since I use it to keep my things clean. Get it?&lt;/p&gt;

&lt;p&gt;To be honest, I also wanted to take Lambda for a ride, since Serverless is pretty trendy nowadays.
Let us discuss it.&lt;/p&gt;

&lt;h2 id=&quot;serverless-and-lambda&quot;&gt;Serverless and Lambda&lt;/h2&gt;

&lt;p&gt;Serverless is another paradigm in the Something-as-a-Service universe. It allows you to execute code
(functions) on the Cloud, without worrying about the underlying servers, middleware or configuration.
The basic premise is simple: your code is executed only when it is needed. You’re also billed to the
execution time of your function, so you actually pay (almost) exactly what you use.&lt;/p&gt;

&lt;p&gt;Even though the paradigm is called Serverless, this doesn’t mean that your code is executed without
any servers. It means that you don’t have to provision, configure or manage the servers that are used
to run your code yourself, and that your provider is going to do it for you. A corollary to the paradigm
is the fact that your code can scale automatically (both up and down) indefinitely according to the
 demand. The only constraint is that your code must be stateless.&lt;/p&gt;

&lt;p&gt;Since Serverless is right now the new fad of IT, there are many new frameworks adopting the model.
For example, &lt;a href=&quot;http://fission.io/&quot;&gt;Fission&lt;/a&gt; is a framework for serverless functions on Kubernetes. If you don’t know
what Kubernetes is, or how it works, you can check it out over &lt;a href=&quot;https://sebiwi.github.io/blog/how-does-it-work-kube-1/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And Lambda? Well, Lambda is AWS serverless computing service. It originally came out in 2014, and
only supported NodeJS at the time. Nowadays it has Python, Java and C# support, and it can also run
native Linux executables, if you can run them from within one of the supported languages/frameworks.
This allows you to run Go and Haskell binaries on Lambda, for example, if you do some hacking. In
terms of billing, Lambda is metered in increments of 100 milliseconds, which is almost negligible
compared to the EC2 minimum usage fee of one hour.&lt;/p&gt;

&lt;p&gt;How does this work? How can AWS spin up EC2 instances, deploy your code in them, and answer
requests that fast? &lt;em&gt;How&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/broom/containers.jpg&quot; alt=&quot;Broom&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Get the hint?&lt;/figcaption&gt;

&lt;p&gt;It’s all about containers, really. When a Lambda function is triggered, Lambda launches a container
with your code and the necessary dependencies in it. This takes some time, but its amount is
negligible compared to the normal server provisioning delay. The container is also reused (on a
best-effort basis) for further invocations of the function, if no changes have been made to
either the code or the configuration.&lt;/p&gt;

&lt;p&gt;Exciting, isn’t it? You wanna know how to set it up? Let’s go!&lt;/p&gt;

&lt;p&gt;Just so you know what we’re going to do next: we’re going to log every API call made on AWS,
put these logs on an S3 bucket, launch a Lambda function that analyzes newly-created instances
for certain tags whenever a new log file is created on the bucket, and publish notifications
on an SNS topic whenever an instance is destroyed due to lack of tags.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: we’re going to be creating a list of AWS resources. Remember to create them on the same region whenever it is possible.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;first-up-see-the-trail&quot;&gt;First up, see the trail&lt;/h2&gt;

&lt;p&gt;We need to be able to trace the AWS API calls in the target AWS account so we can see when
EC2 instances are actually created. You can do this with Cloudtrail.&lt;/p&gt;

&lt;p&gt;From the &lt;a href=&quot;https://aws.amazon.com/cloudtrail/faqs/&quot;&gt;Cloudtrail&lt;/a&gt; FAQs:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;AWS CloudTrail is a web service that records API calls made on your account and delivers
log files to your Amazon S3 bucket
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can create a Cloudtrail trail either using the AWS cli tool:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws cloudtrail create-trail --name broom-cloudtrail --s3-bucket-name broomtrail-bucket --is-multi-region-trail
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Or the AWS web console:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/broom/cloudtrail-console.jpg&quot; alt=&quot;AWS CloudTrail Console&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can create a new S3 bucket in the same single step if you’re using the console. You’ll
have to do it separately if you’re using the CLI. You can also configure an SNS topic in
order to receive notifications each time a log object is stocked in the S3 bucket. &lt;strong&gt;This
is hell&lt;/strong&gt;, and I encourage you not to do it.&lt;/p&gt;

&lt;p&gt;Remember to create a multi-region trail, since we want to be able to audit instances in all regions.&lt;/p&gt;

&lt;h2 id=&quot;notify-me-then&quot;&gt;Notify me then!&lt;/h2&gt;

&lt;p&gt;Next up, we create an SNS (Simple Notification Service) topic in order to receive notifications
whenever instances are destroyed. Same as before, you can do it using either the CLI or the console:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws sns create-topic --name broom-topic
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Remember to note the ARN of your topic, since you will be using it for the next step:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ResponseMetadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;RequestId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1469e8d7-1642-564e-b85d-a19b4b341f83&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;TopicArn&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arn:aws:sns:region:weird_number:topic_name&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/broom/sns-topic.png&quot; alt=&quot;SNS console&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Console version&lt;/figcaption&gt;

&lt;p&gt;Don’t forget to subscribe to this topic if you want to receive notifications. You probably do, so do it.&lt;/p&gt;

&lt;h2 id=&quot;know-your-role&quot;&gt;Know your role&lt;/h2&gt;

&lt;p&gt;The Lambda function needs to be able to access some things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cloudwatch, in order to stock execution logs&lt;/li&gt;
  &lt;li&gt;The Cloudtrail S3 bucket, in order to recover the log files&lt;/li&gt;
  &lt;li&gt;The EC2 API, in order to list instance tags and destroy instances when necessary&lt;/li&gt;
  &lt;li&gt;The previously created SNS topic, in order to send notifications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to do all of these things, we’ll create a policy using the template policy file that
comes with the project. You can find the template policy file here. You can use it almost as
it is, just remember to replace the SNS ARN with the one you got in the previous section:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;Effect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Allow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;Action&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sns:Publish&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;Resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arn:aws:sns:region:weird_number:topic_name&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then, you need to create a role, and attach the previously created policy to the role.
The Lambda function will then be executed using this role, and it will have access to all the
necessary resources.&lt;/p&gt;

&lt;h2 id=&quot;lambda-for-all&quot;&gt;Lambda for all&lt;/h2&gt;

&lt;p&gt;Finally, let’s create the actual Lambda function. You can create a package for your function and
then upload it directly to AWS using the CLI. This is useful when you are using dependencies that
are not standard and that are not already present in the AWS Lambda environment. This is not the
case with the Broom function, so you can just use the AWS console to create it. I used Python and
&lt;a href=&quot;http://boto3.readthedocs.io/en/latest/&quot;&gt;boto3&lt;/a&gt; for Broom. You can see the source code &lt;a href=&quot;https://github.com/sebiwi/broom/blob/master/broom.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Basically, I declare the SNS topic ARN, and the codes that I’ll be using to tag our instances. In
this case, it can be the codes assigned to each person that is currently working on the team.
I always refer to myself as ‘LOL’, for example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SNS_TOPIC_ARN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'arn:aws:sns:region:weird_number:topic_name'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;CODES&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'LOL'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'GGG'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'BRB'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'YLO'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then, I declare some helper functions, the first to decompress the Cloudtrail S3 log files:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;decompress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gzip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GzipFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fileobj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BytesIO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And the second one to generate a report on the destroyed instance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;instance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;User &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; created an instance on region &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;region&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; without proper tagging. &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Instance id: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;instance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'instanceId'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Image Id: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;instance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'imageId'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Instance type: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;instance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'instanceType'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;This instance has been destroyed.&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then, the lambda_handler function, which is the actual function that is going to be triggered by the S3 event.
First, I recover the bucket name and the object key:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lambda_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Records'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bucket'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;urllib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unquote_plus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Records'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'object'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'utf8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This might look a little bit weird. It’s only because the &lt;a href=&quot;http://docs.aws.amazon.com/AmazonS3/latest/dev/notification-content-structure.html&quot;&gt;event message structure&lt;/a&gt; is kinda complex.
Then, I recover the actual log (s3_object), decompress it, and create a JSON object with it:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;s3_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bucket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s3_object_content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s3_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Body'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s3_object_unzipped_content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decompress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s3_object_content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;json_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s3_object_unzipped_content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;A log object may contain many API calls. I’m only interested in the ones that create EC2
instances. That means the ones with the “RunInstances” event name. If I find one of these
events, I’ll connect to that region and recover the created instances:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Records'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'eventName'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;RunInstances&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'userIdentity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'userName'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;region&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'awsRegion'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ec2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boto3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ec2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;region_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;instance&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'responseElements'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'instancesSet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'items'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;instance_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ec2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Instance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;instance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'instanceId'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I’ll check if the tag is present in the instance, and if it exists in the list of valid
codes that I defined previously. If it is not, I’ll destroy the instance, and publish a
report to the SNS topic:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;instance_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Value'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Code'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Code'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CODES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;instance_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;terminate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;publish&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TopicArn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SNS_TOPIC_ARN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Message&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;instance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;That’s it! You can adapt this in order to check your instances in many different ways,
like a different tag, or a combination of many.&lt;/p&gt;

&lt;p&gt;Now, go to the Lambda section on the AWS console, and click on “Create a Lambda function”.
Many function blueprints will be proposed to you then. Just go with “Blank Function”, since
you already have everything that’s needed.&lt;/p&gt;

&lt;p&gt;On the “Configure triggers” section, choose S3, and select the Bucket you created previously.
On “Event type”, choose “Object Created”, since you want your function to be executed when
a new log object is created. You can enable the trigger right away, or afterwards, it’s up to you.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/broom/lambda-trigger.png&quot; alt=&quot;Lambda trigger&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Trigger happy&lt;/figcaption&gt;

&lt;p&gt;Then name your function, select the runtime, and include your code with it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/broom/lambda-function.png&quot; alt=&quot;Lambda trigger&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Be sure to specify the role you created earlier in the “Lambda function handler and role” section.
If you don’t, the function won’t execute properly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/broom/lambda-role.png&quot; alt=&quot;Lambda role&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can change the assigned resources and the maximal execution time in the advanced settings
section. I usually set this up to 128 MB and 20 second timeout, but it might depend on the
specific characteristics of your team.&lt;/p&gt;

&lt;p&gt;After that, create the function on the review page. That’s it, you’re all set!&lt;/p&gt;

&lt;p&gt;If you create an instance without proper tagging, you will receive a notification like this one:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/broom/sns-mail.png&quot; alt=&quot;SNS notifcation mail&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Whoops, sorry!&lt;/figcaption&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;You can see all the Lambda limits &lt;a href=&quot;http://docs.aws.amazon.com/lambda/latest/dg/limits.html&quot;&gt;here&lt;/a&gt;. For this use case, none of them are actually blocking.
There’s a 100 concurrent Lambda function execution limit which might be a problem if you have
a single account and people working on many different regions (shrug). This is actually a soft
limit, which is meant to avoid excessive charges without your approval. You can request a &lt;a href=&quot;http://docs.aws.amazon.com/lambda/latest/dg/limits.html&quot;&gt;limit
increase&lt;/a&gt; if you feel that the default limit is getting in your way.&lt;/p&gt;

&lt;p&gt;Mocking the necessary resources in order to test this was slightly cumbersome too. I had to
create valid logs, make sure that they had the necessary events in them, and then use the “Test”
option in the Lambda dashboard on the AWS console, while specifying the mocked log in the test
JSON object. I’d like to have a way to make this simpler somehow.&lt;/p&gt;

&lt;p&gt;I could have used a serverless framework to address this issue during development: something like
&lt;a href=&quot;https://serverless.com/&quot;&gt;Serverless&lt;/a&gt;, or &lt;a href=&quot;https://github.com/awslabs/chalice&quot;&gt;chalice&lt;/a&gt;. I’ll definitely try one the next time I do some serverless development.
These really come in handy, as you can test your code locally, generating input events without
setting up the whole infrastructure. They also help you manage and update multiple environments,
and create the resources needed for your function, with &lt;a href=&quot;https://serverless.com/framework/docs/providers/aws/guide/resources/#configuration&quot;&gt;Cloudformation&lt;/a&gt;. You can also automate
the creation of these Lambda management functions using a separate infrastructure as code tool,
like Terraform.&lt;/p&gt;

&lt;p&gt;Anyways, I hope you enjoyed that as much as I did!&lt;/p&gt;

</description>
          <pubDate>2017-05-01T21:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/broom/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/broom/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Kubernetes! Episode 5 - Master and worker, at last!</title>
          <description>&lt;h2 id=&quot;lets-install-our-master-node&quot;&gt;Let’s install our Master node!&lt;/h2&gt;

&lt;p&gt;Absolutely.&lt;/p&gt;

&lt;p&gt;So basically, we need to create an API server, a Scheduler, and a Controller Manager
Server on the Master node.&lt;/p&gt;

&lt;p&gt;First, we’ll add the TLS resources needed for the master node. That means the CA
certificate and the API server certificate and key. Nothing complicated about that.&lt;/p&gt;

&lt;p&gt;Next up, networking. For this we’ll use Flannel and Calico.&lt;/p&gt;

&lt;p&gt;In order to configure Flannel, we just add configuration environment variables under
&lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/flannel/options.env&lt;/code&gt;. These specify that the flannel interface is this node’s public
IP, and that the cluster configuration is stocked in etcd cluster:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FLANNELD_IFACE={{ ansible_env.COREOS_PUBLIC_IPV4 }}
FLANNELD_ETCD_ENDPOINTS={{ etcd_endpoints }}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/etc_flannel_options.env.j2&quot;&gt;/etc/flannel/options.env&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;Then, we add a system-drop in (a method for adding or overriding parameters of a systemd
unit) for flannel, in which we specify that we want to use the configuration specified
above when the service launches:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Service]
ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/etc_systemd_system_flanneld.service.d_40-ExecStartPre-symlink.conf.j2&quot;&gt;/etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;System drop-ins are pretty cool because they only modify the specific settings you modified,
and everything else stays the same.&lt;/p&gt;

&lt;p&gt;The flannel configuration is actually stored in etcd. We create it under the coreos.com/network/config
namespace using a simple &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/tasks/configure.yml#L109-L117&quot;&gt;uri task&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After that, we gotta configure Docker on the virtual machine. Actually, the only thing we need is to
be sure that flannel is used for networking. Basically, flanneld needs to be running when Docker starts:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Requires=flanneld.service
After=flanneld.service
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/etc_systemd_system_flanneld.service.d_40-ExecStartPre-symlink.conf.j2&quot;&gt;/etc/systemd/system/docker.service.d/40-flannel.conf&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;Now that the basic requirements are configured, we’re going to configure a whole set of components that are
necessary in order to run a Kubernetes cluster: the kubelet, the Kubernetes Proxy, the Controller
manager, and the Scheduler. With the exception of the kubelet, all the other components will be
deployed in a container form. This is why I said in the first article that the Master node actually
does run some containers. How cool is that?&lt;/p&gt;

&lt;p&gt;First up, the kubelet. This is the agent on the node that actually starts and stops containers,
and communicates with the Docker engine at a host level. It is present on every node in a Kubernetes
cluster: both master and worker nodes. It talks with the API server using the certificates we created
earlier. The kubelet does not manage containers that are not created by Kubernetes. The master
node configuration of the kubelet does not register for cluster work (since it is a master and not a worker).&lt;/p&gt;

&lt;p&gt;The kubelet may use different standards for networking. One of these standards is the Container Network
Interface, or CNI. The CNI is a set of specifications and libraries for writing plugins to configure
network interfaces in Linux containers. The only concern of CNI is network connectivity of these
containers, and then removing allocated resources when the containers are deleted.&lt;/p&gt;

&lt;p&gt;So when using Calico, the kubelet uses CNI for networking. Calico is aware of each pod that is
created, and it allows them into the flannel SDN. Both flannel and Calico communicate using
CNI interfaces to ensure that the correct IP range is used for each node.&lt;/p&gt;

&lt;p&gt;The kubelet configuration can be seen &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/services/kubelet.service.j2&quot;&gt;here&lt;/a&gt;. As of this moment configuration files start to
get really verbose so I’ll just mention the most important parts. This one specifies the
address of the API server, the network plugin to use, the DNS service address and general
kubelet configuration, like log files location and configuration directories. The
configuration also creates a “manifests” directory, and watches it. This means that for
every Pod manifest that is stored in that location, a matching Pod will be created by
the kubelet, just as if the Pod was submitted via the API. We will take advantage of
this functionality extensively from now on.&lt;/p&gt;

&lt;p&gt;After that, we gotta set up the API server. This stateless server takes in requests,
process them and stores the result in etcd. It’s one of the main components of the
Kubernetes cluster. What we would normally do in order to create a server of this
kind is to send a request to the API server, with the Pod manifest of the Pod we want
to create. But we don’t have an API server yet. Huh.&lt;/p&gt;

&lt;p&gt;We’re going to use the manifest directory that we mentioned on the previous paragraph.
When we place the API server manifest inside of the “manifests” directory, it will be
automatically created as soon as the kubelet is launched. Pretty neat, huh? We’re
going to use this same strategy for the Proxy, the Controller manager and the Scheduler.&lt;/p&gt;

&lt;p&gt;The API server configuration is pretty long and might be confusing at first.
Between many other things, it needs to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Be accessible on the host machine address&lt;/li&gt;
  &lt;li&gt;Be able to access etcd&lt;/li&gt;
  &lt;li&gt;Be aware of the service range we’re going to use for service IPs&lt;/li&gt;
  &lt;li&gt;Access the SSL certificates we created earlier&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And so on. If you want to take a look at the configuration, the template is right &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/manifests/kube-apiserver.yaml.j2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then, there’s the Kube Proxy. It redirects traffic directed to specific services and pods
to their destination. It talks to the API server frequently. In this case, it is used in
order to access the API server from the outside. It’s configuration can be found &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/manifests/kube-proxy.yaml.j2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s take on the Controller manager. This component basically applies the necessary
changes based on the Replication Controllers. When you increase or decrease the replica
count of a pod, it sends a scale up/down event to the API server, and then new containers
are scheduled by the Scheduler. It’s configuration can be found &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/manifests/kube-controller-manager.yaml.j2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Last but not least, we need to add the &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/manifests/kube-scheduler.yaml.j2&quot;&gt;Scheduler&lt;/a&gt; configuration. This component
watches the API for unscheduled Pods, then he finds a machine for them to run and
informs the API server of the best choice.&lt;/p&gt;

&lt;p&gt;We haven’t configured Calico yet, have we? Let’s add it as a service. We will create
the “/etc/systemd/system/calico-node.service” file. It’s configuration can be found &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/services/calico-node.service.j2&quot;&gt;here&lt;/a&gt;.
Basically, it talks with etcd in order to store information. Now every container launched
will be able to connect itself to the flannel network with its own IP address, and
policies created using the policy API will be enforced. Calico also needs a
policy-controller in order to work. This component will watch the API and look
for changes in the network policy, in order to implement them on Calico. It’s
configuration can be found &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/manifests/policy-controller.yaml.j2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, (yes, this time it’s true) the kubelet service configuration specified a
cni configuration file we need to create. This file specifies which CNI plugin needs
to be called on startup. This creates the flannel plugin, but then delegates control
to the Calico plugin. This might sound convoluted at first, but it is actually done
so that Calico knows which IP range to use (which is determined before by flannel).
It’s a pretty short configuration so I’ll just put it here:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;calico&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;flannel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;delegate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;calico&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;etcd_endpoints&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{{ etcd_endpoints }}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;log_level&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;none&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;log_level_stderr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;info&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;hostname&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{{ ansible_env.COREOS_PUBLIC_IPV4 }}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;policy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;k8s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;k8s_api_root&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://127.0.0.1:8080/api/v1/&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/etc_kubernetes_cni_net.d_10-calico.conf.j2&quot;&gt;Easy&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;After that, we’ll just start the services and cross our fingers (don’t worry, it will work).&lt;/p&gt;

&lt;p&gt;You can see that this role actually has &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/tasks/main.yml#L11-L12&quot;&gt;another yaml file embedded into the main.yml file&lt;/a&gt;.
It is called namespaces.yml.  It is included because we need to create a Kubernetes
namespace for the Calico policy-controller to run (we specified that &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/templates/manifests/policy-controller.yaml.j2#L6&quot;&gt;here&lt;/a&gt;). And that
needs to be done after the API server starts responding, since it is a Kubernetes
functionality. So we just &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-master/tasks/namespaces.yml#L23&quot;&gt;create the calico-system namespace if it doesn’t exist already&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By the way, this Master node is not highly available. In order for to have high availability
on the Master node, I would need to add a component in order to manage the virtual IP of
the master node, like &lt;a href=&quot;http://www.keepalived.org/&quot;&gt;keepalived&lt;/a&gt;, or to handle it with &lt;a href=&quot;https://github.com/coreos/fleet&quot;&gt;fleet&lt;/a&gt;. I might do something
about this in the future.&lt;/p&gt;

&lt;p&gt;That’s all for the master node configuration. We should have a master Kubernetes node running
alive and well by now. Are you tired? There’s still more!&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-worker-node&quot;&gt;Kubernetes worker node&lt;/h2&gt;

&lt;p&gt;The master node does not run any containers, it just handles and manages the cluster. The
nodes that actually run the containers are the worker nodes. We’re going to configure two of them.&lt;/p&gt;

&lt;p&gt;We’ll just start by configuring the SSL resources the same way we did it on the Master node.
Nothing new here. The code just puts them under the “/etc/kubernetes/ssl” directory. Moving on.&lt;/p&gt;

&lt;p&gt;Network-wise, we’ll use flannel the same way we did on the Master node. I didn’t create a flannel
role because I figured that the flannel configuration might change from Master to Worker in the
next Kubernetes release (turns out it did with 1.5!). Same with the Docker configuration. We just
want flannel to be running before we run Docker.&lt;/p&gt;

&lt;p&gt;Next up, the kubelet. We will not disable the &lt;code class=&quot;highlighter-rouge&quot;&gt;register for cluster&lt;/code&gt; work flag, since we want
for these nodes to do the heavy lifting. We will also configure it to talk to the master node,
to use the CNI network plugin, the specified DNS server, and its own advertising IP. &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-worker/templates/services/kubelet.service.j2&quot;&gt;Here’s&lt;/a&gt; the
configuration if you want to check it out.&lt;/p&gt;

&lt;p&gt;We’re going to tell the kubelet to call flannel as a CNI plugin, and then to delegate the control
to Calico, the same way we did it on the master node. We’ll need to specify the master node’s IP
on the configuration here instead of localhost, since the the configuration needs to access the
API server. The configuration template can be seen &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-worker/templates/etc_kubernetes_cni_net.d_10-calico.conf.j2#L4-L5&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As we said before, there’s a kube-proxy instance in each node. That means there’s one on the
worker nodes too. It’s configuration specifies the master nod. Nothing fancy.
It’s configuration can be found &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-worker/templates/manifests/kube-proxy.yaml.j2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’re going to add a kube-config configuration, in order to specify the TLS resources that
are necessary for secure communication between the different Kubernetes components.
It can be seen &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-worker/templates/etc_kubernetes_worker-kubeconfig.yaml.j2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we need a Calico Node Container too, that will fulfil the same role it did on
the Master node. It’s configuration can be found &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-worker/templates/services/calico-node.service.j2&quot;&gt;here&lt;/a&gt;. After that, we start every service,
and we should have our full Kubernetes up and running. It might take a while, depending
on your internet connection.&lt;/p&gt;

&lt;p&gt;Let’s recap for a second. What just happened?&lt;/p&gt;

&lt;p&gt;We created one etcd node (if you didn’t touch the defaults), then we spawned one Kubernetes
master node that uses the etcd cluster to persist the cluster state, and two Kubernetes
worker nodes that can host container workloads. Yay!&lt;/p&gt;

&lt;p&gt;We still need a way of interacting with the cluster. For that we’ll use the standard
Kubernetes management tool, kubectl.&lt;/p&gt;

&lt;h2 id=&quot;configuring-kubectl&quot;&gt;Configuring kubectl&lt;/h2&gt;

&lt;p&gt;You can download kubectl and put it wherever you want in your computer. Try to add it to
a directory that’s in your PATH, so that you don’t have to reference the whole path every
time you do a kubectl action. Make sure it is executable, too.&lt;/p&gt;

&lt;p&gt;After that you can configure it by specifying the certificates and the Master host’s URL:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Verify if kubectl is already configured&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubectl cluster-info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cluster_info&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;failed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Set default cluster&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubectl config set-cluster default-cluster --server=https://{{ master_host }} --certificate-authority=ca.pem&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;chdir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kube_resource_dir&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}}/ca&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Kubernetes&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;master'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cluster_info.stdout&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Set credentials&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubectl config set-credentials default-admin --certificate-authority=ca.pem --client-key=admin-key.pem --client-certificate=admin.pem&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;chdir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kube_resource_dir&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}}/ca&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Kubernetes&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;master'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cluster_info.stdout&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Set context&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubectl config set-context default-system --cluster=default-cluster --user=default-admin&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;chdir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kube_resource_dir&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}}/ca&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Kubernetes&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;master'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cluster_info.stdout&quot;&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Use context&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubectl config use-context default-system&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;chdir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{{&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kube_resource_dir&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;}}/ca&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Kubernetes&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;master'&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cluster_info.stdout&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kubectl/tasks/main.yml&quot;&gt;There is no kube&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;This configuration is only applied when the cluster is not configured, though. That way, we keep
our idempotence neat and clean.&lt;/p&gt;

&lt;p&gt;That was easy, kinda. Now what? Oh yeah, cool stuff on our cluster.&lt;/p&gt;

&lt;h2 id=&quot;add-ons&quot;&gt;Add-ons&lt;/h2&gt;

&lt;p&gt;Add-ons are… well… things you add-on to your Kubernetes cluster in order to have improved
functionality. They are created using Kuberntetes native resources. Most of the time they will
be pods, so we can juste create a manifest for them and then create them using the API server (through kubectl).&lt;/p&gt;

&lt;p&gt;There are two add-ons that are commonly used on almost every standard Kubernetes installation: the
DNS add-on, and the Kubernetes Dashboard add-on.&lt;/p&gt;

&lt;p&gt;The first enables service discovery for your containers. They can have a DNS name, and they
can be reached by other containers with it. The manifest is huge. It’s because we’re creating
a Service, which provides DNS lookups over port 53 to every resource that demands it, and
also a replication controller, which makes sure that there is one replica of the pod at all
times. The configuration can be seen &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kube-components/templates/dns-addon.yml.j2&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Dashboard allows you to see general information about your cluster. That means Pods,
Services, Replication Controllers, and all that under different namespaces. It’s a pretty
cool tool, honestly. We’ll create a Service and a Replication Controller for it too.&lt;/p&gt;

&lt;p&gt;You can access the Kubernetes Dashboard by using the port forwarding functionality of kubectl:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods --namespace&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-system
kubectl port-forward kubernetes-dashboard-v.1.4.1-ID 9090 --namespace&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-sytem
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And now, your Kubernetes Dashboard should be accessible on port 9090.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/5/kube-dashboard.png&quot; alt=&quot;Kubernetes dashboard&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Show me the money!&lt;/figcaption&gt;

&lt;p&gt;Woot! Almost done. Now we only need to test that it works, and that our code is working as intended.&lt;/p&gt;

&lt;p&gt;So, &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule test&lt;/code&gt; says that:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/5/molecule-1.png&quot; alt=&quot;Molecule infra&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our infrastructure is created without any hiccups.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/5/molecule-2.png&quot; alt=&quot;Molecule playbooks&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The playbook runs as intended.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/5/molecule-3.png&quot; alt=&quot;Molecule lint&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And that our code is properly linted, and it is idempotent as well!&lt;/p&gt;

&lt;p&gt;Let’s have some fun with our cluster now.&lt;/p&gt;

&lt;h2 id=&quot;sample-app&quot;&gt;Sample app&lt;/h2&gt;

&lt;p&gt;We’re going to use the &lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/release-1.4/examples/guestbook/README.md&quot;&gt;guestbook example&lt;/a&gt; that’s included on the Kubernetes samples.
This is a good application to test a proper installation, since it’s a multi-tier
application, with multiple components speaking to each other, on different nodes.&lt;/p&gt;

&lt;p&gt;The guestbook application is created under the kubernetes-resources directory.
It can be launched using kubectl:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f guestbook.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can see that the resources are properly created on the Dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/5/dashboard-guestbook.png&quot; alt=&quot;Molecule lint&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then we can even test the application by port-forwarding to the frontend application:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods
kubectl port-forward frontend-ID 8080:80
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The application should be accessible on port 8080. You can test it by adding a message to the guestbook:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/5/guestbook.png&quot; alt=&quot;Kubernetes dashboard&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Get it?&lt;/figcaption&gt;

&lt;p&gt;Great. It works. Mission accomplished!&lt;/p&gt;

&lt;p&gt;So what did we learn in the end?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CoreOS (Container Linux) is a lightweight Linux distribution that runs almost everything inside of containers&lt;/li&gt;
  &lt;li&gt;An “Ansible” is a fictional device capable of instantaneous or superluminal communication. It is also a pretty powerful IT automation tool.&lt;/li&gt;
  &lt;li&gt;You can move around on remote hosts using Ansible, even when you don’t have Python installed on them&lt;/li&gt;
  &lt;li&gt;etcd is a distributed key-value store, which is used as the standard distributed storage by Kubernetes&lt;/li&gt;
  &lt;li&gt;Flannel and Calico can be used together to provide SDN-based connectivity and network policies for containers located on different hosts&lt;/li&gt;
  &lt;li&gt;You can use Molecule to continuously test important aspects of your Ansible code&lt;/li&gt;
  &lt;li&gt;The egg came before the chicken&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;Phew, yeah, that was kinda long. Anyways, I hope you had fun. I know I did. Still, I would have loved to 
work on some other things too:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use etcd3 instead of etcd2&lt;/li&gt;
  &lt;li&gt;Install Python “The CoreOS way”. The PyPy installation works, but it feels kinda hacky. I’d have loved to run Python from inside a container and provision the host with Ansible somehow&lt;/li&gt;
  &lt;li&gt;Use SSL on the etcd cluster&lt;/li&gt;
  &lt;li&gt;Use fleet for certain tasks, like deploying unit files, or handling the Master node high availability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I might work on these things in the future. Anyways, see you on the next adventure!&lt;/p&gt;

</description>
          <pubDate>2017-04-03T21:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-kube-5/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-kube-5/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Kubernetes! Episode 4 - How to Ansible your CoreOS, and etc(d)!</title>
          <description>&lt;h2 id=&quot;can-i-see-the-code-now&quot;&gt;Can I see the code now?&lt;/h2&gt;

&lt;p&gt;Right, code. The first step is to actually create the CoreOS virtual machines. I used a really
simple Vagrantfile in which I specify how many instances I want, and how much computing
resources each one of them is going to have:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# General cluster configuration&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$etcd_instances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$etcd_instance_memory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$etcd_instance_cpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$kube_master_instances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$kube_master_instance_memory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$kube_master_instance_cpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$kube_worker_instances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$kube_worker_instance_memory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;
&lt;span class=&quot;vg&quot;&gt;$kube_worker_instance_cpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/Vagrantfile#L4-L13&quot;&gt;Amazing complexity&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;You can modify the amount of instances you want to create in this part. Be aware that if you
add hosts, you will also need to add them in the inventory for the Ansible code to target
all the machines.&lt;/p&gt;

&lt;p&gt;I also created a simple IP addressing plan. The principle is that each machine subtype is
going to have less than 10 nodes. So I just number them from 10.0.0.1x1 to 10.0.0.1x9, with
x being 0 for the etcd nodes, 1 for the Kubernetes master nodes, and 2 for the Kubernetes
worker nodes:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Kubernetes Master instances configuration&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;vg&quot;&gt;$kube_master_instances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;define&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vm_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;kube-master-%02d&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Name&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vm_name&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# RAM, CPU&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;provider&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:virtualbox&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;gui&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;false&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;memory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;vg&quot;&gt;$kube_master_instance_memory&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;vb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;vg&quot;&gt;$kube_master_instance_cpus&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# IP&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:private_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;ip: &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10.0.0.&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;110&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/Vagrantfile#L47-L63&quot;&gt;Absolute power&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;By the way, you can configure your virtual machines by specifying an Ansible playbook, using
the Ansible provisioner in your Vagrantfile &lt;a href=&quot;https://www.vagrantup.com/docs/provisioning/ansible.html&quot;&gt;like this&lt;/a&gt;. I choose not to do so because
I like having my &lt;code class=&quot;highlighter-rouge&quot;&gt;vagrant up&lt;/code&gt; actions separate from my actual configuration.
Extra coupling does not abide by this philosophy.&lt;/p&gt;

&lt;p&gt;Now you’re only a &lt;code class=&quot;highlighter-rouge&quot;&gt;vagrant up&lt;/code&gt; away from having your CoreOS virtual machines running on
your computer. After that, you can export the SSH configuration used by Vagrant with
&lt;code class=&quot;highlighter-rouge&quot;&gt;vagrant ssh-config &amp;gt; ssh.config&lt;/code&gt;. You can then use this configuration for the Ansible
configuration, if you include it in your ansible.cfg file:&lt;/p&gt;

&lt;div class=&quot;language-ini highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;[ssh_connection]&lt;/span&gt;
&lt;span class=&quot;py&quot;&gt;ssh_args&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;-F ssh.config&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/ansible.cfg#L7-L8&quot;&gt;Like this&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;I really like Makefiles. I use them quite often when I have write more than one long command, or many different ones that are going to take a while. I’m also kinda lazy. So I just did this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vagrant:
	@vagrant up
	@vagrant ssh-config &amp;gt; ssh.config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/Makefile#L5-L7&quot;&gt;make: automating your life since 1977&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;This way, &lt;code class=&quot;highlighter-rouge&quot;&gt;vagrant up &amp;amp;&amp;amp; vagrant ssh-config &amp;gt; ssh.config&lt;/code&gt; becomes &lt;code class=&quot;highlighter-rouge&quot;&gt;make vagrant&lt;/code&gt;, which saves
me up to 3,7 seconds every day.&lt;/p&gt;

&lt;p&gt;You can check the &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/Vagrantfile&quot;&gt;Vagrantfile&lt;/a&gt;, the &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/ansible.cfg&quot;&gt;ansible.cfg&lt;/a&gt; and the &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/Makefile&quot;&gt;Makefile&lt;/a&gt; on the GitHub repo.&lt;/p&gt;

&lt;p&gt;Let’s set up our testing configuration now.&lt;/p&gt;

&lt;p&gt;Molecule uses a yaml configuration file called molecule.yml. You can use it to define the
configuration of your test infrastructure, and to specify the playbook that you want to test.
Usually, I try to test each role independently. That means that I’ll have a molecule.yml
file per role directory, and also a playbook.yml file that uses the role that I’m testing.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker-gc/
├── molecule.yml
├── playbook.yml
├── tasks
└── templates
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;Like this&lt;/figcaption&gt;

&lt;p&gt;The molecule.yml file specifies an infrastructure (I usually use a single Docker container),
and it also specifies which playbook needs to be used on the previously defined infrastructure.
For the example in the image above, the playbook.yml file would just include the docker-gc role.
Molecule also does this for you automatically if you type &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule init --driver docker&lt;/code&gt;
inside of your role directory. Dope.&lt;/p&gt;

&lt;p&gt;This is a pretty good testing strategy when you have roles that are independent from each other.
It works like a charm when I test my docker-gc role. The thing is that for this project, the
different components depend on each other. I cannot test that my kube-master role is working
if I don’t have an etcd cluster running. I cannot test that my kube-worker role is working
if I don’t have at least one Kubernetes master node. Tricky.&lt;/p&gt;

&lt;p&gt;So instead of creating a molecule.yml file for each role, we’re going to create one at the
root of the project, that is going to test our master playbook. Inside of it, we’re going
to specify a sample test architecture, which is going to be the same one we defined in our
Vagrantfile. We’re also going to specify the playbook that we want to test. We’re going to
name it kubernetes.yml (creative right?). You can see the molecule.yml file &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/molecule.yml&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can test the roles using the &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule test&lt;/code&gt; command. This will:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create up the infrastructure (&lt;code class=&quot;highlighter-rouge&quot;&gt;create&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Verify the syntax of the Ansible roles (&lt;code class=&quot;highlighter-rouge&quot;&gt;syntax&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Execute the playbook (&lt;code class=&quot;highlighter-rouge&quot;&gt;converge&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Check that the playbook is idempotent by checking for the diffs that a new execution would apply with a dry run (&lt;code class=&quot;highlighter-rouge&quot;&gt;idempotence&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Destroy the previously created infrastructure (&lt;code class=&quot;highlighter-rouge&quot;&gt;destroy&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These actions can be ran separately by typing &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule &amp;lt;action&amp;gt;&lt;/code&gt;, and replacing action
with one of the expressions between parentheses above. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule converge&lt;/code&gt;
will just play your playbook on the hosts. You get the idea, right?&lt;/p&gt;

&lt;p&gt;I added the &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule test&lt;/code&gt; command to my &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/Makefile&quot;&gt;Makefile&lt;/a&gt;, under the &lt;code class=&quot;highlighter-rouge&quot;&gt;test&lt;/code&gt; target. That means
that I can run &lt;code class=&quot;highlighter-rouge&quot;&gt;make test&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;molecule test&lt;/code&gt;. That makes me gain 0.4 seconds per day.
We’re up to 4.1 seconds in total. Sweet!&lt;/p&gt;

&lt;p&gt;Now that we got our virtual machines running, and our testing configuration ready, let’s start
configuring stuff. We just gotta take care of a little problem before we can go YOLO with Ansible though.&lt;/p&gt;

&lt;h2 id=&quot;coreos-and-ansible&quot;&gt;CoreOS and Ansible&lt;/h2&gt;

&lt;p&gt;Remember back then when I said that CoreOS ships only with the basics? That means no Python.
By extension, that means &lt;a href=&quot;http://docs.ansible.com/ansible/intro_installation.html#managed-node-requirements&quot;&gt;no Ansible&lt;/a&gt;. That’s why this time it’s a little bit more tricky.&lt;/p&gt;

&lt;p&gt;So we need to do something about that. If CoreOS won’t come to Python, Python must go to CoreOS.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Since we cannot use Ansible to install Python because we need Python to execute Ansible modules
in the first place, we’ll just install Python manually in each one of the machines, right?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Well, no, not really.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;By the way, if the previous question made you think about the chicken or the egg causality dilemma,
just know that the &lt;a href=&quot;http://www.popsci.com/science/article/2013-02/fyi-which-came-first-chicken-or-egg&quot;&gt;egg came first&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There are three Ansible modules that do not need Python installed on the target host: the &lt;a href=&quot;http://docs.ansible.com/ansible/raw_module.html&quot;&gt;raw module&lt;/a&gt;,
the &lt;a href=&quot;http://docs.ansible.com/ansible/script_module.html&quot;&gt;script module&lt;/a&gt;, and the &lt;a href=&quot;http://docs.ansible.com/ansible/synchronize_module.html&quot;&gt;synchronize module&lt;/a&gt;. The first allows you to execute an SSH command, without
going through the module subsystem. The second one allows you to copy a script to a remote host, and
execute it using the remote host’s shell environment. The third is a wrapper around rsync, which just
uses rsync on the control machine and the remote host.&lt;/p&gt;

&lt;p&gt;Using these modules, we can install a lightweight Python implementation called &lt;a href=&quot;http://pypy.org/&quot;&gt;PyPy&lt;/a&gt;. The workflow
is as follows: I verify that Python is installed using the raw module, and if that is not the case,
I install it using a more raw tasks.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if Python is installed&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--version&quot;&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;python_install&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if install tar file exists&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/tmp/pypy-.tar.bz2&quot;&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pypy_tar_file&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if pypy directory exists&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pypy_directory&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;

    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check if libtinfo is simlinked&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/lib/libtinfo.so.5&quot;&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;libtinfo_symlink&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;changed_when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;ignore_errors&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;yes&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Download PyPy&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;wget -O /tmp/pypy-.tar.bz2 https://bitbucket.org/pypy/pypy/downloads/pypy--linux64.tar.bz2&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;when&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pypy_tar_file | failed&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/bootstrap/ansible-bootstrap/tasks/configure.yml#L1-L26&quot;&gt;Right on&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;Why use the &lt;code class=&quot;highlighter-rouge&quot;&gt;changed_when: false&lt;/code&gt; flag on the tasks inside the block, you say? The thing is
that each time you execute a raw task, a change will be made, since the shell command is
actually being executed on the target host. That means that each time you run your playbook,
you will execute the tasks, no matter what. So if you’re downloading things, creating
directories, or adding lines to configuration files, you will do so multiple times. This
is not idempotent. That’s why I verify the state of the Python installation before installing
Python, and only execute it when Python is not installed. I just add the &lt;code class=&quot;highlighter-rouge&quot;&gt;changed_when: false&lt;/code&gt;
flag to the verification tasks, since they only verify the existence of the Python associated
resources; there are no system modifications because of them.&lt;/p&gt;

&lt;p&gt;I feel this is slightly better than executing a shell script with every task embedded into it.
It allows me to replay tasks when certain script do not exist, and to have a clear idea of what
failed when I get an error: I know right away which task failed, which helps in the debugging process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/4/ansible-scripts.png&quot; alt=&quot;Ansible scripts&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;http://docs.ansible.com/ansible/script_module.html#notes&quot;&gt;Thanks mom&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;I did not create &lt;a href=&quot;https://coreos.com/blog/managing-coreos-with-ansible.html&quot;&gt;this approach&lt;/a&gt;, by the way. I just optimised it to make it actually idempotent.
I guess you need to stand in the shoulders of giants in order to further from time to time, right?&lt;/p&gt;

&lt;p&gt;Let us revisit testing for a second. As a general rule when writing Ansible code, I try to tag &lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/kubernetes.yml#L8&quot;&gt;roles&lt;/a&gt;
and tasks as much as possible. This helps a lot if you want to execute only one part of your playbook,
or only one role. I also try to use smoke tests whenever it is possible. This means that I’ll check
that the main feature of my role is working after executing it. If I’m installing Python, I’ll just
do something like &lt;code class=&quot;highlighter-rouge&quot;&gt;python --version&lt;/code&gt; and check that I don’t get any errors. For that, inside of each
role’s tasks directory I’ll try to create a main.yml file, which will in turn include a configure.yml
file and a test.yml file. The configure.yml file will do all the installation/configuration of the
specified component, and the test.yml file (tagged with the test tag) that will test the component,
if possible.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Install and configure PyPy&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;configure.yml&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Test PyPy installation&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test.yml&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/bootstrap/ansible-bootstrap/tasks/main.yml#L4-L9&quot;&gt;Smoke test all the way!&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;By doing this, you will actually test that your infrastructure is running and that it is probably
properly configured. Then, if you want to run nothing but your tests, you can do it if you specify
the &lt;code class=&quot;highlighter-rouge&quot;&gt;test&lt;/code&gt; tag while running a playbook. Something like &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible-playbook -i inventories/vagrant.ini
kubernetes.yml --tags test&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And thus, the ‘smoketest’ target on my Makefile is born.&lt;/p&gt;

&lt;p&gt;Let us continue.&lt;/p&gt;

&lt;h2 id=&quot;ssl&quot;&gt;SSL&lt;/h2&gt;

&lt;p&gt;I won’t go really deep into this part. &lt;a href=&quot;https://www.openssl.org/news/changelog.html#x85&quot;&gt;OpenSSL exists since 1998&lt;/a&gt;, so it’s not exactly recent news.
I create a CA for the whole cluster, and then create keys and sign certificates for the API server,
for each one of the workers, and for the administrator (the one that’s going to be used by you
when configuring kubectl).&lt;/p&gt;

&lt;h2 id=&quot;etcd&quot;&gt;etcd&lt;/h2&gt;

&lt;p&gt;In this deployment we’re using a single etcd node. You can modify the number of instances
from the Vagrantfile, the Ansible code is able to handle a multi-node cluster.
Just use odd numbers, because of &lt;a href=&quot;https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size&quot;&gt;fault tolerance&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyways, configuration is pretty straightforward on the single-node scenario. I just
configure it to listen on every interface, and then add the advertise client url to
the etcd unit using environment variables:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Service]
{% if groups ['etcd'] | length == 1 %}
Environment=ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
Environment=ETCD_ADVERTISE_CLIENT_URLS=http://{{ ansible_env.COREOS_PUBLIC_IPV4 }}:2379
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/etcd/templates/40-listen-address.conf.j2#L2-L6&quot;&gt;Straightforward&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;And it gets slightly trickier with a multi-node configuration, since the nodes need
to be aware of each other, using the ETCD_INITIAL_CLUSTER variable. You also need to
provide a node name, and a cluster token for everything to work. There are other options,
like using an existing etcd cluster as a discovery mechanism (but we don’t have one at the moment),
or a &lt;a href=&quot;https://coreos.com/os/docs/latest/cluster-discovery.html&quot;&gt;public etcd discovery system&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Environment=ETCD_NAME=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;ansible_hostname&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Environment=ETCD_INITIAL_ADVERTISE_PEER_URLS=http://&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;hostvars[ansible_hostname]['ansible_env']['COREOS_PUBLIC_IPV4']&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2380&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Environment=ETCD_LISTEN_PEER_URLS=http://&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;hostvars[ansible_hostname]['ansible_env']['COREOS_PUBLIC_IPV4']&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2380&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Environment=ETCD_LISTEN_CLIENT_URLS=http://&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;hostvars[ansible_hostname]['ansible_env']['COREOS_PUBLIC_IPV4']&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2379&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,http://&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;127.0&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2379&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Environment=ETCD_ADVERTISE_CLIENT_URLS=http://&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;hostvars[ansible_hostname]['ansible_env']['COREOS_PUBLIC_IPV4']&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2379&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Environment=ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Environment=ETCD_INITIAL_CLUSTER=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;groups['etcd']&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}=http://&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;hostvars[host]['ansible_env']['COREOS_PUBLIC_IPV4']&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2380&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;not&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;loop.last&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;endif&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;endfor&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Environment=ETCD_INITIAL_CLUSTER_STATE=new&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;endif&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/etcd/templates/40-listen-address.conf.j2#L6-L16&quot;&gt;Less straightforward&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;All of these configurations can be made either with environment variables or with flags
when starting the etcd2 service. The &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible_env.COREOS_PUBLIC_IPV4&lt;/code&gt; variable will be
replaced by the node’s public IP. I do this often on this project. Then, I just start
and enable the service. This is done with the systemd module, and that’s why we need Ansible 2.2.&lt;/p&gt;

&lt;p&gt;The test part of the role verifies that machine is listening on port 2379, that the etcd
cluster is reachable via etcdctl, and then it verifies that the “coreos.com” default
namespace exists. It’s a simple, effective smoke test.&lt;/p&gt;

&lt;p&gt;With our working 1-node etcd cluster (get it?), we’ll configure the Kubernetes master node.&lt;/p&gt;

&lt;h2 id=&quot;excellent-how&quot;&gt;Excellent, how?&lt;/h2&gt;

&lt;p&gt;Aren’t you tired already? I know I am. That’s all for today. I’ll talk to you about
the really really fun part in the next article.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;

</description>
          <pubDate>2017-03-30T21:19:02+02:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-kube-4/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-kube-4/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Kubernetes! Episode 3 - Infrastructure as code: the tools of the trade</title>
          <description>&lt;h2 id=&quot;i-understand-the-sdn-related-issues-tell-me-about-infrastructure-as-code-already&quot;&gt;I understand the SDN-related issues. Tell me about Infrastructure as Code already!&lt;/h2&gt;

&lt;p&gt;Alright. Whenever I think about automating the creation of a platform or an application
using &lt;a href=&quot;https://en.wikipedia.org/wiki/Infrastructure_as_Code&quot;&gt;Infrastructure as Code&lt;/a&gt;, I think about three different stages: provisioning,
configuration and deployment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Provisioning&lt;/strong&gt; is the process of creating the virtual resources in which your application
or platform will run. The complexity of the resources may vary depending on your platform:
from simple virtual machines if you’re working locally, to something slightly more elaborate
if you’re working on the cloud (network resources, firewalls, and various other services).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt; is the part in which you configure your virtual machines so that they can behave
in a certain way. This stage includes general OS configuration, security hardening, middleware
installation, middleware-specific configuration, and so on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deployment&lt;/strong&gt; is usually application deployment, or where you put your artefacts in the right place
in order to make your applications work on the previously configured resources.&lt;/p&gt;

&lt;p&gt;Sometimes, a single tool for all these stages will do. Sometimes, it will not. Most of the time
I try to keep my code as simple and replaceable as possible (good code is easy to delete, right?).
Don’t get me wrong, I won’t play on hard mode or try to do everything using shell scripts.
I’ll just try to &lt;a href=&quot;https://en.wikipedia.org/wiki/KISS_principle&quot;&gt;keep it simple&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But let me talk to you a bit about CoreOS first.&lt;/p&gt;

&lt;h2 id=&quot;less-is-more-enter-coreos-yeah-yeah-container-linux&quot;&gt;Less is more: Enter CoreOS (yeah yeah, Container Linux)&lt;/h2&gt;

&lt;p&gt;Container Linux by CoreOS (formerly known as CoreOS Linux, or just CoreOS) is a lightweight Linux
distribution that uses containers to run applications. This is a game changer if you’re used to
standard Linux distributions, in which you install packages using a package manager. This thing
doesn’t even have a package manager.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/3/coreos-package-manager.png&quot; alt=&quot;CoreOS package manager&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Now what?!&lt;/figcaption&gt;

&lt;p&gt;It just ships with the basic GNU Core Utilities so you can move around, and then some tools
that will come in handy for our quest. These include &lt;a href=&quot;https://kubernetes.io/docs/admin/kubelet/&quot;&gt;Kubelet&lt;/a&gt;, &lt;a href=&quot;https://www.docker.com/what-docker&quot;&gt;Docker&lt;/a&gt;, &lt;a href=&quot;https://github.com/coreos/etcd&quot;&gt;etcd&lt;/a&gt; and &lt;a href=&quot;https://github.com/coreos/flannel&quot;&gt;flannel&lt;/a&gt;.
I’ll explain how these things work and how I’m going to use them in the whole Kubernetes journey
later. Just keep in mind that CoreOS (yeah yeah, Container Linux) is an OS specifically
designed to run containers, and that we’re going to take advantage of that in our context.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/3/coreos-explanation.png&quot; alt=&quot;CoreOS distribution&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://coreos.com/why/#distro&quot;&gt;CoreOS as a distribution&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;&lt;em&gt;So we’re going to manually create these CoreOS virtual machines before installing Kubernetes, right?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Well, no, not really.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;provisioning-vagrant-to-the-rescue&quot;&gt;Provisioning: Vagrant to the rescue&lt;/h2&gt;

&lt;p&gt;Vagrant is pretty rad too. It allows you to create reproducible environments based on virtual
machines on many different backends, using code. So you just write something called a Vagrantfile,
in which you specify all the machines you want and their configuration using Ruby. Then, you type
&lt;code class=&quot;highlighter-rouge&quot;&gt;vagrant up&lt;/code&gt; and all your virtual machines will start popping up.&lt;/p&gt;

&lt;p&gt;For this we’re using VirtualBox as a provider. &lt;a href=&quot;https://www.vagrantup.com/docs/providers/&quot;&gt;There are many others&lt;/a&gt;, in case you’re feeling creative.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So, once we have all the virtual hosts we need running in our computer, we’re just going to
manually configure everything in them, right?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Well, no, not really.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;configuration-and-deployment-ansible-zen&quot;&gt;Configuration and Deployment: Ansible zen&lt;/h2&gt;

&lt;p&gt;You do know Ansible, right? Just so you know, an ansible is a category of fictional device or
technology capable of instantaneous or superluminal communication. The term was first used in
Rocannon’s World, a science fiction novel by Ursula K. Le Guin. Oh, it is also an IT automation tool.&lt;/p&gt;

&lt;p&gt;I really like Ansible because &lt;em&gt;most of the time&lt;/em&gt;, the only thing you need in order to use it is a
control machine (which can be the same computer you’re using to code) and SSH access to the
target hosts. No complex architectures or master-slave architectures. You can start coding right away!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: This is not one of those cases, but we’ll get to that in the next article.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So, we can configure our platform automatically using Ansible. We’re just going to create our
machines automatically, configure our resources automatically, and just hope it works, right?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Well, no, not really.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;test-and-conquer-molecule&quot;&gt;Test and conquer: Molecule&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://molecule.readthedocs.io/en/stable-1.18/&quot;&gt;Molecule&lt;/a&gt; is a testing tool for Ansible code. It spins up ephemeral infrastructure, it test
your roles on the newly created infrastructure, and then it destroys the infrastructure. It
also checks for a whole range of other things, like syntax, code quality and impotence,
so it’s pretty well adapted for what we’re trying to do here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/3/molecule-logo.png&quot; alt=&quot;Molecule logo&quot; class=&quot;center-image&quot; width=&quot;360px&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;It's an actual molecule!&lt;/figcaption&gt;

&lt;p&gt;There are 3 main Molecule drivers: Docker, OpenStack and Vagrant. I usually use the Docker
driver for testing roles, due to the fact that a container is usually lightweight, easy to
spin up and destroy, and faster than a virtual machine. The thing is that it’s hard to
create a CoreOS container, in order to install Kubernetes to create more containers. Like,
I heard you like containers so let me put a container inside your container so you can schedule
containers inside containers while you schedule containers inside containers. Besides, there
are no CoreOS Docker images as of this moment. Therefore, we’ll be using the Vagrant driver.
The target platform just happens to be Vagrant and VirtualBox. Huh.&lt;/p&gt;

&lt;p&gt;So we’re going to test our code on VirtualBox virtual machines launched by Vagrant, which is
exactly the platform we’re using for our project. Great.&lt;/p&gt;

&lt;h2 id=&quot;excellent-just-show-me-the-code&quot;&gt;Excellent, just show me the code!&lt;/h2&gt;

&lt;p&gt;I could, but I won’t. That’s all for today. I’ll talk to you about the really really fun part in the next article.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;

</description>
          <pubDate>2017-03-24T20:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-kube-3/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-kube-3/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Kubernetes! Episode 2 - Kubernetes networking</title>
          <description>&lt;h2 id=&quot;i-dont-even-understand-what-the-problem-is-dude&quot;&gt;I don’t even understand what the problem is dude&lt;/h2&gt;

&lt;p&gt;Well, as I said in the previous article, communication between pods that are
hosted on different machines can be a little bit tricky. Docker will create by
default a virtual bridge called “docker0” on the host machine, and it will assign
a private network range to it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/2/docker-bridge.png&quot; alt=&quot;Docker bridge&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Super bridge (127.17.0.1/16)&lt;/figcaption&gt;

&lt;p&gt;For each container that is created, a virtual ethernet device is attached to this bridge,
which is then mapped to eth0 inside the container, with an ip within the aforementioned network range.
Note that this will happen for each host that is running Docker, without any coordination between
the hosts. Therefore, the network ranges might collide.&lt;/p&gt;

&lt;p&gt;Because of this, containers will only be able to communicate with containers that are connected to
the same virtual bridge. In order to communicate with other containers on other hosts, they must rely
on port-mapping. This means that you need to assign a port on the host machine to each container, and
then somehow forward all traffic on that port to that container. What if your application needs to
advertise its own IP address to a container that is hosted on another node? It doesn’t actually knows
its real IP, since his local IP is getting translated into another IP and a port on the host machine.
You can automate the port-mapping, but things start to get kinda complex when following this model.&lt;/p&gt;

&lt;p&gt;That’s why Kubernetes chose simplicity and skipped the dynamic port-allocation deal. It just assumes
that all containers can communicate with each other without Network Address Translation (NAT), that
all containers can communicate with each node (and vice-versa), and that the IP that a container sees
for itself is the same that the other containers see for it. Aside from being simpler, it also enables
applications to be ported rather easily from virtual machines to containers, since they do not have to
change the way they work network-wise.&lt;/p&gt;

&lt;p&gt;There are many different networking options that offer these capabilities for Kubernetes: &lt;a href=&quot;https://github.com/contiv/netplugin&quot;&gt;Contiv&lt;/a&gt;, &lt;a href=&quot;https://github.com/coreos/flannel#flannel&quot;&gt;Flannel&lt;/a&gt;,
&lt;a href=&quot;http://www.nuagenetworks.net/&quot;&gt;Nuage Networks&lt;/a&gt;, &lt;a href=&quot;https://kubernetes.io/docs/admin/ovs-networking/&quot;&gt;OpenVSwitch&lt;/a&gt;, &lt;a href=&quot;https://github.com/openvswitch/ovn-kubernetes&quot;&gt;OVN&lt;/a&gt;, &lt;a href=&quot;http://docs.projectcalico.org/v2.0/introduction/&quot;&gt;Project Calico&lt;/a&gt;, &lt;a href=&quot;http://romana.io/&quot;&gt;Romana&lt;/a&gt; and &lt;a href=&quot;https://www.weave.works/products/weave-net/&quot;&gt;Weave Net&lt;/a&gt;.
For this project, we will use the combination of two of these options: Calico and Flannel, or &lt;a href=&quot;https://github.com/projectcalico/canal&quot;&gt;Canal&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;show-me-the-canal&quot;&gt;Show me the Canal!&lt;/h2&gt;

&lt;p&gt;Alright. Let’s talk about Flannel and Calico then.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/2/flannel-calico.jpg&quot; alt=&quot;Flannel and Calico&quot; class=&quot;center-image&quot; width=&quot;360px&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Great logos&lt;/figcaption&gt;

&lt;p&gt;Flannel allows inter-pod communication between different hosts by providing an overlay software-defined
network (SDN). This solves the main issue we had the Docker networking model. As I said before, when using
Docker, each container has an IP address that allows it to communicate with other containers &lt;strong&gt;on the same host&lt;/strong&gt;.
When pods are placed in different hosts, they rely on their host IP address. Therefore, communication between
between them is possible by port-mapping. This is fine at a container-level, but applications running inside
these containers can have a hard time if they need to advertise their external IP and port to everyone else.&lt;/p&gt;

&lt;p&gt;Flannel helps by giving each host a different IP subnet range. The Docker daemon will then assign IPs from
this range to containers. Then containers can talk to each user using these unique IP addresses by means of
packet encapsulation. Imagine that you have two containers, Container A and Container B. Container A is
placed on Host Machine A, and Container B is placed on Host Machine B. When Container A wants to talk to
Container B, it will use container B’s IP address as the destination address of his packet. This packet will
then be encapsulated with an outer UDP packet between Host Machine A and Host Machine B, which will be sent
by Host Machine A, and that will have Host Machine B’s IP address as the destination address. Once the packet
arrives to Host Machine B, the encapsulation is removed and the packet is routed to the container using the
inner IP address. The flannel configuration regarding the container/Host Machine mapping is stored in etcd.
The routing is done by a flannel daemon called flanneld.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/2/flannel-sdn.png&quot; alt=&quot;Flannel SDN diagram&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/coreos/flannel/blob/master/README.md#theory-of-operation&quot;&gt;Like this, see?&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;Calico secures this overlay network, restricting traffic between the pods based on a fine-grained network policy.
As I said before, the default Kubernetes behaviour is to allow traffic from all sources inside or outside the
cluster to all pods within the cluster. Little reminder from the Kubernetes networking model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;all containers can communicate with all other containers without NAT&lt;/li&gt;
  &lt;li&gt;all nodes can communicate with all containers (and vice-versa) without NAT&lt;/li&gt;
  &lt;li&gt;the IP that a container sees itself as is the same IP that others see it as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For security and multi-tenancy reasons, it is coherent to restrict communication between sets of pods on the
Kubernetes cluster. Calico supports the &lt;a href=&quot;https://github.com/caseydavenport/kubernetes/blob/network-policy/docs/admin/network-policy.md#v1alpha1-api&quot;&gt;v1alpha1&lt;/a&gt; network policy  API for Kubernetes. Basically what it does
is that it enables network isolation to limit connectivity from an optional set of sources to an optional
set of destination TPC/UDP ports. This does not limit the access to the pods by the host itself, as it is
necessary for Kubernetes health checks.&lt;/p&gt;

&lt;p&gt;With that in mind, inter-pod communication can be restricted at a namespace level, or using particular network
policies, using selectors to select the concerned nodes.&lt;/p&gt;

&lt;p&gt;I chose Flannel for the SDN part because it is the standard SDN tool for CoreOS (Container Linux), it is
shipped with the distribution, it is rather easy to configure, and the documentation is great. I chose
Calico because I wanted to use test policy-based security management on Kubernetes, and because of its
tight integration with Flannel. They both rely on etcd, which is rather cool since I’m running an etcd
cluster anyways (Calico can be used &lt;a href=&quot;http://docs.projectcalico.org/master/getting-started/kubernetes/installation/hosted/k8s-backend/&quot;&gt;without an etcd cluster&lt;/a&gt;, using the Kubernetes API as a datastore,
but this is still experimental).&lt;/p&gt;

&lt;p&gt;By the way, Calico can be used as a standalone component, that will handle both the SDN and the network
policy-based security management. Then again, Flannel has a few additional networking options, such as
udp, vxlan, and even AWS VPC route programming (in case you ever need it).&lt;/p&gt;

&lt;h2 id=&quot;ok-now-i-get-it-so-how-did-you-do-it&quot;&gt;Ok, now I get it. So, how did you do it?&lt;/h2&gt;

&lt;p&gt;I think I have to talk to you about the way I see Infrastructure as Code, and explain the tools of the
trade first. That’s all for today though. The fun part starts in the next article.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;

</description>
          <pubDate>2017-03-17T20:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-kube-2/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-kube-2/</guid>
        </item>
      
    
      
        <item>
          <title>How does it work? Kubernetes! Episode 1 - Kubernetes general architecture</title>
          <description>&lt;h2 id=&quot;hey-everybody&quot;&gt;Hey everybody&lt;/h2&gt;

&lt;p&gt;I hacked something together in order to create a Kubernetes cluster on CoreOS
(or Container Linux) using Vagrant and Ansible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you keep reading, I’m going to talk to you about Kubernetes, etcd, CoreOS, flannel,
Calico, Infrastructure as Code and Ansible testing strategies. It’s gonna be super fun.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts.
This is episode 1, regarding the Kubernetes general architecture.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to try it:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/sebiwi/kubernetes-coreos
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;kubernetes-coreos
make up
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will spin up 4 VMs: an etcd node, a Kubernetes Master node, and two Kubernetes Worker nodes.
You can modify the size of the cluster by hacking on the Vagrantfile and the Ansible inventory.&lt;/p&gt;

&lt;p&gt;You will need Ansible 2.2, Vagrant, Virtualbox and kubectl. You will also need molecule and docker-py,
if you want to run the tests.&lt;/p&gt;

&lt;h2 id=&quot;why&quot;&gt;Why?&lt;/h2&gt;

&lt;p&gt;The last time I worked with Kubernetes was &lt;a href=&quot;https://github.com/kubernetes/kubernetes/releases/tag/v1.0.0&quot;&gt;last year&lt;/a&gt;. Things have &lt;a href=&quot;https://github.com/kubernetes/kubernetes/releases/tag/v1.0.0&quot;&gt;changed&lt;/a&gt; since then. A guy I know once
said that in order to understand how something complex works, you need to build it up from scratch.
I guess that’s one of the main points of this project, really. Scientia potentia est. I also wanted
to be able to test different Kubernetes features in a set up reminiscent of a production cluster.
&lt;a href=&quot;https://github.com/kubernetes/minikube&quot;&gt;Minikube&lt;/a&gt; is great and all, but you don’t actually get to see communication between different containers
on different hosts, node failover scenarios, scheduling policies, or hardcore scale up procedures
(with many nodes).&lt;/p&gt;

&lt;p&gt;Finally, I thought it would be nice to explain how every Kubernetes component fits together, so everyone
can understand what’s under the hood. I keep getting questions like “what is a Kubernetes” at work, and
if it is “better than a Docker”. You won’t find the answer to the last question in this article, but at
least you will (hopefully) understand how Kubernetes works. You can make up your own mind afterwards.&lt;/p&gt;

&lt;h2 id=&quot;okay-i-was-just-passing-by-but-what-is-kubernetes&quot;&gt;Okay, I was just passing by, but what is Kubernetes?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt; is a container cluster management tool. I will take a (not so) wild guess and assume that you’ve
already heard about Docker and containers in general.&lt;/p&gt;

&lt;p&gt;The thing is that Docker by itself will probably not suffice when using containers in production. What if your
application is composed of multiple containers? You will need to be able to handle not only the creation of these
containers, but also the communication between them. What if you feel that putting all your containers on the same
host sucks, since if that host goes down, all your containers die with it? You will need to be able to deploy
containers on many hosts, and also handle the communication between them, which translates into port
mapping hell unless you’re using an SDN solution. What about deploying a new version of your
application without service interruption? What about container failure management, are you going
to do go check on every container independently to see if it is healthy, and relaunch it manually
if it is not? Grrrrrrrraaaaah.&lt;/p&gt;

&lt;p&gt;Kubernetes is a tool you can use if you do not want to develop something specific in order to
handle all the aforementioned issues. It can help you pilot your container cluster, hence its
name, which means pilot or helmsman in greek.&lt;/p&gt;

&lt;p&gt;Just a little heads up before we start talking about architecture: I will often talk about pods
during these series. A pod is a group of one or more containers, that run in a shared context.
A pod models an application-specific “logical host”, and theoretically it contains one or more
applications that are tightly coupled: the kind of applications that you would have executed on
the same physical or virtual host before containers. In practice, and for our use-case, you can
just think of a pod as a container, since we will only have one container inside each pod.&lt;/p&gt;

&lt;p&gt;A standard Kubernetes installation consists of both Master and Worker nodes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/1/kube-general-architecture.png&quot; alt=&quot;Kubernetes general architecture&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture.md&quot;&gt;Kubernetes general architecture&lt;/a&gt;, for real&lt;/figcaption&gt;

&lt;h2 id=&quot;wait-where-is-the-master-node&quot;&gt;Wait, where is the Master node?&lt;/h2&gt;

&lt;p&gt;This architecture diagram specifies a set of master components, but not a Master node per-se.
You will probably notice that the Distributed Watchable Storage is considered among these components.
Nevertheless, we will not install it on the same node.  Our Distributed Watchable Storage will be a
separate component (more on this later).&lt;/p&gt;

&lt;p&gt;So the Master node actually becomes everything that is inside the red square:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/1/kube-master-node.png&quot; alt=&quot;Kubernetes master node&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture.md&quot;&gt;Here, see?&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;All these components are part of the Kubernetes control pane. Keep in mind that you can have these
on one single node, but you can also put them on many different ones. In our case we’re putting them
all together. So basically, we have an API server, a Scheduler, and a Controller Manager Server.&lt;/p&gt;

&lt;p&gt;The API server exposes the Kubernetes API (duh). It processes REST operations, and then updates etcd
consequently. The scheduler binds the unscheduled pods to a suitable worker node. If none are available,
the pod remains unscheduled until a fitting node is found. The controller manager server does all the
other cluster-level functions, such as endpoint creation, node discovery, and replication control.
Many controllers are embedded into this controller manager, such as the endpoint controller, the node
controller and the replication controller. It watches the shared state of the Kubernetes cluster using
the API server, and makes changes on it with the intention of making the current state and the desired
state of the cluster match.&lt;/p&gt;

&lt;h2 id=&quot;what-about-the-worker-node&quot;&gt;What about the Worker node?&lt;/h2&gt;

&lt;p&gt;The Master node does not run any containers, it just handles and manages the cluster. The nodes that
actually run the containers are the Worker nodes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: This is not actually true in our case, but we will talk about that later, during episode 5.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Worker node is composed of a kubelet, and a proxy (kube-proxy). You can see these components
inside the red square, in the diagram below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/1/kube-worker-node.png&quot; alt=&quot;Kubernetes worker node&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture.md&quot;&gt;Right here&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;The kubelet is the agent on the worker node that actually starts and stops the pods, and communicates
with the Docker engine at a host level. This means it also manages the containers, the images and the
associated volumes. It communicates with the API server on the Master node.&lt;/p&gt;

&lt;p&gt;The kube-proxy redirects traffic directed to specific services and pods to their destination. It
communicates with the API server too.&lt;/p&gt;

&lt;h2 id=&quot;and-what-about-that-distributed-watchable-storage-thing&quot;&gt;And what about that Distributed Watchable Storage thing?&lt;/h2&gt;

&lt;p&gt;Oh, you mean etcd. This is one of the components that is actually included in Container Linux, and
developed by CoreOS. It is a distributed, fault tolerant key-value store used for shared configuration
and service discovery. It actually means &lt;a href=&quot;https://www.youtube.com/watch?v=2ByAMZ7CZyY&amp;amp;t=9m48s&quot;&gt;“something like etc, distributed on many hosts&lt;/a&gt;”. This &lt;del&gt;sucks&lt;/del&gt;
is a weird name because it is not a filesystem, but a key-value store. They are aware of it though. Just in
case you missed it in the previous diagrams, it is the black square inside the red square:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/how-does-it-work-kube/1/etcd.png&quot; alt=&quot;etcd&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;a href=&quot;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture.md&quot;&gt;You never know&lt;/a&gt;&lt;/figcaption&gt;

&lt;p&gt;All the persistent master state is stocked in etcd. Since components can actually “watch” components, they are
able to realise that something has changed rather quickly, and then do something about it.&lt;/p&gt;

&lt;p&gt;And that’s what I deployed on CoreOS using Ansible. All of these things, and then some more.&lt;/p&gt;

&lt;h2 id=&quot;thats-rad-how-did--you-do-it&quot;&gt;That’s rad, how did  you do it?&lt;/h2&gt;

&lt;p&gt;You need to understand a little bit about Kubernetes networking before we get to that. That’s all
for today though. I’ll talk to you about networking in the next episode.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;

</description>
          <pubDate>2017-03-11T20:19:02+01:00</pubDate>
          <link>https://sebiwi.github.io/blog/how-does-it-work-kube-1/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/how-does-it-work-kube-1/</guid>
        </item>
      
    
      
        <item>
          <title>fpaste-cli: Share content with magic and style</title>
          <description>&lt;h2 id=&quot;hey-people&quot;&gt;Hey people,&lt;/h2&gt;

&lt;p&gt;I hacked something together in order to highlight text and send it automatically to fpaste, then
put the fpaste link in your clipboard automatically.&lt;/p&gt;

&lt;h2 id=&quot;why&quot;&gt;Why?&lt;/h2&gt;

&lt;p&gt;Well, I just happen to share a lot of content (code snippets, application or middleware logs, ASCII
art, you name it!) with other people using both fpaste and pastebin. It makes it easier to read
text when trying to debug something.&lt;/p&gt;

&lt;p&gt;Basically, I really dislike this format:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/fpaste-cli/awful-message.png&quot; alt=&quot;Awful hangouts message&quot; class=&quot;center-image&quot; width=&quot;220px&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Sigh.&lt;/figcaption&gt;

&lt;p&gt;It makes me want to rage quit. And then throw my computer out the window.
What I’ve realised is that people don’t use pastebin/fpaste because it takes approximately 13
seconds to open the website, paste the content, click on post, and then share the URL.
Everyone knows that 13 seconds is way too long in 2017.&lt;/p&gt;

&lt;h2 id=&quot;so-i-just-do-this-instead&quot;&gt;So I just do this instead:&lt;/h2&gt;

&lt;p&gt;I highlight the stuff I want to share, I right click it, and then I click on “Send to fpaste”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/fpaste-cli/send-to-fpaste.png&quot; alt=&quot;Send to fpaste example&quot; /&gt;&lt;/p&gt;
&lt;figcaption class=&quot;caption&quot;&gt;This looks simple, right?&lt;/figcaption&gt;

&lt;p&gt;This is a pretty cool picture by the way. It has send to fpaste, vim, and zebras on it.&lt;/p&gt;

&lt;p&gt;Then, magically, I have an fpaste link on my clipboard, which contains the content I just
highlighted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/fpaste-cli/send-to-fpaste-result.png&quot; alt=&quot;Send to fpaste result&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pretty cool huh? And stylish, while at it.&lt;/p&gt;

&lt;p&gt;That being said, I use an Automator workflow in order to do the magic, so this part only works on
OS X. I would really like to do something similar on Linux, but I don’t know any tools similar to
Automator that would allow to create a similar workflow.&lt;/p&gt;

&lt;p&gt;If you think of any, please let me know!&lt;/p&gt;

&lt;h2 id=&quot;why-fpaste-and-not-pastebin&quot;&gt;Why fpaste and not pastebin?&lt;/h2&gt;

&lt;p&gt;Mostly because I was working at a place where pastebin was blocked when I wrote this, and fpaste
wasn’t. Besides, someone &lt;a href=&quot;https://github.com/tupton/pastebin-cl&quot;&gt;already did it for pastebin&lt;/a&gt;. It gave me a good starting point.&lt;/p&gt;

&lt;h2 id=&quot;so-how-do-i-set-it-up&quot;&gt;So how do I set it up?&lt;/h2&gt;

&lt;p&gt;You just have to clone the &lt;a href=&quot;https://github.com/sebiwi/fpaste-cli&quot;&gt;GitHub project&lt;/a&gt;, and then create an Automator service that sends the
highlighted text to your script as stdin. You can use the one that’s already on the GitHub repo,
but you will have to adapt it depending on the location of fpaste.py in your system. You can create
a symbolic link in your path if you’re really diggin’ it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sebiwi.github.io/assets/images/fpaste-cli/service-creation.png&quot; alt=&quot;Service creation&quot; class=&quot;bigger-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the script, I didn’t wrap the whole API, since I just wanted to get something working fast.
Maybe I’ll do it sometime in the future. You’re free to contribute as well if you feel like it.&lt;/p&gt;

&lt;p&gt;Anyway, I just thought I’d share.&lt;/p&gt;

</description>
          <pubDate>2017-03-04T10:50:02+01:00</pubDate>
          <link>https://sebiwi.github.io/blog/fpaste-cli/</link>
          <guid isPermaLink="true">https://sebiwi.github.io/blog/fpaste-cli/</guid>
        </item>
      
    
  </channel>
</rss>
